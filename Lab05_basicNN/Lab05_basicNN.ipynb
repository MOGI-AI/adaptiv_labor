{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Labor: Egyszerű Neurális Hálózat implementációja\n",
    "\n",
    "A gyakorlat során az eddig tanult ismeretekre támaszkodva a lehető legegyszerűbb, egy darab rejtett réteget tartalmazó (nem deep) MLP (multi-layer preceptron) vagy FF (feed-forward) neurális háló kerül implementálásra.\n",
    "\n",
    "### A XOR probléma\n",
    "\n",
    "A XOR vagy kizáró vagy művelete egy tipikus iskolapéldája a neurális hálók lehetőségeinek demonstrációjára. Ugyan a XOR kérdés könnyen implementálható `if` elágazások segítségével, az adatokat elválasztó nem lineáris (még csak nem is egy fügvény által leírható) határvonal gyakorlatilag megoldhatatlan a hagyományos statisztikai / regressziós modellek segítségével."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A perceptron\n",
    "\n",
    "Az eddig alkalmazott Logisztikus regressziós modell grafikusan az alábbi alakban ábrázolható:\n",
    "\n",
    "<!--\n",
    "<center><img src=\"img/perceptron.svg\" width=\"300\"></center>\n",
    "-->\n",
    "<center><img src=\"https://raw.githubusercontent.com/MOGI-AI/adaptiv_labor/main/Lab05_basicNN/img/perceptron.svg\" width=\"300\"></center>\n",
    "\n",
    "\n",
    "ahol\n",
    "\n",
    "$$ s = \\sum_{i=0}^n(w_i x_i); \\qquad \\hat{y} = a(s) $$\n",
    "\n",
    "A modell bemenetei az $x_1$ - $x_n$ változók, az ezekhez rendelt súlyok (modellparaméterek) a $w_1$ - $w_n$ értékek (adott esetben kiegészítve a $w_0$ bias paraméterrel), a regressziós értéket valószínűséggé konvertáló `a()`, aktivációs függvény pedig a _sigmoid_ függvény volt. A modell kimenete a $\\hat{y}$ becslés, az adott esemény bekövetkezési valószínűsége. Ezt a struktúrát _perceptronnak_ nevezik. Alakjában és funkciójában nagyon hasonlít egy neuron működésére: a dendriteken beérkező elektromos ingerek (_bemenetek_) súlyozott (_súlyok/modellparaméterek_) összege (_s_) alapján a neuron testében (soma) adott szint elérésekor aktivációs potenciál keletkezik (_aktivációs függvény_), amelyet a neuron az axonon keresztül továbbít ($\\hat y$) a következő neuronok felé. A perceptront ezért ezekben a hálózatokban szokás neuronnak is nevezni.\n",
    "\n",
    "<!--\n",
    "<center><img src=\"img/neuron.svg\" width=\"600\"></center>\n",
    "-->\n",
    "<center><img src=\"https://raw.githubusercontent.com/MOGI-AI/adaptiv_labor/main/Lab05_basicNN/img/neuron.svg\" width=\"600\"></center>\n",
    "\n",
    "\n",
    "\n",
    "Amennyiben az élő szervezetekhez hasonlóan a perceptronok több rétegben egymás után kapcsolódnak, megkapjuk a multilayer perceptron / feed forward neural network arkhitektúrát:\n",
    "<!--\n",
    "<center><img src=\"img/mlp.svg\" width=\"600\"></center>\n",
    "-->\n",
    "<center><img src=\"https://raw.githubusercontent.com/MOGI-AI/adaptiv_labor/main/Lab05_basicNN/img/mlp.svg\" width=\"600\"></center>\n",
    "\n",
    "Ebben a az elrendezésben az első réteget (amely nem valódi réteg, csak a bemeneteket jelképezi) bemeneti rétegnek, az utolsó réteget kimeneti rétegnek, a közbülső reteg(ek)et pedig rejtett rég(ek)enek nevezik. A rejtett és a kimeneti rétegek mindegyikéhez tartoznak:\n",
    "- $x_i$ bemenetek (ez előző réteg neuronjainak + a mindig egy értéket adó bias neuronnak kimenetei) mint $\\mathbf{X}$ bemeneti vektor(sormátrix)\n",
    "- $w_{i,j}$ súlyok (az $i$-edik bemenethez és a réteg $j$-edik neuronjához tartozó súly) mint $\\mathbf{W}$ súlymátrix\n",
    "- $\\hat y_j$ kimenetek mint $\\mathbf{\\hat Y}$ kimeneti vektor (sormátrix)\n",
    "\n",
    "Egy $k$-adik réteg $j$-edik neuronjának kimenete a perceptron modell szerint:\n",
    "$$\\hat y_j^{(k)} = a\\left( s_j^{(k)} \\right) = a\\left( w_0^{(k)} + \\sum_{i=0}^{n}(w_{i,j}^{(k)} x_i^{(k)}) \\right)$$\n",
    "\n",
    "A teljes $k$-adik réteg kimenete  mártrixos alakban pedig:\n",
    "$$\\mathbf{\\hat Y}^{(k)} = a\\left( \\mathbf{S}^{(k)} \\right) = a\\left( \\mathbf{W}^{(k)} \\cdot \\mathbf{X}^{(k)} \\right)$$\n",
    "\n",
    "Amennyiben egyszerre több bemeneti adatpontra történik a számítás, úgy mind az $\\mathbf{X}$, $\\mathbf{S}$ és $\\mathbf{\\hat Y}$ egyetlen sor helyett az adatpontok számának megfelelő számú sorral rendelkeznek."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation - predikció neurális hálóval\n",
    "\n",
    "A neurális hálók esetén a bemeneti adatainkat a bemeneti rétegen elindítva, az információ rétegről rétegre halad át a modellen, azaz információ a bemenetektől a kimenetig (előrefele) terjed, *'propagál'*. Vizsgáljuk meg, hogy hogyan fognak alakulni az egyes mátrixok méretei, miközben ez a folyamat végbemegy.\n",
    "\n",
    "A bemeneti mátrix az eddigiekkel megegyezően néz ki, minden sora egy-egy külön bemeneti adatpont, oszlopai tartalmazzák az egyes bementi változókat:\n",
    "\n",
    "$$\n",
    "\\underset{[m \\ \\times \\ n(k-1)+1]}{\\mathbf{X}^{(k)}} = \\left[\n",
    "\t\\begin{array}{ccccc}\n",
    " \t\t1 & x_{1,1}^{(k)} & x_{1,2}^{(k)} & \\ldots & x_{1,n(k-1)}^{(k)}\\\\\n",
    "\t\t1 & x_{2,1}^{(k)} & x_{2,2}^{(k)} & \\ldots & x_{2,n(k-1)}^{(k)}\\\\\n",
    " \t\t\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "        1 & x_{m,1}^{(k)} & x_{m,1}^{(k)} & \\ldots & x_{m,n(k-1)}^{(k)}\\\\\n",
    "\t\\end{array}\t\\right]\n",
    "$$\n",
    "ahol $n(k)$ a $k$-adik réteg neuronjainak száma.\n",
    "\n",
    "A súlyokat tartalamzó mátrixból eddig csak egy darab szerepelt, annak pedi egy darab oszlopa volt. Az MLP struktúra esetén minden réteghez (leszámítva a bemeneti réteget) külön mátrix tartozik. Minden rétegben, neurononként egy oszlop tartalmazza az adott súlyokat, az egyes oszlopok az adott réteg adott neuronjaihoz tartoznak:\n",
    "\n",
    "$$\n",
    "\\underset{[n(k-1)+1 \\ \\times \\ n(k)]}{\\mathbf{W}^{(k)}} = \\left[\n",
    "\t\\begin{array}{ccccc}\n",
    " \t\tw_{0,1} & w_{0,2} & \\ldots & w_{0,n(k))}\\\\\n",
    "\t\tw_{1,1} & w_{1,2} & \\ldots & w_{1,n(k)}\\\\\n",
    " \t\t\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "        w_{n(k-1),1} & w_{n(k-1),2} & \\ldots & w_{n(k-1),n(k)}\\\\\n",
    "\t\\end{array}\\right]\n",
    "$$\n",
    "ahol $n(k)$ az n. rétegben található neuronok száma, $n(k-1)$ pedig az azt megelőző réteg neuronjainak száma.\n",
    "\n",
    "A $k$ réteg kimeneteinek alakulása:\n",
    "$$\\underset{[m \\ \\times \\ n(k)]}{\\mathbf{\\hat{Y}}^{(k)}} = a\\left(\\underset{[m \\ \\times \\ n(k)]}{\\mathbf{S}^{(k)}}\\right) = a\\left(\\underset{[m \\ \\times \\ n(k-1)+1]}{\\mathbf{X}^{(k)}} \\cdot \\underset{[n(k-1)+1 \\ \\times \\ n(k)]}{\\mathbf{W}^{(k)}} \\right)$$\n",
    "$$\\underset{[m \\ \\times \\ n(k-1)+1]}{\\mathbf{X}^{(k)}} = \\underset{[m \\ \\times \\ 1]}{BIAS} + \\underset{[m \\ \\times \\ n(k-1)]}{\\mathbf{Y}^{(k-1)}}$$\n",
    "ahol $a$ az adott rétegben haszált aktivációs függvény (az utolsó sorban található $+$ ebben az esetben konkatenációt jelképez). A tárgy keretében csak a _sigmoid_ függvénnyel foglalkoztunk, mint aktivációs függvény, de számos másik létezik (_identity_, _tanh_, _ReLU_, _LeakyReLU_, stb.), amelyek közül szinte akármelyik szabadon alkalmazható. Az egyetlen megkötés általában az utolós rétegnél van, ahol a költségfüggvényhez hasonlóan az aktivációs függvényt is egyeztetni kell a feladattal, hiszen ez határozza meg, hogy a lehetséges kimenetek halmaza:\n",
    "- regresszió --> MSE & identitás aktivációs függvény [$-\\inf;+\\inf$]\n",
    "- bináris klasszifikáció --> BCE & sigmoid (vagy más [$0;1$] vagy [$-1;1$] kimenetű aktivációs függvény, feltéve hogy a tanítóadatok elvárt kimeneteinek értékkészletével összegyeztethető a tartománya)\n",
    "\n",
    "Egy egyszerű, 2 bemeneti neuront és egy  rejtett rétegben 3 neuront tartalmazó bináris klasszifikációt megvalósító háló esetén a mátrixok méretének alakulása a forward propagation folyamán:\n",
    "- a rejtett rétegre\n",
    "$$ \\underset{m \\ \\times \\ 3}{\\mathbf{X^{(1)}}} =  \\underset{m \\ \\times \\ 1}{BIAS} + \\underset{m \\ \\times \\ 2}{\\mathbf{X}} $$\n",
    "$$ \\underset{m \\ \\times \\ 3}{\\mathbf{S^{(1)}}} = \\underset{m \\times 3}{\\mathbf{X^{(1)}}} \\times \\underset{ 3 \\ \\times \\ 3}{\\mathbf{W^{(1)}}} $$\n",
    "$$ \\underset{m \\ \\times \\ 3}{\\mathbf{\\hat{Y}}^{(1)}} = sigmoid(\\underset{m \\ \\times \\ 3}{{\\mathbf{S}^{(1)}}}) $$\n",
    "- a kimeneti rétegre\n",
    "$$ \\underset{m \\times 4}{\\mathbf{X^{(2)}}} = \\underset{[m \\ \\times \\ 1]}{BIAS} + \\underset{m \\ \\times \\ 3}{\\mathbf{\\hat{Y}}^{(1)}} $$\n",
    "$$ \\underset{m \\ \\times \\ 1}{\\mathbf{S^{(2)}}} = \\underset{m \\times 4}{\\mathbf{X^{(2)}}} \\times \\underset{ 4 \\ \\times \\ 1}{\\mathbf{W^{(2)}}} $$\n",
    "$$ \\underset{m \\ \\times \\ 1}{\\mathbf{\\hat{Y}}^{(2)}} = sigmoid(\\underset{m \\ \\times \\ 1}{{\\mathbf{S}^{(2)}}}) $$\n",
    "Az utolsó rétegünk kimenete maga a modell által adott becslés.\n",
    "$$ \\underset{m \\ \\times \\ 1}{\\mathbf{\\hat{Y}}} = \\underset{m \\ \\times \\ 1}{\\mathbf{\\hat{Y}}^{(2)}} $$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BackPropagation - neurális háló tanítása\n",
    "\n",
    "### Kimeneti réteg\n",
    "\n",
    "A neurális hálók esetén a gradiens módszer elve nem változik. Az egyes súlyokat a költségfüggvény adott súly szerinti parciális deriváltjának alapján módosítjuk, azonban a gradiens számítása jelentősen komplikáltabb, számítására a lánc szabályt használhatjuk. A gradiens számítását a kimeneti réteghez tartozó súlyokra ($p$ réteget: $1$ bemeneti + $(p-1)$ rejtett + $1$ kimeneti - tartalmazó háló esetén):\n",
    "\n",
    "$$ \\nabla_{(p)} C \\approx \\frac{\\partial C}{\\partial \\mathbf{W}^{(p)}} =\n",
    "\\frac{\\partial C}{\\partial \\mathbf{\\hat  Y}^{(p)}} \\frac{\\partial \\mathbf{\\hat  Y}^{(p)}}{\\partial \\mathbf{S}^{(p)}} \\frac{\\partial \\mathbf{S}^{(p)}}{\\partial \\mathbf{W}^{(p)}} $$\n",
    "\n",
    "A parciális derivált egyes tagjati végigszámolva az alábbi összefüggésekre jutunk:\n",
    "\n",
    "$$ \\boxed{ \\frac{\\partial C}{\\partial \\mathbf{W}^{(p)}} =  \\left(\\mathbf{X}^{(p)}\\right)^T \\left( \\frac{\\partial C}{\\partial \\mathbf{\\hat  Y}} a'(\\mathbf{S}^{(p)}) \\right)  = \\left(\\mathbf X^{(p)}\\right)^T \\cdot \\mathbf \\delta^{(p)} } $$\n",
    "\n",
    "\n",
    "A *BCE* költségfüggvény és a *sigmoid* aktiváció együttes használatakor\n",
    "$$ \\boxed{\\mathbf \\delta^{(p)} = \\frac{1}{m} \\left(\\mathbf{\\hat Y} - \\mathbf{Y} \\right) = \\frac{1}{m} \\left(sigmoid(\\mathbf{X}^{(p)}\\cdot \\mathbf{W}^{(p)} ) - \\mathbf{Y} \\right)}$$\n",
    "\n",
    "Látható, hogy visszakaptuk az egyzserű logisztikus regresszió esetén alkalmazott gradiensvektort. \n",
    "\n",
    "### Rejtett rétegek\n",
    "\n",
    "A $k$ rejtett réteg esetében a gradiensek számítása a következőként írható fel:\n",
    "\n",
    "$$ \\nabla_{(k)} C \\approx \\frac{\\partial C}{\\partial \\mathbf{W}^{(k)}} =\n",
    "\\frac{\\partial C}{\\partial \\mathbf{\\hat  Y}^{(k)}} \\frac{\\partial \\mathbf{\\hat  Y}^{(k)}}{\\partial \\mathbf{S}^{(k)}} \\frac{\\partial \\mathbf{S}^{(k)}}{\\partial \\mathbf{W}^{(k)}} $$\n",
    "\n",
    "A kifejezést átalakítva az alábbi összefüggésekre jutunk:\n",
    "\n",
    "$$ \\boxed{ \\frac{\\partial C}{\\partial \\mathbf{W}^{(k)}} =  \\left(\\mathbf{X}^{(k)}\\right)^T \\left( \\frac{\\partial C}{\\partial \\mathbf{\\hat  Y}^{(k)}} a'(\\mathbf{S}^{(k)}) \\right)  = \\left(\\mathbf X^{(k)}\\right)^T \\cdot \\mathbf \\delta^{(k)} } $$\n",
    "\n",
    "A $k$ réteghez tartozó delta tag számítása azonban nem triviális:\n",
    "\n",
    "$$ \\mathbf{\\delta^{(k)}} = \\frac{\\partial C}{\\partial \\mathbf{\\hat  Y^{(k)}}} a'(\\mathbf{S^{(k)}}) $$\n",
    "\n",
    "A lánc szabály alkalmazásával a költségfüggvény parciális deriváltjai a $k$ réteg kimenetei szerint:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial \\mathbf{\\hat  Y}^{(k)}} = \\frac{\\partial C}{\\partial \\mathbf{\\hat  Y}^{(k+1)}} \\frac{\\partial \\mathbf{\\hat  Y}^{(k+1)}}{\\partial \\mathbf{S}^{(k+1)}} \\frac{\\partial \\mathbf{S}^{(k+1)}}{\\partial \\mathbf{X}^{(k+1)}} \\frac{\\partial \\mathbf{X}^{(k+1)}}{\\partial \\mathbf{\\hat Y}^{(k)}}$$\n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial \\mathbf{\\hat  Y}^{(k)}} = \\left( \\frac{\\partial C}{\\partial \\mathbf{\\hat  Y^{(k+1)}}} a'(\\mathbf{S^{(k+1)}}) \\cdot \\left(\\mathbf{W}^{(k+1)}_{-BIAS}\\right)^T \\right) = \\left( \\mathbf{\\delta^{(k+1)}} \\cdot \\left(\\mathbf{W}^{(k+1)}_{-BIAS}\\right)^T \\right)$$\n",
    "\n",
    "**Itt az $()_{-BIAS}$ tag azt jelképezi, hogy a BIAS neuronhoz tartozó megfelelő tagokat el kell távolítanunk a súlymátrixból!** Azok ugyanis a $k$-adik réteg BIAS neuronjához tartozó $\\delta$ érétkhez vezetnek, azonban annak nincsenek bemenetei, így súlyok sem tartoznak hozzá, 'zsákutcába' vezet.\n",
    "\n",
    "A fentiek alapján $\\mathbf{\\delta^{k}}$ teljes számítása:\n",
    "\n",
    "$$\\boxed{ \\mathbf{\\delta^{(k)}} = \\left( \\mathbf \\delta^{(k+1)} \\cdot \\left( \\mathbf W^{(k+1)}_{-BIAS} \\right)^T \\right) a'\\left(\\mathbf s^{(k)} \\right) }$$\n",
    "\n",
    "**Fontos megjegyezni, hogy az itt alkalmazott Mátrix szerint vett parciális derivált nem egy matematikalag definált művelet, itt alkalmazott jelentése a mátrix egyes elemei szertint vett parciális deriváltak, az eredeti mátrixnak megfelelő alakba rendezve.** A backpropagation teljes matematikai levezetését lásd az _Appendix_ részben.\n",
    "\n",
    "A fentiek alapján a BackProp algoritmus folyamata a háló minden súlyához tartozó gradiens kiszámítására:\n",
    " - Kimeneti réteghez tartozó $\\mathbf{\\delta}$ számítása (költségfüggvény és kimeneti réteg aktivációs függvénye alapján)\n",
    " - Rejtett rétegekhez tartozó $\\mathbf{\\delta}$ számítása hátulról előrefelé\n",
    " - Súlymódosítások mátrixának számítása minden rétegre a réteghez tartozó $\\mathbf{\\delta}$ és $\\mathbf{X}$ értékek alapján\n",
    " - Súlyok fissítése a számolt súlymódosításoknak és a tanulási rátának megfelelően\n",
    "\n",
    "A $\\mathbf{\\delta}$ értékek számításához szükséges az aktivációs függvény deriváltja is. Ez a _sigmoid_ függvény esetében:\n",
    "$$ \\boxed{ \\frac{d}{dz}sigmoid(z) = sigmoid(z)(1-sigmoid(z))} $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00: Könyvtár importálások\n",
    "\n",
    "Az első lépés a feladat megoldása során használt könyvtárak importálása. Esetünkben ezek a következők lesznek:\n",
    "- Numpy a matematikia műveletek elvégzéséhez\n",
    "- Pandas az adatok beolvasásához és kezeléséhez\n",
    "- MatPlotLib.pyplot az eredményeink ábrázolásához\n",
    "- Plotly Express interaktív vizualizációhoz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Használjuk ezeket sötét téma esetén\n",
    "plt.style.use('dark_background')\n",
    "styleTemplate = 'plotly_dark'\n",
    "\n",
    "# Használjuk ezeket világos téma esetén\n",
    "#plt.style.use('default')\n",
    "#styleTemplate = 'plotly_white'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01: Adatgenerálás\n",
    "Az eddigiekkel eltérő módon most nincsenek előre adott adatok. A XOR probléma adatstruktúrája viszonylag egyszerű, az adatok egyszerűen generálhatók, így tetszőleges számú tanító adatpont használható."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE XOR DATA\n",
    "nSamples = 100 # total samples = 4*nSamples\n",
    "clusters = [[-0.5, -0.5, 0], [-0.5, 0.5, 1], [0.5, -0.5, 1], [0.5, 0.5, 0]]\n",
    "std = 0.25\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "X = np.ones([4*nSamples, 2])\n",
    "Y = np.ones([4*nSamples, 1])\n",
    "for count, params in enumerate(clusters):\n",
    "    X[count*nSamples:(count+1)*nSamples, 0] = rng.normal(params[0], std, nSamples)\n",
    "    X[count*nSamples:(count+1)*nSamples, 1] = rng.normal(params[1], std, nSamples)\n",
    "    Y[count*nSamples:(count+1)*nSamples] = params[2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02: Adatfelfedezés\n",
    "\n",
    "A szintetikus, generált adatok esetén is érdemes előzetes adatfelfedezést / adatvizualizációt alkalmazni, így ellenőrizhető, hogy a geenrált adatok bizotsan megfelelnek a vártnak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "falseData = X[Y[:,0] == 0, :]\n",
    "trueData = X[Y[:,0] == 1, :]\n",
    "\n",
    "plt.scatter(falseData[:, 0], falseData[:, 1], marker='o', c=\"r\", label=\"False\")\n",
    "plt.scatter(trueData[:, 0], trueData[:, 1], marker='o', c=\"g\", label=\"True\")\n",
    "\n",
    "plt.title(\"Generált XOR data\")\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A generált adatk ebben az esetben már megfelelnek a tanításra, a tartományukból adódóan normalizálásra nincs szükség. A bias tag implementációja most a modellen belül történik majd."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03: Modell implementálása\n",
    "\n",
    "Először implementáljuk az aktivációs függvényként használt `sigmoid()` függvényt. Mivel a hálónk tanításához szükség lesz a sigmoid deriváltjának számítására is, az implementációnba ezt is belefoglaljuk."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feladat:** implementálja a `sigmoid()` aktivációs függvényt, amely második bementeként egy `bool` értéket vár, ami alapján vagy a *sigmoid* függvényt vagy annak a deriváltját számítja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z, derivate = False):       # Alapértelmezett a rendes sigmoid érték számítása\n",
    "######################################\n",
    "\n",
    "\n",
    "\n",
    "######################################\n",
    "    return g"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Az implementációt most vizuálisan kerül ellenőrzésre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-6, 6, 100)\n",
    "y = sigmoid(x)\n",
    "dy = sigmoid(x, derivate = True)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x = x, y = y, mode='lines', name ='sigmoid'))\n",
    "fig.add_trace(go.Scatter(x = x, y = dy, mode='lines', name = 'derivate of sigmoid'))\n",
    "\n",
    "fig.update_layout(\n",
    "    template='plotly_dark',\n",
    "    xaxis_title = \"z\",\n",
    "    title = \"Sigmoid függvény\",\n",
    "    width=600,\n",
    "    height=320,\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y = 1.02,\n",
    "        xanchor=\"center\",\n",
    "        x = 0.5)\n",
    ")\n",
    "\n",
    "fig.update_traces(line=dict( width=3))\n",
    "fig.update_xaxes(showgrid=True, gridwidth=0.5, gridcolor='grey', zeroline=True, zerolinewidth=3, zerolinecolor='grey')\n",
    "fig.update_yaxes(showgrid=True, gridwidth=0.5, gridcolor='grey', zeroline=True, zerolinewidth=3, zerolinecolor='grey')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Szintén külön függvényként definiáljuk a bemeneti mátrix biassal való kiegészítését."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addBias(z):\n",
    "    return np.hstack([np.ones([z.shape[0] ,1]), z])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias kiegészítés tesztelése:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = np.array([[0.3146, -0.65432, 0.24], [-1.0123, -0.4215, -0.12412], [0.2351, 0.7533456, 2.346]])\n",
    "print(addBias(testX))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neurális háló implementálásához és tanításához szükséges metódusok és adattagok most egy osztályba rendezve kerülnek implementálásra. A még üres metódusokat egy `pass` utasítással feltöltve elkerülhetjük, hogy a futtatás során hibs lépjen fel, így egyenként ellenőrizhetők a metódusok.\n",
    "\n",
    "**Feladat:** implementálja a súlyok incializálását véletlenszerű, -1 és 1 közötti egyeneletes eloszlásból származó értékekkel (használjon ehhez _list comprehension_ alakot)!\n",
    "\n",
    "**Feladat:** implementálja a háló `forwardProp()` metódusát, amely elvégzi az adott bemeneti adatokra a előreterjesztés lépését! A metódus töltse fel az osztály belső X[k] és Yhat[k] listákat a megfelelő értékekkel! Az implementáció ellenőrzésére használja a forwardProp tesztelő cellát az osztálydefiníció alatt.\n",
    "\n",
    "**Feladat:** implementálja a háló `backProp()` metódusát, amely a backPropagation módzser segítségével kiszámolja az egyes súlyokhoz tartozó gradienseket. Az implementáció ellenőrzésére használja a backProp tesztelő cellát az osztálydefiníció alatt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN:\n",
    "    def __init__(self, layerSizes, activationFunction, seed):\n",
    "        self.layerSizes = layerSizes            # Háló konfigurációja\n",
    "        self.a = activationFunction             # Aktivációs függvény (később más aktivációs függvény is alkalmazható)\n",
    "        self.noLayers = len(layerSizes)-1       # Rétegek száma (iterációhoz, bemeneti réteg nélkül)\n",
    "        self.inputSize = layerSizes[0]          # Bemeneti változók (input feature) száma\n",
    "        self.Yhat = []                          # Lista az egyes rétegek kimenetének tárolására\n",
    "        self.X = []                             # Lista az egyes rétegek bemenetének tárolására\n",
    "        self.W = []                             # Lista a rétegekhez tartozó súlyoknak\n",
    "        self.initWeights(seed)                  # Súlyok inicializálása\n",
    "    \n",
    "    def initWeights(self, seed = None):\n",
    "        rng = np.random.default_rng(seed)\n",
    "        # Weight matrix dimension are based on number of neurons in previous and current layer\n",
    "        ######################################\n",
    "        \n",
    "        ######################################\n",
    "\n",
    "    def checkInputSize(self, X):\n",
    "        if X.shape[1] != self.inputSize:\n",
    "            raise ValueError('Unexpected number of input features! Expected {} features but got {}.'.format(self.inputSize, X.shape[1])) \n",
    "\n",
    "    def forwardProp(self, X):\n",
    "        self.checkInputSize(X)\n",
    "\n",
    "        self.X = [[] for i in range(self.noLayers)]\n",
    "        self.Yhat = [[] for i in range(self.noLayers)]\n",
    "        \n",
    "        ######################################\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ######################################\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.forwardProp(X)\n",
    "        return self.Yhat[-1]\n",
    "\n",
    "    def costBCE(self, X, Y):\n",
    "        eps = 10e-15\n",
    "        self.forwardProp(X)\n",
    "        return np.mean(-Y*np.log(self.Yhat[-1]+eps)-(1-Y)*np.log(1-self.Yhat[-1]+eps))\n",
    "\n",
    "    def backProp(self, trueY):\n",
    "        ######################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ######################################\n",
    "        return dW\n",
    "\n",
    "    def updateWeights(self, learning_rate, dW):\n",
    "        for i in range(self.noLayers):\n",
    "            self.W[i] = self.W[i] - learning_rate * dW[i]\n",
    "\n",
    "    def fit(self, X, Y, learning_rate, epochs):\n",
    "        C_history = np.zeros([epochs+1])\n",
    "        C_history[0] = self.costBCE(X, Y)    # Forwardprop megtörténik\n",
    "        print('''\n",
    "        \\%\\%\\% ------- TANÍTÁS ------- \\%\\%\\%\n",
    "        ''')\n",
    "        for i in range(epochs):\n",
    "            dW = self.backProp(Y)\n",
    "            self.updateWeights(learning_rate, dW)\n",
    "            C_history[i+1] = self.costBCE(X,Y) # Forwardprop megtörténik\n",
    "\n",
    "            if ((i+1) % 250) == 0:\n",
    "                \n",
    "                print('Epoch {} / {} completed. Cost value:{}'.format(i+1, epochs, C_history[i+1])) \n",
    "\n",
    "        return C_history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teszteljük az egyes metódusokat, hogy megbizonyosodjunk az elvárt működésről!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layerSizes = [2, 3, 1]\n",
    "seed = 42\n",
    "\n",
    "testNN = FeedForwardNN(layerSizes, sigmoid, seed)\n",
    "\n",
    "testX = np.array([[0.3146, -0.65432], [-1.0123, -0.4215], [0.2351, 0.7533456]])\n",
    "testY = np.array([[0], [1], [1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test initweights\n",
    "print('''Expected initial weights with layer sizes [2, 3, 1] and random seed 42:\n",
    "[array([[ 0.5479121 , -0.12224312,  0.71719584],\n",
    "       [ 0.39473606, -0.8116453 ,  0.9512447 ],\n",
    "       [ 0.5222794 ,  0.57212861, -0.74377273]]), array([[-0.09922812],\n",
    "       [-0.25840395],\n",
    "       [ 0.85352998],\n",
    "       [ 0.28773024]])]''')\n",
    "print('''Actual initial weights with layer sizes {0} and random seed {1}:\n",
    "{2}'''.format(layerSizes, seed, testNN.W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forwardProp & Predict\n",
    "testPred = testNN.predict(testX)\n",
    "\n",
    "print('''Expected prediction for test parameters:\n",
    "[[0.56445555]\n",
    " [0.610119  ]\n",
    " [0.58247854]]'''.format(layerSizes, seed))\n",
    "print('''Actual prediction for test parameters:\n",
    "{0}'''.format(testPred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test backprop\n",
    "testNN.forwardProp(testX)\n",
    "testdW = testNN.backProp(testY)\n",
    "\n",
    "print('''Expected weight change values for test parameters:\n",
    "[array([[ 0.00351405, -0.02095111, -0.01093665],\n",
    "       [-0.0105734 ,  0.03069215,  0.00971723],\n",
    "       [ 0.00944793, -0.034079  , -0.0086122 ]]), array([[-0.0809823 ],\n",
    "       [-0.05584407],\n",
    "       [-0.09301554],\n",
    "       [ 0.00406624]])]''')\n",
    "\n",
    "print('''Actual prediction for test parameters:\n",
    "{0}'''.format(testdW))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04: Modell tanítása\n",
    "\n",
    "Amennyiben minden teszt megfelelő eredményt ad, a háló tanítása a `.fit()` metódus meghívásával történik. A tanítás lezajlása után a költségfüggvény alakulása ábrázolható."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.5\n",
    "trainNN = FeedForwardNN([2, 10, 1], sigmoid, 42)\n",
    "\n",
    "C_history = trainNN.fit(X, Y, learning_rate, 3000)\n",
    "plt.plot(range(C_history.size), C_history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05: Modell értékelése"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A beépített `predict()` függvénynek köszönhetően vizualizálható az háló döntési stratégiája: vizsgálható a működés 2D-ben kontúrvonalakkal, vagy 3D-ben a teljes illesztett felületet ábrázolva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "falseData = X[Y[:,0] == 0, :]\n",
    "trueData = X[Y[:,0] == 1, :]\n",
    "\n",
    "plt.scatter(falseData[:, 0], falseData[:, 1], marker='o', c=\"r\", label=\"False\")\n",
    "plt.scatter(trueData[:, 0], trueData[:, 1], marker='o', c=\"g\", label=\"True\")\n",
    "\n",
    "x1 = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 200)    # grid létrehozása\n",
    "x2 = np.linspace(np.min(X[:, 1]), np.max(X[:, 1]), 200)    # második paraméter\n",
    "\n",
    "z=np.zeros((len(x1),len(x2)))                          # eredményváltozó 1 inicializálása\n",
    "\n",
    "for i in range(len(x1)):                                 # valószínűség számolása a teljes háló felett\n",
    "    for j in range(len(x2)):     \n",
    "        testPoint = np.array([[x1[i], x2[j]]])\n",
    "        z[i,j] = trainNN.predict(testPoint)\n",
    "\n",
    "plt.contour(x1, x2,z.transpose(), 3)                                  # kirajzoljuk contour plottal a döntési határt                                # kirajzoljuk contour plottal a döntési határt\n",
    "\n",
    "\n",
    "plt.title(\"XOR becslés\")\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A beépített `predict()` fügvénynek köszönhetően a teljes illesztett felületet is vizsgálható."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ábrázolás Plotly-val\n",
    "fig = go.Figure()\n",
    "\n",
    "# A magyarázott változót transzponálni kell a helyes megjelenítésért.\n",
    "fig.add_trace(go.Scatter3d(x=X[:,0], y=X[:,1], z=Y[:,0], mode= \"markers\"))\n",
    "fig.add_trace(go.Surface(x=x1, y=x2, z=z.T, colorscale ='Blues'))\n",
    "\n",
    "#Plot formázása\n",
    "fig.update_layout(\n",
    "    title = \"XOR becslés\",\n",
    "    scene = dict(\n",
    "        xaxis_title = \"x1\",\n",
    "        yaxis_title = \"x2\",\n",
    "        zaxis_title = \"Prob\"),\n",
    "    template=styleTemplate,\n",
    "    width=750,\n",
    "    height=500,\n",
    ")\n",
    "\n",
    "#Plot megjelenítése\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06: További tesztelés\n",
    "\n",
    "A gyakorlat keretében a tesztelés csak az illesztett felület vizualizálására, illetve a költség alakulására korlátozódott. Az eddig tanult ismeretek alapján érdemes egyéni munkában a hálót módosítani, kiegészítgetni, pl.: accuracy számolás implementálása és nyomonkövetése a tanulás során, animáció készítés a döntési határ alakulásáról a tanulás során, vagy a regularizációs technikák implementálása és hatásuknak vizsgálata."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XX: Megoldás Keras könyvtár segítségével\n",
    "\n",
    "Az egyik legelterjedteb deep learning könyvtár a google által fejlesztett Tensorflow, amelyre épül a Keras mint magasabb absztrakciós szintű wrapper. A fenti XOR probléma megoldása a Keras könyvtárral pár sorral megoldható, a könyvtárban implementált függvények jól optimalizáltak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tömbkezelés és ábrázolás\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Keras modulok importálása\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# GENERATE XOR DATA\n",
    "nSamples = 100 # total samples = 4*nSamples\n",
    "clusters = [[-0.5, -0.5, 0], [-0.5, 0.5, 1], [0.5, -0.5, 1], [0.5, 0.5, 0]]\n",
    "std = 0.25\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "X = np.ones([4*nSamples, 2])\n",
    "Y = np.ones([4*nSamples, 1])\n",
    "for count, params in enumerate(clusters):\n",
    "    X[count*nSamples:(count+1)*nSamples, 0] = rng.normal(params[0], std, nSamples)\n",
    "    X[count*nSamples:(count+1)*nSamples:,1] = rng.normal(params[1], std, nSamples)\n",
    "    Y[count*nSamples:(count+1)*nSamples] = params[2]\n",
    "\n",
    "# Keras háló definiálása\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_shape=(2,)))\n",
    "model.add(Activation('sigmoid'))                            \n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# compiling the sequential model\n",
    "model.compile(loss='binary_crossentropy', metrics=['binary_crossentropy'], optimizer=SGD(learning_rate=0.5))\n",
    "\n",
    "# training the model and saving metrics in history\n",
    "history = model.fit(X, Y, epochs=500, verbose = 2)\n",
    "\n",
    "# plotting the metrics\n",
    "fig = plt.figure()\n",
    "plt.plot(history.history['binary_crossentropy'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('binary_crossentropy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'])\n",
    "\n",
    "# plotting fitted surface\n",
    "x1 = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 200)    # grid létrehozása\n",
    "x2 = np.linspace(np.min(X[:, 1]), np.max(X[:, 1]), 200)    # második paraméter\n",
    "\n",
    "z=np.zeros((len(x1),len(x2)))                          # eredményváltozó 1 inicializálása\n",
    "testPoints = np.zeros([len(x1)*len(x2), 2])\n",
    "for i in range(len(x1)):                                 # valószínűség számolása a teljes háló felett\n",
    "    for j in range(len(x2)):     \n",
    "        testPoints[i*len(x1)+j,:] = np.array([[x1[i], x2[j]]])\n",
    "\n",
    "z = model.predict(testPoints, verbose = 0).reshape([len(x1), len(x2)])\n",
    "\n",
    "# Ábrázolás Plotly-val\n",
    "fig = go.Figure()\n",
    "\n",
    "# A magyarázott változót transzponálni kell a helyes megjelenítésért.\n",
    "fig.add_trace(go.Scatter3d(x=X[:,0], y=X[:,1], z=Y[:,0], mode= \"markers\"))\n",
    "fig.add_trace(go.Surface(x=x1, y=x2, z=z.T, colorscale ='Blues'))\n",
    "\n",
    "#Plot formázása\n",
    "fig.update_layout(\n",
    "    title = \"XOR becslés\",\n",
    "    scene = dict(\n",
    "        xaxis_title = \"x1\",\n",
    "        yaxis_title = \"x2\",\n",
    "        zaxis_title = \"Prob\"),\n",
    "    template='plotly_dark',\n",
    "    width=750,\n",
    "    height=500,\n",
    ")\n",
    "\n",
    "#Plot megjelenítése\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix - BackProp levezetése\n",
    "\n",
    "### Alapfeltevések\n",
    "\n",
    "Neurális hálók esetében a _'gradiensvektor'_ fogalma kicsit megtévesztő lehet, mivel az adott rétegtől függően nem feltétlen vektor alakot vesz fel. Egy rejtett réteg esetén, ahol több kimenet és bement van, a _'gradiensvketort'_ mátrixos alakban határozzuk meg, a súlyoknak megfelelő alakban. Ha az adott réteg egyetlen perceptronjához tartozó súlyokat nézzük, ezekhez egy-egy valóban (oszlop)vektor tartozik, ezeket egymás mellé helyezve kapjuk az adott réteg _gradiensét_. Általánosságban a gradiens alatt a célfüggvény minden módosítandó paraméter szerinti parciális deriváltját értjük, amely parciális deriváltakat a számításhoz megfelelő alakba rendezünk."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiens a kimenti réteg súlyaira\n",
    "\n",
    "Az egy neuront tartalmazó kimeneti ($p$) réteg esetén a költségfüggvény egy adott súlyhoz tartozó parciális deriváltja a láncszabályt alkalmazva az alábbi módon számítható ki (az $i$ index a réteg $i$-edik bementi változóját, az $l$ index pedig az $l$-edik adatpontot jelzi):\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial w^{(p)}_{i}} = \\sum_{l=1}^m \\left(\n",
    "\\frac{\\partial C}{\\partial \\hat  y_l} \\frac{\\partial \\hat y_l}{\\partial \\hat y^{(p)}_l} \\frac{\\partial \\hat y^{(p)}_l}{\\partial s^{(p)}_l} \\frac{\\partial s^{(p)}_l}{\\partial w^{(p)}_{i}} \\right)$$\n",
    "\n",
    "Az egyes parciális deriváltakat külön megnézve:\n",
    "\n",
    "$$\\frac{\\partial s^{(p)}_l}{\\partial w^{(p)}_{i}}: \\quad s^{(p)}_l = w^{(p)}_{0} + w^{(p)}_{1}x^{(p)}_{l,1} + w^{(k)}_{2}x^{(p)}_{l,2} + ... + w^{(p)}_{i}x^{(p)}_{l,i} + ... w^{(p)}_{n(p-1)}x^{(p)}_{l,n(p-1)} \\Rightarrow \\frac{\\partial s^{(p)}}{\\partial w^{(p)}_{i}} = x^{(p)}_{l,i}$$\n",
    "\n",
    "( $n(p-1)$ az utolsó előtti  réteg kimeneteinek száma)\n",
    "\n",
    "$$\\frac{\\partial \\hat y^{(p)}_l}{\\partial s^{(p)}_l}: \\quad \\hat y^{(p)}_l = a(s^{(p)}_l) \\Rightarrow \\frac{\\partial \\hat y^{(p)}_l}{\\partial s^{(p)}_l} = a'(s^{(p)}_l)$$\n",
    "\n",
    "$$\\frac{\\partial \\hat y_l}{\\partial \\hat y^{(p)}_l}: \\quad \\hat y_l = \\hat y^{(p)}_l \\Rightarrow \\frac{\\partial \\hat y_l}{\\partial \\hat y^{(p)}_l} = 1$$\n",
    "\n",
    "A teljes parciális derivált így egy általános $C$ költségfüggvényre és $a()$ aktivációs függvényre:\n",
    "$$ \\frac{\\partial C}{\\partial w^{(p)}_{i}} = \\sum_{l=1}^m \\left( \\frac{\\partial C}{\\partial \\hat  y_l} a'(s^{(p)}_l) x^{(p)}_{l,i} \\right)$$\n",
    "\n",
    "Jelölje $\\mathbf{X}^{(p)}_i$ a $p$-edik réteg $i$-edik bemeneti változójának összes $m$ darab adatpontját tartalmazó oszlopvektorát ($[m \\times 1]$-es mátrix). $\\mathbf{\\hat Y}$ és $\\mathbf{S}^{(p)}$ az eddigi jelöléseknek megfelelően szintén $[m \\times 1]$-es oszlopvektorok, így $a'(\\mathbf{S}^{(p)})$ szintén oszlopvektor, míg $\\frac{\\partial C}{\\partial \\mathbf{\\hat  Y}}$ a költségfüggvény egyes adatpontokra vett kimenetek szerinti parciális deriváltjiaból összeállított oszlopvektor (a kimenetek szerint vett gradiensvektor). \n",
    "\n",
    "$$\\frac{\\partial C}{\\partial \\mathbf{\\hat  Y}} =\n",
    "\\left[\n",
    "\t\\begin{array}{c}\n",
    " \t\t\\frac{\\delta C}{\\delta \\hat{y}_1}\\\\\n",
    "\t\t\\frac{\\delta C}{\\delta \\hat{y}_2}\\\\\n",
    " \t\t\\vdots\\\\\n",
    "        \\frac{\\delta C}{\\delta \\hat{y}_m}\\\\\n",
    "\t\\end{array}\\right]\n",
    "\n",
    "\\quad\n",
    "\n",
    "a'(\\mathbf{S}^{(p)}) =\n",
    "\\left[\n",
    "\t\\begin{array}{c}\n",
    " \t\ta'(s^{(p)}_1)\\\\\n",
    "\t\ta'(s^{(p)}_2)\\\\\n",
    " \t\t\\vdots\\\\\n",
    "        a'(s^{(p)}_m)\\\\\n",
    "\t\\end{array}\n",
    "\\right] \n",
    "\n",
    "\\quad\n",
    "\n",
    "\\mathbf{X}^{(p)}_i =\n",
    "\\left[\n",
    "\t\\begin{array}{c}\n",
    " \t\tx^{(p)}_{1,i}\\\\\n",
    "\t\tx^{(p)}_{2,i}\\\\\n",
    " \t\t\\vdots\\\\\n",
    "        x^{(p)}_{m,i}\\\\\n",
    "\t\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "\n",
    "Ekkor a $w^{(p)}_{i}$ súly szerint vett parciális derivált számítása az adatpontonként vett szummázás helyett az alábbi mátrixműveletes alakban írható fel:\n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial w^{(p)}_{i}} = \\left(\\mathbf{X}^{(p)}_i\\right)^T \\cdot \\frac{\\partial C}{\\partial \\mathbf{\\hat Y}} a'(\\mathbf{S}^{(p)})$$\n",
    "\n",
    "A kimeneti réteghez tartozó teljes súlyvektorra nézve a gradiensvektor a költségfüggvény súlyonként vett parciális deriváltja így:\n",
    "\n",
    "$$\\nabla_{(p)} C = \n",
    "\\left[\n",
    "\t\\begin{array}{c}\n",
    " \t\t\\frac{\\delta C}{\\delta w^{(p)}_0}\\\\\n",
    "\t\t\\frac{\\delta C}{\\delta w^{(p)}_1}\\\\\n",
    "        \\frac{\\delta C}{\\delta w^{(p)}_2}\\\\\n",
    " \t\t\\vdots\\\\\n",
    "        \\frac{\\delta C}{\\delta w^{(p)}_n}\\\\\n",
    "\t\\end{array}\\right] \n",
    "= \\left(\\mathbf{X}^{(p)}\\right)^T \\cdot \\frac{\\partial C}{\\partial \\mathbf{\\hat Y}} a'(\\mathbf{S}^{(p)})$$\n",
    "\n",
    "Bevezetve a\n",
    "\n",
    "$$\\mathbf{\\delta}^{(k)} = \\frac{\\partial C}{\\partial \\mathbf{\\hat Y}^{(k)}} a'(\\mathbf{S}^{(k)})$$ \n",
    "\n",
    "változót a kimeneti réteghez tartozó gradiens:\n",
    "\n",
    "$$ \\nabla_{(p)} C = \\left(\\mathbf{X}^{(p)}\\right)^T \\cdot \\mathbf{\\delta}^{(p)}$$\n",
    "\n",
    "---\n",
    "\n",
    "$\\mathbf{\\delta}^{(p)}$ a BCE költségfüggvény és sigmoid aktivációs függvény esetében:\n",
    "\n",
    "$$ \\boxed{ a'(z) = \\frac{d}{dz}sigmoid(z) = sigmoid(z)(1-sigmoid(z)) }$$\n",
    "$$ a'(s^{(p)}_l) = sigmoid(s^{(p)}_l)(1-sigmoid(s^{(p)}_l)) = \\hat y^{(p)}_l(1- \\hat y^{(p)}_l) = \\hat y_l(1- \\hat y_l)$$\n",
    "$$ a'(\\mathbf{S}^{(p)}) =\n",
    "\\left[\n",
    "\t\\begin{array}{c}\n",
    " \t\ta'(s^{(p)}_1)\\\\\n",
    "\t\ta'(s^{(p)}_2)\\\\\n",
    " \t\t\\vdots\\\\\n",
    "        a'(s^{(p)}_m)\\\\\n",
    "\t\\end{array}\n",
    "\\right] = \n",
    "\\left[\n",
    "\t\\begin{array}{c}\n",
    " \t\t\\hat y_1(1- \\hat y_l)\\\\\n",
    "\t\t\\hat y_2(1- \\hat y_2)\\\\\n",
    " \t\t\\vdots\\\\\n",
    "        \\hat y_m(1- \\hat y_m)\\\\\n",
    "\t\\end{array}\n",
    "\\right] = \n",
    "\\mathbf{\\hat Y} (1 - \\mathbf{\\hat Y})$$\n",
    "\n",
    "$$ \\\\[50pt] $$\n",
    "\n",
    "$$ \\frac{\\partial C_{BCE}}{\\partial \\hat  y_l} = \\frac{\\partial \\left( \\frac{1}{m} \\sum_{l=1}^m \\left(-y_l \\ln (\\hat{y}_l) - (1-y_l) \\ln \\left(1-\\hat{y}_l \\right) \\right) \\right)}{\\partial \\hat  y_{l=l_0}} = \\frac{1}{m} \\left( -y_l \\frac{1}{\\hat{y}_l} + (1-y_l)\\frac{1}{1-\\hat{y}_l}\\right)$$\n",
    "$$ \\frac{\\partial C_{BCE}}{\\partial \\mathbf{\\hat Y}} = \\frac{1}{m} \\left( -\\mathbf{Y} \\frac{1}{\\mathbf{\\hat Y}} + (1-\\mathbf{Y})\\frac{1}{1-\\mathbf{\\hat Y}}\\right)$$\n",
    "\n",
    "$$ \\\\[50pt] $$\n",
    "\n",
    "$$\\mathbf{\\delta}^{(p)} = \\frac{\\partial C_{BCE}}{\\partial \\mathbf{\\hat Y}} a'(\\mathbf{S}^{(p)}) = \\frac{1}{m} \\left( -\\mathbf{Y} \\frac{1}{\\mathbf{\\hat Y}} + (1-\\mathbf{Y})\\frac{1}{1-\\mathbf{\\hat Y}}\\right) \\mathbf{\\hat Y} (1 - \\mathbf{\\hat Y}) = \\frac{1}{m} \\left( -\\mathbf{Y} (1 - \\mathbf{\\hat Y}) + (1-\\mathbf{Y})\\mathbf{\\hat Y}\\right) = \\\\\n",
    "= \\frac{1}{m} \\left(  -\\mathbf{Y} + \\mathbf{Y} \\mathbf{\\hat Y} + \\mathbf{\\hat Y} - \\mathbf{Y} \\mathbf{\\hat Y} \\right) = \\frac{1}{m} \\left( \\mathbf{\\hat Y} -\\mathbf{Y} \\right)$$\n",
    "\n",
    "---\n",
    "\n",
    "A teljes gradiens a kimeneti réteg súlyaira így:\n",
    "\n",
    "$$ \\boxed{ \\nabla_{(p)} C = \\left(\\mathbf{X}^{(p)}\\right)^T \\cdot \\mathbf{\\delta}^{(p)} = \\frac{1}{m} \\left(\\mathbf{X}^{(p)}\\right)^T \\cdot \\left( \\mathbf{\\hat Y} -\\mathbf{Y} \\right) = \\frac{1}{m} \\left(\\mathbf{X}^{(p)}\\right)^T \\cdot \\left( sigmoid\\left( \\mathbf{X}^{(p)} \\cdot \\mathbf{W}^{(p)} \\right) -\\mathbf{Y} \\right)}$$\n",
    "\n",
    "Visszakaptuk tehát a logisztikus regressziónál használt gradienst."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiens egy rejtett réteg súlyaira\n",
    "\n",
    "A több neuront tartalmazó rejtett ($k$) réteg esetén a költségfüggvény egy adott súlyhoz tartozó parciális deriváltja a láncszabályt alkalmazva az alábbi módon számítható ki (az $i$ index a réteg $i$-edik bementi változóját, a $j$ index a réteg $j$-edik kimenetét, az $l$ index pedig az $l$-edik adatpontot jelzi):\n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial w^{(k)}_{i,j}} = \\sum_{l=1}^m \\left(\n",
    "\\frac{\\partial C}{\\partial \\hat  y^{(k)}_{l,j}} \\frac{\\partial \\hat y^{(k)}_{l,j}}{\\partial s^{(k)}_{l,j}} \\frac{\\partial s^{(k)}_{l,j}}{\\partial w^{(k)}_{i,j}} \\right)$$\n",
    "\n",
    "Az egyes parciális deriváltakat külön megnézve:\n",
    "\n",
    "$$\\frac{\\partial s^{(k)}_{l,j}}{\\partial w^{(k)}_{i,j}}: \\quad s^{(k)}_{l,j} = w^{(k)}_{0,j} + w^{(k)}_{1,j}x^{(k)}_{l,1} + w^{(k)}_{2,j}x^{(k)}_{l,2} + ... + w^{(k)}_{i,j}x^{(k)}_{l,i} + ... w^{(k)}_{n(k-1),j}x^{(k)}_{l,n(k-1)} \\Rightarrow \\frac{\\partial s^{(k)}_{l,j}}{\\partial w^{(k)}_{i,j}} = x^{(k)}_{l,i}$$\n",
    "\n",
    "( $n(p-1)$ az utolsó előtti  réteg kimeneteinek száma)\n",
    "\n",
    "$$\\frac{\\partial \\hat y^{(k)}_{l,j}}{\\partial s^{(k)}_{l,j}}: \\quad \\hat y^{(k)}_{l,j} = a(s^{(k)}_{l,j}) \\Rightarrow \\frac{\\partial \\hat y^{(k)}_{l,j}}{\\partial s^{(k)}_{l,j}} = a'(s^{(k)}_{l,j})$$\n",
    "\n",
    "A teljes parciális derivált így egy általános $C$ költségfüggvényre és $a()$ aktivációs függvényre:\n",
    "$$ \\frac{\\partial C}{\\partial w^{(k)}_{i,j}} = \\sum_{l=1}^m \\left( \\frac{\\partial C}{\\partial \\hat  y^{(k)}_{l,j}} a'(s^{(k)}_{l,j}) x^{(k)}_{l,i} \\right)$$\n",
    "\n",
    "Jelölje $\\mathbf{X}^{(k)}_i$ a $k$-adik réteg $i$-edik bemeneti változójának összes $m$ darab adatpontját tartalmazó oszlopvektorát ($[m \\times 1]$-es mátrix). $\\mathbf{\\hat Y^{(k)}_j}$ és $\\mathbf{S}^{(k)}_j$ minden adatponra a $k$-adik réteg $j$-edik neuronjához tartozó szintén $[m \\times 1]$-es oszlopvektorok. Így $a'(\\mathbf{S}^{(p)}_j)$ szintén oszlopvektor, míg $\\frac{\\partial C}{\\partial \\mathbf{\\hat Y^{(k)}_j}}$ a költségfüggvény egyes adatpontokra vett $j$-edik neuron kimenete szerinti parciális deriváltjiaból összeállított oszlopvektor. \n",
    "\n",
    "$$\\frac{\\partial C}{\\partial \\mathbf{\\hat Y^{(k)}_j}} =\n",
    "\\left[\n",
    "\t\\begin{array}{c}\n",
    " \t\t\\frac{\\delta C}{\\delta \\hat{y}^{(k)}_{1,j}}\\\\\n",
    "\t\t\\frac{\\delta C}{\\delta \\hat{y}^{(k)}_{2,j}}\\\\\n",
    " \t\t\\vdots\\\\\n",
    "        \\frac{\\delta C}{\\delta \\hat{y}^{(k)}_{m,j}}\\\\\n",
    "\t\\end{array}\\right]\n",
    "\n",
    "\\quad\n",
    "\n",
    "a'(\\mathbf{S}^{(k)}_j) =\n",
    "\\left[\n",
    "\t\\begin{array}{c}\n",
    " \t\ta'(s^{(k)}_{1,j})\\\\\n",
    "\t\ta'(s^{(k)}_{2,j})\\\\\n",
    " \t\t\\vdots\\\\\n",
    "        a'(s^{(k)}_{m,j})\\\\\n",
    "\t\\end{array}\n",
    "\\right] \n",
    "\n",
    "\\quad\n",
    "\n",
    "\\mathbf{X}^{(k)}_i =\n",
    "\\left[\n",
    "\t\\begin{array}{c}\n",
    " \t\tx^{(k)}_{1,i}\\\\\n",
    "\t\tx^{(k)}_{2,i}\\\\\n",
    " \t\t\\vdots\\\\\n",
    "        x^{(k)}_{m,i}\\\\\n",
    "\t\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "\n",
    "Ekkor a $w^{(k)}_{i,j}$ súly szerint vett parciális derivált számítása az adatpontonként vett szummázás helyett az alábbi mátrixműveletes alakban írható fel:\n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial w^{(k)}_{i,j}} = \\left(\\mathbf{X}^{(k)}_i\\right)^T \\cdot \\frac{\\partial C}{\\partial \\mathbf{\\hat Y^{(k)}_j}} a'(\\mathbf{S}^{(k)}_j)$$\n",
    "\n",
    "A réteg $j$-edik neuronjához tartozó teljes súlyvektorra nézve a gradiensvektor a költségfüggvény súlyonként vett parciális deriváltja így megkapható, ha a teljes bemeneti $X^{(k)}$ mátrixot vesszük annak csak az $i$-edik sora helyett:\n",
    "\n",
    "$$\\nabla_{(pk, j)} C = \n",
    "\\left[\n",
    "\t\\begin{array}{c}\n",
    " \t\t\\frac{\\delta C}{\\delta w^{(k)}_{0,j}}\\\\\n",
    "\t\t\\frac{\\delta C}{\\delta w^{(k)}_{1,j}}\\\\\n",
    "        \\frac{\\delta C}{\\delta w^{(k)}_{2,j}}\\\\\n",
    " \t\t\\vdots\\\\\n",
    "        \\frac{\\delta C}{\\delta w^{(k)}_{n(k-1),j}}\\\\\n",
    "\t\\end{array}\\right] \n",
    "= \\left(\\mathbf{X}^{(p)}\\right)^T \\cdot \\frac{\\partial C}{\\partial \\mathbf{\\hat Y^{(k)}_j}}  a'(\\mathbf{S}^{(k)}_j)$$\n",
    "\n",
    "A réteg teljes súlymátrixának minden eleméhez tartozó parciális deriváltak innen megkaphatók, ha a fenti képletet minden kimeneti neuronna kiszámítjuk. Amennyibenaz $\\mathbf{\\hat Y^{(k)}_j}$ és $a'(\\mathbf{S}^{(k)}_j)$ oszlopok helyett a teljes $\\mathbf{\\hat Y^{(k)}}$ és $a'(\\mathbf{S}^{(k)})$ mátrixokat vesszük, egy művelettel megkaphatjuk a teljes súlymátrixra vett gradienst:\n",
    "\n",
    "$$\\nabla_{(k)} C = \n",
    "\\left[\n",
    "\t\\begin{array}{cccc}\n",
    " \t\t\\frac{\\delta C}{\\delta w^{(k)}_{0,1}} & \\frac{\\delta C}{\\delta w^{(k)}_{0,2}} & \\ldots & \\frac{\\delta C}{\\delta w^{(k)}_{0,n(k)}}\\\\\n",
    "\t\t\\frac{\\delta C}{\\delta w^{(k)}_{1,1}} & \\frac{\\delta C}{\\delta w^{(k)}_{1,2}} & \\ldots & \\frac{\\delta C}{\\delta w^{(k)}_{1,n(k)}}\\\\\n",
    "       \\frac{\\delta C}{\\delta w^{(k)}_{2,1}} & \\frac{\\delta C}{\\delta w^{(k)}_{2,2}} & \\ldots & \\frac{\\delta C}{\\delta w^{(k)}_{2,n(k)}}\\\\\n",
    " \t\t\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "        \\frac{\\delta C}{\\delta w^{(k)}_{n(k-1),1}} & \\frac{\\delta C}{\\delta w^{(k)}_{n(k-1),2}} & \\ldots & \\frac{\\delta C}{\\delta w^{(k)}_{n(k-1),n(k)}}\\\\\n",
    "\t\\end{array}\\right] \n",
    "= \\left(\\mathbf{X}^{(k)}\\right)^T \\cdot \\frac{\\partial C}{\\partial \\mathbf{\\hat Y^{(k)}}}  a'(\\mathbf{S}^{(k)})$$\n",
    "\n",
    "A korábban bevezetett\n",
    "\n",
    "$$ \\boxed{\\mathbf{\\delta}^{(k)} = \\frac{\\partial C}{\\partial \\mathbf{\\hat Y}^{(k)}} a'(\\mathbf{S}^{(k)})}$$ \n",
    "\n",
    "változóval felírva:\n",
    "\n",
    "$$ \\boxed{ \\nabla_{(k)} C = \\left(\\mathbf{X}^{(k)}\\right)^T \\cdot \\mathbf{\\delta}^{(k)} }$$\n",
    "\n",
    "---\n",
    "\n",
    "A $\\mathbf{\\delta}^{(k)}$ mátrix egy elemének számítása:\n",
    "\n",
    "$$ \\delta^{(k)}_{l,j} = \\frac{\\partial C}{\\partial \\hat y^{(k)}_{l,j}} a'(s^{(k)}_{l,j})$$ \n",
    "\n",
    "A $\\frac{\\partial C}{\\partial \\hat y^{(k)}_{l,j}}$ parciális derivált felírása a lánc szabály segítségével:\n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial \\hat y^{(k)}_{l,j}} = \\sum_{j_{+1}=1}^{n(k+1)}\\left(\n",
    "\\frac{\\partial C}{\\partial \\hat  y^{(k+1)}_{l,j_{+1}}} \\frac{\\partial \\hat y^{(k+1)}_{l,j_{+1}}}{\\partial s^{(k+1)}_{l,j_{+1}}} \\frac{\\partial s^{(k+1)}_{l,j_{+1}}}{\\partial x^{(k+1)}_{l,i_{+1}}} \\frac{\\partial x^{(k+1)}_{l,i_{+1}}}{\\partial \\hat y^{(k)}_{l,j}} \\right)$$\n",
    "\n",
    "Az egyes parciális deriváltakat külön megnézve:\n",
    "\n",
    "$$ \\frac{\\partial x^{(k+1)}_{l,i_{+1}}}{\\partial \\hat y^{(k)}_{l,j}}: \\quad x^{(k+1)}_{l,i_{+1}} = \n",
    "\\begin{cases}\n",
    "    1, & \\text{ha}\\ i_{+1} = 0 \\\\\n",
    "    \\hat y^{(k)}_{l,i_{+1}}, & \\text{ha}\\ i_{+1} > 0\n",
    "\\end{cases} \\Rightarrow \\frac{\\partial x^{(k+1)}_{l,i_{+1}}}{\\partial \\hat y^{(k)}_{l,j}} = \n",
    "\\begin{cases}\n",
    "    0, & \\text{ha}\\ i_{+1} \\neq j \\\\\n",
    "    1, & \\text{ha}\\ i_{+1} = j \\ \\text{ahol} \\ j=1,2,...n(k) \n",
    "\\end{cases}$$ \n",
    "\n",
    "$$\\frac{\\partial s^{(k+1)}_{l,j_{+1}}}{\\partial x^{(k+1)}_{l,i_{+1}}}: \\quad s^{(k)}_{l,j} = w^{(k)}_{0,j} + w^{(k)}_{1,j}x^{(k)}_{l,1} + w^{(k)}_{2,j}x^{(k)}_{l,2} + ... + w^{(k)}_{i,j}x^{(k)}_{l,i} + ... w^{(k)}_{n(k-1),j}x^{(k)}_{l,n(k-1)} \\Rightarrow \\frac{\\partial s^{(k+1)}_{l,j_{+1}}}{\\partial x^{(k+1)}_{l,i_{+1}}} = w^{(k+1)}_{i_{+1},j_{+1}}$$\n",
    "\n",
    "$$\\frac{\\partial \\hat y^{(k+1)}_{l,j_{+1}}}{\\partial s^{(k+1)}_{l,j_{+1}}}: \\quad \\hat y^{(k)}_{l,j} = a(s^{(k)}_{l,j}) \\Rightarrow \\frac{\\partial \\hat y^{(k+1)}_{l,j_{+1}}}{\\partial s^{(k+1)}_{l,j_{+1}}} = a'(s^{(k+1)}_{l,j_{+1}})$$\n",
    "\n",
    "A teljes $\\hat y^{(k)}_{l,j}$ szerinti parciális derivált így egy általános $C$ költségfüggvényre és $a()$ aktivációs függvényre:\n",
    "$$ \\frac{\\partial C}{\\partial \\hat y^{(k)}_{l,j}} = \\sum_{j_{+1}=1}^{n(k+1)}\\left( \\frac{\\partial C}{\\partial \\hat  y^{(k+1)}_{l,j_{+1}}} a'(s^{(k+1)}_{l,j_{+1}}) w^{(k+1)}_{j,j_{+1}} \\right) = \\sum_{j_{+1}=1}^{n(k+1)}\\left( \\delta^{(k+1)}_{l,j_{+1}} w^{(k+1)}_{j,j_{+1}} \\right)$$\n",
    "\n",
    "Jelölje $\\mathbf{W}^{(k+1)}_j$ a $k+1$-edik réteg $j$-edik bemeneti változójához tartozó összes $n(k+1)$ darab súlyát tartalmazó sorvektorát ($[1 \\times n(k+1)]$-es mátrix), és $\\mathbf{\\delta}^{(k+1)}_l$ a $k+1$-edik réteg $l$ bemeneti adatponthoz tartozó összes $n(k+1)$ darab $\\delta$ értékét (szintén $[1 \\times n(k+1)]$-es sorvektor). Ekkor a fenti szummázott érték megkapható az alábbi mátrixszorzat segítségével:\n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial \\hat y^{(k)}_{l,j}} = \\mathbf{\\delta}^{(k+1)}_l \\cdot \\left(\\mathbf{W}^{(k+1)}_j\\right)^T$$\n",
    "\n",
    "Innen a $\\mathbf{\\delta}^{(k)}$ mátrix egy elemének számítása:\n",
    "\n",
    "$$ \\delta^{(k)}_{l,j} = \\frac{\\partial C}{\\partial \\hat y^{(k)}_{l,j}} a'(s^{(k)}_{l,j}) = \\left(\\mathbf{\\delta}^{(k+1)}_l \\cdot \\left(\\mathbf{W}^{(k+1)}_j\\right)^T \\right) a'(s^{(k)}_{l,j})$$\n",
    "\n",
    "A $k$-adik réteg $j$-edik kimenetének összes adatpont szerinti $\\delta$ értéke megkapható az egy adatpont szerinti $\\hat y^{(k)}_{l,j}$ és $s^{(k)}_{l,j}$ skalárok illetve a $\\mathbf{\\delta}^{(k+1)}_l$ sormátrix helyett a $\\hat y^{(k)}_{j}$ és $\\mathbf{s}^{(k)}_{j}$ oszlopmátrixok és a teljes $\\mathbf{\\delta}^{(k+1)}$ mátrix használatával:\n",
    "\n",
    "$$ \\mathbf{\\delta}^{(k)}_{j} = \\frac{\\partial C}{\\partial \\mathbf{\\hat Y}^{(k)}_{j}} a'(\\mathbf{S}^{(k)}_{j}) = \\left( \\mathbf{\\delta}^{(k+1)} \\cdot \\left(\\mathbf{W}^{(k+1)}_j\\right)^T \\right)a'(\\mathbf{S}^{(k)}_{j})$$\n",
    "\n",
    "A $k$-adik réteg összes kimenetének összes adatpont szerinti $\\delta$ értéke megkapható megkapható, amennyiben az egy kimenet szerinti $\\mathbf{\\hat Y}^{(k)}_{j}$ és $\\mathbf{S}^{(k)}_{j}$ oszlopmátrix és $\\mathbf{W}^{(k+1)}_j$ sormátrix helyett a teljes $\\mathbf{\\hat Y}^{(k)}$ és $\\mathbf{S}^{(k)}$ máétrixok, illetve a *BIAS-hoz tartozó sor kivételével* a teljes $\\mathbf{W}^{(k+1)}_{-BIAS}$ súlymátrix használatával. \n",
    "\n",
    "$$ \\boxed{\\mathbf{\\delta}^{(k)} = \\frac{\\partial C}{\\partial \\mathbf{\\hat Y}^{(k)}} a'(\\mathbf{S}^{(k)}) = \\left( \\mathbf{\\delta}^{(k+1)} \\cdot \\left(\\mathbf{W}^{(k+1)}_{-BIAS}\\right)^T \\right) a'(\\mathbf{S}^{(k)})}$$ \n",
    "\n",
    "Amennyiben a $w^{k+1}_{0,j_{+1}}$ súlyokat is megtartanánk a $k$-adik réteghez hozzáadott BIAS neuronhoz tartozó $\\delta$ értékeket is megkapjuk. Ennek a neuronnak azonban nincsenek bemenetei, és így súlyok se tartoznak hozzá a $k$ rétegben, a további számításainkban problémát okozna a mátrixdimenziókban (már a $a'(\\mathbf{S}^{(k)})$ mátrixxal történő elemenkénti szorzásnál sem stimmelnének a dimenziók - a BIAS neuronnak nics $s$ értéke)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiens a teljes hálóra\n",
    "\n",
    "A levezetett képletekkel meghatározható a teljes hálót reprezentáló összes súlymátrixtra a gradiens az alábbi lépésekkel:\n",
    "- Kimeneti réteghez tartozó $\\mathbf{\\delta}^{(p)}$ számítása (költségfüggvény és kimeneti réteg aktivációs függvénye alapján)\n",
    "- Rejtett rétegekhez tartozó $\\mathbf{\\delta}^{(k)}$ számítása a kimeneti réteghez meghatározott $\\mathbf{\\delta}^{(p)}$ értékekből kiindulva, hátulról előrefelé\n",
    "- Súlymódosítások mátrixának számítása *minden* rétegre a réteghez tartozó $\\mathbf{\\delta}^{(k)}$ és $\\mathbf{X}^{(k)}$ értékek alapján"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
