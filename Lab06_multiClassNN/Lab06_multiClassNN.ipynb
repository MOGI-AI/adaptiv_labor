{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Labor: Többosztályos klasszifikáció Neurális Hálózattal\n",
    "\n",
    "A gyakorlat során az egyszerű MLP neurális hálót fogjuk adaptálni kettőnél több osztályba való kalsszifikációhoz.\n",
    "\n",
    "### Kézzel írott számjegyek felismerése\n",
    "\n",
    "A több osztályba történő klasszifikáció feladatát a kézzel írott számjegyeket tartalmazó MNIST dataset egy származtatásán végezzük el. A cél egy olyan egyszerű MLP modell felépítése, amely képes egy 20x20-as szürkeárnyalatos kép alapján meghatározni milyen számjegy (0-9) szerepel az adott mintán."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Többosztályos klasszifikáció\n",
    "\n",
    "Az eddigiekben a bináris klasszifikáció feladatára láttunk példákat a logisztikus regresszió / perceptron struktúra alkalmazásával. Hogyan lehetne a perceptron struktúra felhasználásával egy olyan modellt készíteni, ami a bemenetek alapján képes 10 különböző osztály közül választani?\n",
    "- Perceptron 0-9 közötti kimenettel?\n",
    "\n",
    "Ha 10 osztály közül kell választani, akkor egy [0,10] kimeneti tartományú modell megfelelőnek tűnhet, azonban jelentősen korlátozza a megoldás lehetőségeit a kimenetek közötti hierarchia feltételezésével: *a '4' számjegyet tartazó kép bemeneti alapján a kimenet kétszer akkora legyen, mintha a kép egy '2' számjegyet tartalmazna*; *a macskát ábrázoló kép (3. osztály) a kimenetre harmadakkora értéket eredményezzen, mintha a képe nyulat (9. osztály) ábrázolna*. Könnyen belátható, hogy ez nem egy jó megközelítés, ritákn célszerű a mintákra ilyen hierarchiát rákényszeríteni.\n",
    "\n",
    "- Külön-külön modell minden osztályba való tartozásra\n",
    "\n",
    "Egy perceptronnal egy bináris klasszifkációs probléma modellezhető. Az eredeti feladatot (*melyik állat szerepel a képen?*) felbonthatjuk több, egyenként bináris kalsszifikációt tartalmazó feladatra (*mekkora az esélye, hogy macskát ábrázol a kép?*, *mekkora az esélye, hogy nyulat a kép?*, *stb*). Az egyes perceptronokkal egy **One vs. All** klasszifikációs feladatot valósítunk meg (*mekkora a valószínűsége, hogy az adott osztályba és nem bármelyik másikba tartozik inkább a bemeneti adatpont?*)\n",
    "\n",
    "10 osztály esetén ez 10 különböző logisztikus regresszió illesztése, amelyek egyenként az adott osztályra ad becslést a **One vs. All** struktúrának megfelelően. A gyakorlaton használt adatokra 10 független logisztikus regresszió illesztésével a kézzel írott számjegyek felismerésének feladata (több-kevesebb pontossággal) megoldható a **One vs. All** struktúrát alkalmazva.\n",
    "\n",
    "<!---\n",
    "<center><img src=\"img/oneVSall.svg\" width=\"400\"></center>\n",
    "-->\n",
    "<center><img src=\"https://drive.google.com/uc?export=view&id=1I1TZHLYuVGekQFuD-M-343i5NXKjHyTx\" width=\"400\"></center>\n",
    "\n",
    "\n",
    "\n",
    "Vegyük észre, hogy az azonos bemeneti adatokon 10 különböző bináris klasszifikációt megvalósító perceptronok képe megyegyezik egy MLP modell 10 neuront tartalmazó rétegének alakjával. Amennyiben ez a réteg az MLP modell utolsó (kimeneti) rétege, úgy egy olyan neurális háló struktúra adódik, amely a bemenet 10 különböző osztályba való tartozásnak a valószínűségét képes modellezni. Ekkor elmondható, hogy a rejtett rétegek a bemenetek magasabb absztrakciós szintű *'tulajdonságait'* határozzák meg, amely jól reprezentálja a bemenetet, majd a legutolsó, kimeneti réteg ezen absztrakt tulajdonságok alapján végzi el az adott osztályba való tartozás valószínűségének megbecslését. Állatok osztályozásánál például a bemeneti kép alapján elképzelhető, hogy az utolsó rejtett rétegben egy neuron a szőrösséget, egy másik kimenet a lábak számát, míg megint egy másik kimenet a testméretet, vagy a testszínt írja le. A klasszifikáció ezen tulajdonságok alapján történik az eredeti egy-egy pixel adatot tartalmazó változók helyett."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding\n",
    "\n",
    "A bemenetekhez tartozó címkék a legtöbb esetben a bemenethez tartozó osztály számaként, vagy az osztálynak megfelelő szöveges formátumú címkeként vannak megadva (sparse encoding).\n",
    "\n",
    "$$ y_l = 3; \\qquad y_l = \\text{\"cat\"} $$\n",
    "\n",
    "A fenti modell esetében egy bemenetre a kimenet az osztályok számának megfelelő darab valószínűségi értéket fog tartalmazni egy sorvektor formájában. A modell tanításához a bemeneti adatokhoz tartozó cimkéket ennek a struktúrának megfelelően kell megadni. Amennyiben az osztályok száma $c$ :\n",
    "\n",
    "$$\\mathbf{y}_l = \n",
    "\t\\left[ \\begin{array}{cccc}\n",
    " \t\ty_{l, 1} & y_{l, 2} & \\cdots & y_{l, c}\\\\\n",
    "\t\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "Mivel a bemenet pontosan egy osztályba tartozik, a $y_{l, j}$ értékek közül pontosan egy lesz 1, a többi 0. Egy 4 különöző osztályt megkülönböztető példa esetén, ahol a bemenet a harmadik osztályba tartozik:\n",
    "\n",
    "$$\\mathbf{y_l} = \n",
    "\t\\left[ \\begin{array}{cccc}\n",
    " \t\t0 & 0 & 1 & 0\\\\\n",
    "\t\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "Ezt a formátumot 'one-hot' enkódolt formátumnak nevezik, mivel a lehetséges értékek közül pontosan egy érvényes, vagy 'hot'. A feladatmegoldás során a kapott címkéket ilyen formátumra kell konvertálnunk. Ennek implementációját segíti, hogy számmal adott osztálycímkék esetén a one-hot enkódolt érték pont megfelel egy $c$ méretű egységmátrix megfelelő sorának:\n",
    "\n",
    "$$\\mathbf{Y}_{sparse} = \\left[ \\begin{array}{c}\n",
    " \t\t 1 \\\\\n",
    "\t\t 2 \\\\\n",
    "\t\t 3 \\\\\n",
    "\t\t 4 \\\\\n",
    "\t\\end{array}\\right] \\Rightarrow\n",
    "\n",
    "\\mathbf{Y}_{one-hot} = \\left[ \\begin{array}{cccc}\n",
    " \t\t 1 & 0 & 0 & 0\\\\\n",
    "\t\t 0 & 1 & 0 & 0\\\\\n",
    "\t\t 0 & 0 & 1 & 0\\\\\n",
    "\t\t 0 & 0 & 0 & 1\\\\\n",
    "\t\\end{array}\\right]$$\n",
    "\n",
    "Szövegként megadott címkék esetén hasonlóan járhatunk el, ha egy listába rendezzük a különböző címkéket, majd az adott osztály listában elfoglalt helye szerint rendeljük hozzá a one-hot enkódolt kimenetet."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SoftMax aktivációs függvény\n",
    "\n",
    "A kimeneti rétegben *sigmoid* aktivációs függvényt alkalmazva a modell minden osztályba való tartozásra meghatároz egy valószínűséget. A single label osztályozás feladattípusnál (egy bemenet pontosan egy osztályba tartozik) az összes osztályra vett valószínűségek összegének egyet kell adnia, mivel annak a valószínűsége, hogy a bemenet az adott osztályok közül valamelyikbe tartozik pontosan egy (biztos esemény). Matematikailag azonban semmi nem akadályozza, hogy a modell egy adott bemenetre olyan valószínűségeket prediktáljon, melyek összege nem egy. Például állatokról készült képek klasszifikációjánál előfordulhat, hogy a predikció szerint 0.75 eséllyel kutya és 0.80 eséllyel macska. \n",
    "\n",
    "A kimeneti rétegre ezért egy más aktivációs függvényt kell alkalmaznunk, amely a kimeneti réteg $\\mathbf{s}_l^{(p)}$ értékei alapján úgy határozza meg a kimeneteket, hogy azok tartománya [0,1] legyen, összegük 1 és értékeik az $\\mathbf{s}_l^{(p)}$ értékekkel nem feltétlenül lineárisan, de arányosak (nagyobb $s$ értékhez nagyobb kimeneti valószínűség tartozzon). Jó megoldásnak tűnhet erre, ha a az adott kimenethez tartozó $s^{(p)}_{l,j_0}$ értéket az adatponthoz tartozó összes $s^{(p)}_{l,j}$ érték összegével normáljuk:\n",
    "\n",
    "$$\\hat y_{l,j_0} = a(s^{(p)}_{l,j_0}) = \\frac{s^{(p)}_{l,j_0}}{\\sum_{j=1}^c s^{(p)}_{l,j}}$$\n",
    "\n",
    "Amíg minden $s^{(p)}_{l,j}$ érték pozitív, ez teljesíti a követelményeket, miszerint\n",
    "\n",
    "$$ \\sum_{j=1}^c \\hat y_{l,j_0} = 1$$\n",
    "és\n",
    "$$ a(s^{(p)}_{l,j_a}) > a(s^{(p)}_{l,j_b}) \\quad \\text{ha} \\quad s^{(p)}_{l,j_a} > s^{(p)}_{l,j_b}$$\n",
    "\n",
    "Könnyen látható azonban, hogy ha az $s^{(p)}_{l,j}$ értékek között szerepel negatív is, a feltételek sérülnek. érdemes tehát először transzformálnunk ezeket az összegeket egy olyan függvénnyel, amely a $[-\\infty, +\\infty]$ tartományról a $[0, +\\infty]$ tartományra képez, és szigorúan monoton növekvő függvény. Ezeknek a feltételeknek eleget tesz az exponenciális függvény, amellyel az új aktivációs függvény:\n",
    "\n",
    "$$ a(s^{(p)}_{l,j_0}) = \\frac{e^{s^{(p)}_{l,j_0}}}{\\sum_{j=1}^c e^{s^{(p)}_{l,j}}}$$\n",
    "\n",
    "Az így kapott *softmax* aktivációs függvény (amelyet kizárólag a kimeneti rétegben szokás alkalmazni többosztályos klasszifikáció esetén) teljesíti mind a valószínűségek összegére, mind a bemenetekkel való arányosságra vonatkozó követelményeket:\n",
    "\n",
    "$$ \\boxed{softmax(z_{j_0}) = \\frac{e^{z_{j_0}}}{\\sum_{j=1}^c e^{z_j}}}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kategórikus keresztentrópia\n",
    "\n",
    "A több kategóriás klasszifikációhoz a bináris klasszifikáció - BCE költséghez hasonlóan  a *Maximum Likelihood* becslést alklamazva rendelhető költségfüggvény. A modell paramétereitől függően az összes $m$ adatpontra a likelihood függvény $\\mathcal{L}(\\mathbf{w} \\ | \\ \\mathbf{X})$ az egyes adatpontok valószínűségének szorzata:\n",
    "\n",
    "$$ \\mathcal{L}(\\mathbf{w} \\ | \\ \\mathbf{X}) = \\prod_{l=1}^m  \\mathcal{L}(\\mathbf{w} \\ | \\ \\mathbf{x}_{l}) $$\n",
    "\n",
    "ahol $\\mathbf{w}$ itt a modell *összes* paraméterét jelöli, $\\mathbf{X}$ az összes bemeneti adatpont minden változóját tartalmazó mátrix, míg $\\mathbf{X}_{l}$ az $l$-edik adatpont változóit tartalmazó (sor)vektor.\n",
    "Egy bemeneti adatpont előfordulásának valószínűsége a $\\mathbf{w}$ modellparaméterek mellett a bementek tényleges osztályára prediktált valószínűség:\n",
    "\n",
    "$$ \\mathcal{L}(\\mathbf{w} \\ | \\ \\mathbf{x}_{l}) = \\hat y_{l,j_{l}} $$\n",
    "\n",
    "ahol $j_{l}$ az $l$-edik adatpont valós osztálya.\n",
    "\n",
    "A bináris keresztentrópiához hasonlóan a likelihood helyett annak logaritmusának, a log-likelihood függvény alakja:\n",
    "\n",
    "$$ ln \\left(\\mathcal{L}(\\mathbf{w} \\ | \\ \\mathbf{X})\\right) = ln \\left( \\prod_{l=1}^m  \\hat y_{l,j_{l}} \\right) $$\n",
    "\n",
    "A $\\log(a \\cdot b) = \\log(a)+\\log(b)$ azonosság alapján:\n",
    "\n",
    "$$ ln \\left(\\mathcal{L}(\\mathbf{w} \\ | \\ \\mathbf{X})\\right) =  \\sum_{l=1}^m ln \\left( \\hat y_{l,j_{l}} \\right) $$\n",
    "\n",
    "A $\\hat y_{l,j_{l}}$ specifikus kiválasztása helyett a one-hot enkódolt kimeneti vektorokat felhasználva\n",
    "\n",
    "$$ln \\left(\\hat y_{l,j_{l}} \\right) = \\sum_{j=1}^c y_{l,j} \\ ln \\left(\\hat y_{l,j} \\right)$$  \n",
    "\n",
    "ahol $c$ a feladathoz tartozó osztályok száma. A teljes log-likelihood függvény így:\n",
    "\n",
    "$$ ln \\left(\\mathcal{L}(\\mathbf{w} \\ | \\ \\mathbf{X})\\right) =  \\sum_{l=1}^m \\sum_{j=1}^c \\left( y_{l,j} \\ ln \\left(\\hat y_{l,j} \\right) \\right) $$\n",
    "\n",
    "A log-likelihood függvény maximalizálása megegyezik a a függvény negatívjának minimalizálsával, így a kategórikus keresztentrópia (Categorical Cross-Entropy) költségfüggvény értéke, a bemenetek számától függetlenítve:\n",
    "\n",
    "$$ \\boxed{C_{CCE} = \\frac{1}{m} \\sum_{l=1}^m \\sum_{j=1}^c \\left( - y_{l,j} \\ ln \\left(\\hat y_{l,j} \\right) \\right) }$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BackProp a többosztályos klasszifikáció esetén\n",
    "\n",
    "A többosztályos klasszifikáció esetén a kimeneti réteg $\\mathbf{\\delta}^{(p)}$ értékének számítását az alkalmazott CCE költségfüggvény és a *softmax* aktivációs függvény apaján kell elvégezni. Mivel az egyes $\\mathbf{\\hat y}_{l,j}$ kimenetek az utolsó réteg minden $\\mathbf{S}_l$ értékétől függenek a *softmax* miatt, a számítása nem triviális. A számítások elvégzése után az eddigiekkel megegyező formát kapunk. A teljes levezetés az **Appendix** részben megtalálható.\n",
    "\n",
    "$$ \\mathbf{\\delta}^{(p)} = \\frac{1}{m} \\left(\\mathbf{\\hat Y} - \\mathbf{Y}\\right) $$\n",
    "\n",
    "A többosztályos klasszifikációnál használt hálózat rejtett rétegeinek struktúrája az eddigiekhez képest változatlan. Az itt használható szokásos aktivációs függvények ritkán tartalmaznak a réteg többi $s$ értékétől való függést, így a backprop folyamata ezekre a rétegekre változatlan:\n",
    "\n",
    "$$ \\mathbf{\\delta^{(k)}} = \\left( \\mathbf \\delta^{(k+1)} \\cdot \\left( \\mathbf W^{(k+1)}_{-BIAS} \\right)^T \\right) a'\\left(\\mathbf s^{(k)} \\right) $$\n",
    "\n",
    "Illetve a gradiensek értéke minden rétegben az eddigiekkel megegyezően:\n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial \\mathbf{W}^{(k)}} = \\left(\\mathbf X^{(k)}\\right)^T \\cdot \\mathbf \\delta^{(k)} $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00: Könyvtár importálások\n",
    "\n",
    "Első lépésként importáljuk a feladat megoldása során használt könyvtárakat. Esetünkben ezek a következők lesznek:\n",
    "- Numpy a matematikia műveletek elvégzéséhez\n",
    "- Pandas az adatok beolvasásához és kezeléséhez\n",
    "- MatPlotLib.pyplot az eredményeink ábrázolásához\n",
    "- SciPy 'loadmat' modulja a matlab változókat tartalamzó fájlok importálására\n",
    "- SciKit-Learn confusion_matrix számoló modulja a hálózat értékeléséhez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Használjuk ezeket sötét téma esetén\n",
    "plt.style.use('dark_background')\n",
    "styleTemplate = 'plotly_dark'\n",
    "\n",
    "# Használjuk ezeket világos téma esetén\n",
    "#plt.style.use('default')\n",
    "#styleTemplate = 'plotly_white'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01: Adatbeolvasás\n",
    "Az adatok az  `mnist.mat` matlab változófájlban találhatók. Ilyen fájltípus a SciPy `loadmat` moduljával importálhatók."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = loadmat(\"mnist.mat\")                 # .mat file importálás\n",
    "X = mat[\"X\"]                               # X rendezése\n",
    "Y = mat[\"y\"]                               # Y rebbdezése\n",
    "m = X.shape[0]                             # minták száma\n",
    "n = X.shape[1]                             # változók száma\n",
    "c = np.unique(Y).size                      # osztályok száma\n",
    "\n",
    "print('''Shape of the dataset in order X and Y:\n",
    "''',X.shape,'\\n',Y.shape,'\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02: Adatfelfedezés\n",
    "\n",
    "A beolvasott adatainkon látszik, hogy 5000 mintát tartalmaz. A bemenet 400 változót tartalmaz, amely a 20x20-as kép egyes pixelei egy kiterített vektorban, az alábbihoz hasonló logika szerint (a jelen adataink sorok helyett oszloponként vannak kiterítve - *Fortran-like index order*). \n",
    "<!--\n",
    "<center><img src=\"img/img_flatten.svg\" width=\"455\"></center>\n",
    "-->\n",
    "<center><img src=\"https://drive.google.com/uc?export=view&id=1HjqjYJVmAouaV2nD-SQplYCBeaNAPybP\" width=\"455\"></center>\n",
    "\n",
    "A modellválasztás nem feltétlenül optimális, mivel a Feed forward neurális háló bemenetei között nincs struktúra feltételezve, véletlenszerű pixlesorrendben ugyan annyi infromációt tudnánk átadni. A modellünk a síkbeli struktúrát nem fogja jól érzékelni (melyik pixel melyik felett/mellett/alatt van), azonban a feladat egyszerűsége miatt még így is eredményes. Magasabb komplexitás esetén érdemes konvolúciós neurális háló struktúrát alkalmazni kép jellegű bemenetekre, amely képes az egyes pixelek egymáshoz képesti elhelyezkedését is figyelembe venni.\n",
    "\n",
    "Ábrázoljuk ez első 10 mintát, hogy jobb képet kapjunk az adatainkról, illetve vizsgáljuk meg a bemeneti adataink nagyságrendjét! Először nézzük meg a címkék formátumát."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Az adatokban található osztályok: {}\".format(np.unique(Y)))\n",
    "\n",
    "print(\"Az első 10 mintához tartozó címkék: {}\".format(Y[:10,:].flatten())) # kiíratás sorvektorként"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ez alapján megállíptható, hogy 10 különböző osztály áll rendelkezésünkre (0-9 számjegyek), láthatóan rendezett sorrendben. Ábrázoljunk egy pár véletlen mintát, a hozzájuk tartozó címkékkel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"15 véletlenszerű minta ...\")                                    # kirajzoltatjuk az első 100 elemet\n",
    "rng = np.random.default_rng(42)                                   # véletlenszám generátor\n",
    "samples = rng.integers(m, size=15)\n",
    "fig, axis = plt.subplots(1,15, figsize = (10,4))                # mivel egy plotra akarunk több ábrát -> subplots\n",
    "for i in range(15):                                             # 15 kis ábra\n",
    "    pixels = X[samples[i],:].reshape([20,20], order='F')        # 20x20 alak visszaállítása, Fortran index order\n",
    "    axis[i].imshow(pixels)                                      # kirajzolás\n",
    "    axis[i].axis(\"off\")\n",
    "    axis[i].set_title(Y[samples[i],0])\n",
    "plt.show()\n",
    "\n",
    "print(\"A bemeneti adatok (pixelértékek) tartománya: [{}, {}]\".format(np.min(X), np.max(X)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Az ábrázolás alapján látszik, hogy (feltehetően a matlabos indexelés miatt) a 0-ás számjegyhez 10-es címke van rendelve. A bemeneti adatok bár nem a megszokott 0..1 vagy 0..255 tartományon vannak, nagyságrendük nem igényli a normalizálást.\n",
    "\n",
    "**Megjegyzés:** a képek szürkeárnyalatos formában adottak (1 érték pixelenként), a színskálát a matplotlib rendeli hozzá."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03: Adatok előkészítése\n",
    "\n",
    "A legtöbb esetben nem szerencsés a tanításra használt adatokat rendezett módon megadni (ennek jelentősége akkor nagyobb, ha a súlymódosítást nem a teljes adathalmazra számoljuk, hanem 'batch' módban, egy-egy kissebb részre külön-külön végezzük el). Az adatok előkészítése így jelen esestben a követtkező lépésekből áll:\n",
    "- adatok sorrendjének randomizálása\n",
    "- 10-es címkék javítása 0-ra\n",
    "- one-hot enkódolt kimenetek előállítása"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "shuffledData = np.hstack([X,Y])\n",
    "rng.shuffle(shuffledData, axis = 0)\n",
    "X = shuffledData[:,:400]\n",
    "Y = shuffledData[:,400:].astype('uint8')\n",
    "\n",
    "# 10->0\n",
    "Y[Y[:,0]==10, 0] = 0\n",
    "\n",
    "# Méretek és keverés sikerességének ellenőrzése\n",
    "print('''Shape of the dataset in order X and Y:\n",
    "''',X.shape,'\\n',Y.shape,'\\n')\n",
    "print(\"Az első 20 mintához tartozó címke: {}\".format(Y[:20,:].flatten())) # kiíratás sorvektorként"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Láthatóan sikeres volt az elemek keverése és a 0 számjegyekhez tartozó címkék cseréje.\n",
    "\n",
    "**Feladat:** állítsa elő a címkéknek megfelelő one-hot enkódolt kimeneti mátrixot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "\n",
    "\n",
    "######################################################\n",
    "\n",
    "print(\"Az első 6 mintához várt one-hot enkódolt címkék:\")\n",
    "print('''[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
    " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
    " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
    " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
    " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]''')\n",
    "print(\"Az első 6 mintához tartozó tényleges one-hot enkódolt címkék:\")\n",
    "print(Y_oneHot[:6,:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04: Modell implementálása\n",
    "\n",
    "Az előző órán elkészített egyszerű neurális háló modellből kiindulva végezzük el a szükséges módosításokat a többosztályos klasszifikáció implementálásához."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z, derivate = False):       # Alapértelmezett a rendes sigmoid érték számítása\n",
    "    g = 1 / (1 + np.exp(-z))\n",
    "    if derivate == True:\n",
    "        g = g * (1-g)\n",
    "    return g"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feladat:** implementálja a `softmax()` aktivációs függvényt, amely a bemenetként adott $\\mathbf{S}^{(p)}$ mátrixra kiszámítja a kimeneti $\\mathbf{\\hat Y}^{(p)}$ mátrixot a *softmax* aktivációnak megfelelően!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):       # Alapértelmezett a rendes sigmoid érték számítása\n",
    "######################################    \n",
    "\n",
    "\n",
    "\n",
    "######################################\n",
    "    return g\n",
    "\n",
    "# Test softmax\n",
    "z = np.array([[2.52640812, 4.75720026, 5.76544649],\n",
    "       [4.93992941, 6.70753489, 4.90961306],\n",
    "       [5.64891358, 5.49916996, 4.72750292],\n",
    "       [5.26816593, 6.27247156, 2.79979483]])\n",
    "\n",
    "print()\n",
    "print('''Expected softmax outputs for test array:\n",
    "[[0.02792016 0.2598595  0.71222034]\n",
    " [0.12776367 0.74828786 0.12394847]\n",
    " [0.4426961  0.38112977 0.17617413]\n",
    " [0.26214149 0.71564916 0.02220935]]''')\n",
    "print('Actual softmax outputs for test array:')\n",
    "print(softmax(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addBias(z):\n",
    "    return np.hstack([np.ones([z.shape[0] ,1]), z])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feladat:** végezze el a szükséges változtatásokat az alábbi bináris klasszifikációra implementált neurális háló osztályon, hogy az képes legyen a többosztályos klasszifikáció elvégzésére!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN:\n",
    "    def __init__(self, layerSizes, activationFunction, seed):\n",
    "        self.layerSizes = layerSizes            # Háló konfigurációja\n",
    "        self.a = activationFunction             # Aktivációs függvény (később más aktivációs függvény is alkalmazható)\n",
    "        self.noLayers = len(layerSizes)-1       # Rétegek száma (iterációhoz, bemeneti réteg nélkül)\n",
    "        self.inputSize = layerSizes[0]          # Bemeneti változók (input feature) száma\n",
    "        self.Yhat = []                          # Lista az egyes rétegek kimenetének tárolására\n",
    "        self.X = []                             # Lista az egyes rétegek bemenetének tárolására\n",
    "        self.W = []                             # Lista a rétegekhez tartozó súlyoknak\n",
    "        self.initWeights(seed)                  # Súlyok inicializálása\n",
    "    \n",
    "    def initWeights(self, seed = None):\n",
    "        rng = np.random.default_rng(seed)\n",
    "        # Weight matrix dimension are based on number of neurons in previous and current layer\n",
    "        self.W = [rng.uniform(low = -1, high = 1, size = [self.layerSizes[i]+1, self.layerSizes[i+1]]) for i in range(self.noLayers)]\n",
    "\n",
    "    def checkInputSize(self, X):\n",
    "        if X.shape[1] != self.inputSize:\n",
    "            raise ValueError('Unexpected number of input features! Expected {} features but got {}.'.format(self.inputSize, X.shape[1])) \n",
    "\n",
    "    def forwardProp(self, X):\n",
    "        self.checkInputSize(X)\n",
    "\n",
    "        self.X = [[] for i in range(self.noLayers)]\n",
    "        self.Yhat = [[] for i in range(self.noLayers)]\n",
    "        \n",
    "        self.X[0] = addBias(X)  # FirstLayer\n",
    "        self.Yhat[0] = self.a(self.X[0]@self.W[0])\n",
    "\n",
    "        for i in range(1, self.noLayers-1): # HiddenLayers\n",
    "            self.X[i] = addBias(self.Yhat[i-1])\n",
    "            self.Yhat[i] = self.a(self.X[i]@self.W[i])\n",
    "\n",
    "        self.X[-1] = addBias(self.Yhat[-2])\n",
    "        self.Yhat[-1] = sigmoid(self.X[-1]@self.W[-1])\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.forwardProp(X)\n",
    "        return self.Yhat[-1]\n",
    "\n",
    "    def costBCE(self, X, Y, forwardProp = True):\n",
    "        eps = 10e-15\n",
    "        if forwardProp: self.forwardProp(X)\n",
    "        return np.mean(-Y*np.log(self.Yhat[-1]+eps)-(1-Y)*np.log(1-self.Yhat[-1]+eps))\n",
    "    \n",
    "    def accuracy(self, X, Y, forwardProp = True):\n",
    "        if forwardProp: self.forwardProp(X)\n",
    "        return np.mean(Y == (self.Yhat[-1] > 0.5)) * 100\n",
    "\n",
    "    def backProp(self, trueY):\n",
    "        deltas = [[] for i in range(self.noLayers)]\n",
    "        m = trueY.shape[0]\n",
    "\n",
    "        # Delta a kimeneti rétegben\n",
    "        deltas[-1] = 1/m * (self.Yhat[-1] - trueY)\n",
    "        # Delta kiszámítása a rejtett réteg(ek)ben\n",
    "        for i in range(self.noLayers-2, -1, -1):\n",
    "            deltas[i] = (deltas[i+1]@self.W[i+1][1:,:].T) * self.a(self.X[i]@self.W[i], derivate = True)\n",
    "        \n",
    "        dW = [self.X[i].T@deltas[i] for i in range(self.noLayers)]\n",
    "        return dW\n",
    "\n",
    "    def updateWeights(self, learning_rate, dW):\n",
    "        for i in range(self.noLayers):\n",
    "            self.W[i] = self.W[i] - learning_rate * dW[i]\n",
    "\n",
    "    def fit(self, X, Y, learning_rate, epochs):\n",
    "        self.forwardProp(X)\n",
    "\n",
    "        C_history = np.zeros([epochs+1])\n",
    "        C_history[0] = self.costBCE(X, Y, forwardProp = False)\n",
    "\n",
    "        acc_history = np.zeros([epochs+1])\n",
    "        acc_history[0] = self.accuracy(X, Y, forwardProp = False) \n",
    "        print('''\n",
    "        \\%\\%\\% ------- TANÍTÁS ------- \\%\\%\\%\n",
    "        ''')\n",
    "        for i in range(epochs):\n",
    "            dW = self.backProp(Y)\n",
    "            self.updateWeights(learning_rate, dW)\n",
    "            self.forwardProp(X)                     # Forwardprop with new weights\n",
    "            C_history[i+1] = self.costBCE(X, Y, forwardProp = False)     \n",
    "            acc_history[i+1] = self.accuracy(X, Y, forwardProp = False) \n",
    "\n",
    "            if ((i+1) % 100) == 0:\n",
    "                \n",
    "                print('Epoch {} / {} completed. Cost value: {}; Accuracy: {:.2f}'.format(i+1, epochs, C_history[i+1], acc_history[i+1])) \n",
    "\n",
    "        return C_history, acc_history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A modell helyes működése az alábbi tesztek futtatásával ellenőrizhető:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layerSizes = [2, 3, 4]\n",
    "seed = 42\n",
    "\n",
    "testNN = FeedForwardNN(layerSizes, sigmoid, seed)\n",
    "\n",
    "testX = np.array([[0.3146, -0.65432], [-1.0123, -0.4215], [0.2351, 0.7533456]])\n",
    "testY = np.array([[0, 0, 1, 0], [1, 0, 0, 0], [0, 1, 0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test initweights\n",
    "print('''Expected initial weights with layer sizes {0} and random seed {1}:\n",
    "[array([[ 0.5479121 , -0.12224312,  0.71719584],\n",
    "       [ 0.39473606, -0.8116453 ,  0.9512447 ],\n",
    "       [ 0.5222794 ,  0.57212861, -0.74377273]]), array([[-0.09922812, -0.25840395,  0.85352998,  0.28773024],\n",
    "       [ 0.64552323, -0.1131716 , -0.54552256,  0.10916957],\n",
    "       [-0.87236549,  0.65526234,  0.2633288 ,  0.51617548],\n",
    "       [-0.29094806,  0.94139605,  0.78624224,  0.55676699]])]'''.format(layerSizes, seed))\n",
    "print('''Actual initial weights with layer sizes {0} and random seed {1}:\n",
    "{2}'''.format(layerSizes, seed, testNN.W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forwardProp & Predict\n",
    "testPred = testNN.predict(testX)\n",
    "\n",
    "print('''Expected prediction for test parameters:\n",
    "[[0.08834134 0.21661991 0.39782789 0.29721086]\n",
    " [0.07640996 0.21790532 0.39044155 0.31524317]\n",
    " [0.09595343 0.21828394 0.35748864 0.32827399]]'''.format(layerSizes, seed))\n",
    "print('''Actual prediction for test parameters:\n",
    "{0}'''.format(testPred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test backprop\n",
    "testNN.forwardProp(testX)\n",
    "testdW = testNN.backProp(testY)\n",
    "\n",
    "print('''Expected weight change values for test parameters:\n",
    "[array([[-0.03519045,  0.07277344,  0.04905192],\n",
    "       [ 0.07725816, -0.10234505, -0.08827836],\n",
    "       [ 0.00673892, -0.06410875, -0.04750743]]), array([[-0.24643176, -0.11573027,  0.04858602,  0.31357601],\n",
    "       [-0.10767695, -0.11520305,  0.03385651,  0.18902349],\n",
    "       [-0.16222644, -0.07032992,  0.07850704,  0.15404933],\n",
    "       [-0.11605659, -0.05816385, -0.02614065,  0.20036109]])]''')\n",
    "\n",
    "print('''Actual prediction for test parameters:\n",
    "{0}'''.format(testdW))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05: Modell tanítása\n",
    "\n",
    "Amennyiben minden tesztünkön megfelelő eredményt kaptunk, végezzük el a háló tanítását a `.fit()` metódus segítségével, és ábrázoljuk a költségfüggvény alakulását."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.5\n",
    "trainNN = FeedForwardNN([n, 25, c], sigmoid, 42)\n",
    "\n",
    "C_history, acc_history = trainNN.fit(X, Y_oneHot, learning_rate, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(2,1)     \n",
    "\n",
    "axis[0].plot(range(C_history.size), C_history)\n",
    "axis[0].set_ylabel('Cost value [-]')\n",
    "\n",
    "axis[1].plot(range(acc_history.size), acc_history)\n",
    "axis[1].set_ylabel('Accuracy [%]')\n",
    "axis[1].set_xlabel('epochs')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06: Modell értékelése\n",
    "\n",
    "Jelen feladatnál már nehezen megoldható döntési határok vagy illesztett felületek vizsgálata. A modell értékelésének egy lehetséges módja egy véletlenszerű adatpont kiválasztása, és a modell kimenetének összevetése a valósággal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rndID = rng.integers(m)\n",
    "pred = trainNN.predict(X[rndID, :].reshape(1,400))\n",
    "\n",
    "plt.imshow(X[rndID, :].reshape([20,20], order='F'))\n",
    "plt.show\n",
    "\n",
    "print('Real label of data: {}'.format(Y[rndID,0]))\n",
    "print('Predicted label of data: {} --> {}'.format(np.argmax(pred[0]), pred[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A klasszifikáció értékelésénk egy másik lehetősége a konfúziós mátrix ábrázolása, amely megmutatja a valós és prediktált osztályok közötti kapcsolatokat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = trainNN.predict(X)\n",
    "conf = confusion_matrix(Y, np.argmax(preds, axis = 1))  # Sparse predicitons\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (6,6))\n",
    "im = ax.imshow(conf)\n",
    "\n",
    "ax.set_xticks(np.arange(c), labels=np.unique(Y))\n",
    "ax.set_xlabel('Prediktált számjegy')\n",
    "ax.set_yticks(np.arange(c), labels=np.unique(Y))\n",
    "ax.set_ylabel('Valós számjegy')\n",
    "\n",
    "# Annotate cells\n",
    "for i in range(c):\n",
    "    for j in range(c):\n",
    "        text = ax.text(j, i, conf[i, j],\n",
    "                       ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "ax.set_title(\"Kézzel írott számjegyek konfúziós mátrixa\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 07: További tesztelés\n",
    "\n",
    "Érdemes a modellt tovább vizsgálni, megfigyelni a tanítás paramétereinek hatásást, vagy akár implementálni a tanítóadatok szétbontását tanító és validációs halmazokra, és megvizsgálni a tanítás korai leállásának lehetőségét a validációs adatokon számított költségfüggvény alapján."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XX: Megoldás Keras könyvtár segítségével\n",
    "\n",
    "A bináris klasszifikációhoz hasonlóan ezt a feladatot is megodldhatjuk a amgaszintű Keras könyvtár segítségével."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for array-handling and plotting\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# keras imports for building and training our neural network\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Adatok előkészítése\n",
    "mat = loadmat(\"mnist.mat\")                 # .mat file importálás\n",
    "X = mat[\"X\"]                               # X rendezése\n",
    "Y = mat[\"y\"]                               # Y rendezése\n",
    "c = np.unique(Y).size\n",
    "Y[Y[:,0]==10, 0] = 0                       # 10 -> 0 label csere\n",
    "oneHotMatrix = np.eye(c)\n",
    "Y_oneHot = oneHotMatrix[Y[:,0],:]          # onehot Mátrix\n",
    "\n",
    "# Keras háló definiálása\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_shape=(400,)))\n",
    "model.add(Activation('sigmoid'))                            \n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compiling the sequential model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=SGD(learning_rate=0.5))\n",
    "\n",
    "# training the model and saving metrics in history \n",
    "history = model.fit(X, Y_oneHot, epochs=10, verbose = 2) # Stochastic gradient descent can learn in a lot fewer epochs\n",
    "\n",
    "# Plot metrics\n",
    "fig, axis = plt.subplots(2,1)     \n",
    "\n",
    "axis[0].plot(history.history['loss'])\n",
    "axis[0].set_ylabel('Cost value [-]')\n",
    "\n",
    "axis[1].plot(history.history['accuracy'])\n",
    "axis[1].set_ylabel('Accuracy [%]')\n",
    "axis[1].set_xlabel('epochs')\n",
    "plt.show()\n",
    "\n",
    "# Plot confusion matrix\n",
    "preds = model.predict(X)\n",
    "conf = confusion_matrix(Y, np.argmax(preds, axis = 1))\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (6,6))\n",
    "im = ax.imshow(conf)\n",
    "\n",
    "ax.set_xticks(np.arange(c), labels=np.unique(Y))\n",
    "ax.set_xlabel('Prediktált számjegy')\n",
    "ax.set_yticks(np.arange(c), labels=np.unique(Y))\n",
    "ax.set_ylabel('Valós számjegy')\n",
    "\n",
    "# Annotate cells\n",
    "for i in range(c):\n",
    "    for j in range(c):\n",
    "        text = ax.text(j, i, conf[i, j],\n",
    "                       ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "ax.set_title(\"Kézzel írott számjegyek konfúziós mátrixa\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "## BackProp a kimeneti rétegre multiclass classification feladat esetében\n",
    "\n",
    "A backpropagation algoritmus implementálásához meg kell határoznunk a kimeneti réteghez tartozó $\\mathbf{\\delta}^{(p)}$ értékeket, amely alapján a háló többi $\\mathbf{\\delta}^{(k)}$ értéke számolható. Több kimeneti neuron esetében az egyes $\\delta$ értékek meghatározása:\n",
    "\n",
    "$$ \\delta^{(p)}_{l,j} = \\frac{\\partial C}{\\partial s^{(p)}_{l,j}}$$\n",
    "\n",
    "**Megjegyzés**: a bináris klasszifikáció esetén ezt a kifejezést a lánc szabály segítségével tovább bonthatuk, mivel adatpontonként egyetlen $\\hat y_{l}$ kimenet volt, amely kizárólag egy $s^{(p)}_{l}$ értéktől függött.\n",
    "\n",
    "Multiclass classification feladat esetén a költségfüggvény a kategórikus keresztentrópia, az aktivációs függvény pedig a softmax függvény:\n",
    "\n",
    "$$\\frac{\\partial C_{CCE}}{\\partial s_{l_0,j_0}} = \\frac{\\partial \\frac{1}{m} \\sum_{l=1}^m \\sum_{j=1}^c \\left( - y_{l,j} \\ ln \\left(\\hat y_{l,j} \\right) \\right)}{\\partial s^{(p)}_{l_0,j_0}} = \\frac{1}{m} \\sum_{l=1}^m \\sum_{j=1}^c \\left( - y_{l,j} \\frac{\\partial ln \\left(\\hat y_{l,j} \\right) }{\\partial s^{(p)}_{l_0,j_0}} \\right)$$\n",
    "\n",
    "Az összetett függvény deriválási szabája szerint:\n",
    "\n",
    "$$\\frac{1}{m} \\sum_{l=1}^m \\sum_{j=1}^c \\left( - y_{l,j} \\frac{\\partial ln \\left(\\hat y_{l,j} \\right) }{\\partial s^{(p)}_{l_0,j_0}} \\right) = \\frac{1}{m} \\sum_{l=1}^m \\sum_{j=1}^c \\left( - y_{l,j} \\ \\frac{1}{\\hat y_{l,j}} \\frac{\\partial \\hat y_{l,j}}{\\partial s^{(p)}_{l_0,j_0}} \\right)$$\n",
    "\n",
    "Amennyiben $l \\neq l_0$, úgy $\\frac{\\partial \\hat y_{l,j}}{\\partial s^{(p)}_{l_0,j_0}} = 0$, azaz egy bemeneti adatpont eredményei függetlenek a többi adatpont eredményeitől:\n",
    "\n",
    "$$ \\frac{1}{m} \\sum_{l=1}^m \\sum_{j=1}^c \\left( - y_{l,j} \\ \\frac{1}{\\hat y_{l,j}} \\frac{\\partial \\hat y_{l,j}}{\\partial s^{(p)}_{l_0,j_0}} \\right) = \\frac{1}{m} \\sum_{j=1}^c \\left( - y_{l_0,j} \\ \\frac{1}{\\hat y_{l_0,j}} \\frac{\\partial \\hat y_{l_0,j}}{\\partial s^{(p)}_{l_0,j_0}} \\right)$$ \n",
    "\n",
    "A single-label classification esetén (egy kimenet csak egy kategóriához tartozhat) az utolsó réteg aktivációs függvénye a *softmax*, így\n",
    "\n",
    "$$ \\hat y_{l_0,j} = softmax(s^{(p)}_{l_0,j}) = \\frac{e^{s^{(p)}_{l_0,j}}}{\\sum_{j'=1}^{c} e^{s^{(p)}_{l_0,j'}}} $$\n",
    "\n",
    "A törtfüggvény deriválásának szabálya $\\left(\\frac{f(x)}{g(x)}\\right)' = \\frac{f'(x)g(x)-g'(x)f(x)}{g^2(x)}$ alapján:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial z_{j_0}}softmax(z_{j}) = \\frac{\\partial}{\\partial z_{j_0}}\\left(\\frac{e^{z_{j}}}{\\sum_{j'=1}^{c} e^{z_j'}}\\right)$$\n",
    "\n",
    "Az egyes részfüggvények deriváltjai:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial z_{j_0}} e^{z_{j}} =\n",
    "\\begin{cases}\n",
    "    e^{z_{j_0}}, & \\text{ha}\\ j = j_0 \\\\\n",
    "    0, & \\text{ha}\\ j \\neq j_0\n",
    "\\end{cases}e^{z_{j_0}} $$\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial z_{j_0}} \\sum_{j'=1}^{c} e^{z'_j} = \\sum_{j=1}^{c} \\frac{\\partial}{\\partial z_{j_0}}e^{z'_{j}} = e^{z_{j_0}}$$\n",
    "\n",
    "Ezekből\n",
    "\n",
    "$$ \\frac{\\partial \\hat y_{l_0,j}}{\\partial s^{(p)}_{l_0,j_0}} = \\frac{\\partial}{\\partial s^{(p)}_{l_0,j_0}}softmax(s^{(p)}_{l_0,j}) = \\frac{\\partial}{\\partial s^{(p)}_{l_0,j_0}}\\left(\\frac{e^{s^{(p)}_{l_0,j}}}{\\sum_{j'=1}^{c} e^{s^{(p)}_{l_0,j'}}}\\right) $$\n",
    "\n",
    "Ha $j = j_0$:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial s^{(p)}_{l_0,j_0}}\\left(\\frac{e^{s^{(p)}_{l_0,j}}}{\\sum_{j'=1}^{c} e^{s^{(p)}_{l_0,j'}}}\\right) = \\frac{e^{s^{(p)}_{l_0,j_0}} \\ \\sum_{j'=1}^{c} e^{s^{(p)}_{l_0,j'}} - e^{s^{(p)}_{l_0,j_0}} \\ e^{s^{(p)}_{l_0,j}}}{\\left( \\sum_{j'=1}^{c} e^{s^{(p)}_{l_0,j'}} \\right)^2} = \\\\\n",
    "= \\frac{e^{s^{(p)}_{l_0,j_0}}}{\\sum_{j'=1}^{c} e^{s^{(p)}_{l_0,j'}}} \\frac{\\sum_{j'=1}^{c} e^{s^{(p)}_{l_0,j'}} - e^{s^{(p)}_{l_0,j}}}{\\sum_{j'=1}^{c} e^{s^{(p)}_{l_0,j'}}} = \\frac{e^{s^{(p)}_{l_0,j_0}}}{\\sum_{j'=1}^{c} e^{s^{(p)}_{l_0,j'}}} \\left(\\frac{\\sum_{j'=1}^{c} e^{s^{(p)}_{l_0,j'}}}{\\sum_{j'=1}^{c} e^{s^{(p)}_{l_0,j'}}} - \\frac{ e^{s^{(p)}_{l_0,j}}}{\\sum_{j'=1}^{c} e^{s^{(p)}_{l_0,j'}}}\\right) = $$\n",
    "$$ \\\\[20pt] $$\n",
    "$$ = \\hat y_{l_0,j_0} \\ \\left(1 -\\hat y_{l_0,j}\\right) = \\hat y_{l_0,j_0} - \\hat y_{l_0,j_0} \\ \\hat y_{l_0,j}$$\n",
    "\n",
    "Ha $j \\neq j_0$:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial s^{(p)}_{l_0,j_0}}\\left(\\frac{e^{s^{(p)}_{l_0,j}}}{\\sum_{j'=1}^{c} e^{s^{(p)}_{l_0,j'}}}\\right) = \\frac{- e^{s^{(p)}_{l_0,j_0}} \\ e^{s^{(p)}_{l_0,j}}}{\\left( \\sum_{j'=1}^{c} e^{s^{(p)}_{l_0,j'}} \\right)^2} = \\frac{e^{s^{(p)}_{l_0,j_0}}}{\\sum_{j'=1}^{c} e^{s^{(p)}_{l_0,j'}}} \\frac{- e^{s^{(p)}_{l_0,j}}}{\\sum_{j'=1}^{c} e^{s^{(p)}_{l_0,j'}}} = - \\hat y_{l_0,j_0} \\ \\hat y_{l_0,j}$$\n",
    "\n",
    "**Megjegyzés:** Az így kiszámított értékek gyakorlatilag az $ \\hat y_{l_0,j} = softmax(s^{(p)}_{l_0,j})$ vektor-vektor függvény Jakobi mátrixának elemei:\n",
    "\n",
    "$$\\mathbf{J}\\left(\\hat y_{l_0,j}\\right) = \\left[\n",
    "\t\\begin{array}{cccc}\n",
    " \t\t\\frac{\\partial \\hat y_{l_0,1}}{\\partial s^{(p)}_{l_0,1}} & \\frac{\\partial \\hat y_{l_0,1}}{\\partial s^{(p)}_{l_0,2}} & \\ldots & \\frac{\\partial \\hat y_{l_0,1}}{\\partial s^{(p)}_{l_0,c}}\\\\\n",
    "\t\t\\frac{\\partial \\hat y_{l_0,2}}{\\partial s^{(p)}_{l_0,1}} & \\frac{\\partial \\hat y_{l_0,2}}{\\partial s^{(p)}_{l_0,2}} & \\ldots & \\frac{\\partial \\hat y_{l_0,2}}{\\partial s^{(p)}_{l_0,c}}\\\\\n",
    " \t\t\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "        \\frac{\\partial \\hat y_{l_0,c}}{\\partial s^{(p)}_{l_0,1}} & \\frac{\\partial \\hat y_{l_0,c}}{\\partial s^{(p)}_{l_0,2}} & \\ldots & \\frac{\\partial \\hat y_{l_0,c}}{\\partial s^{(p)}_{l_0,c}}\\\\\n",
    "\t\\end{array}\t\\right] = $$\n",
    "$$ \\\\[20pt] $$\n",
    "$$    = \\left[\n",
    "\t\\begin{array}{cccc}\n",
    " \t\t\\hat y_{l_0,1} - \\hat y_{l_0,1} \\ \\hat y_{l_0,1} &- \\hat y_{l_0,2} \\ \\hat y_{l_0,1} & \\ldots & - \\hat y_{l_0,c} \\ \\hat y_{l_0,1}\\\\\n",
    "\t\t- \\hat y_{l_0,1} \\ \\hat y_{l_0,2} & \\hat y_{l_0,2} - \\hat y_{l_0,2} \\ \\hat y_{l_0,2} & \\ldots & - \\hat y_{l_0,c} \\ \\hat y_{l_0,2}\\\\\n",
    " \t\t\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "        - \\hat y_{l_0,1} \\ \\hat y_{l_0,c} & - \\hat y_{l_0,2} \\ \\hat y_{l_0,c} & \\ldots & \\hat y_{l_0,c} - \\hat y_{l_0,c} \\ \\hat y_{l_0,c}\\\\\n",
    "\t\\end{array}\t\\right]$$\n",
    "\n",
    "Az eredeti $\\frac{\\partial C_{CCE}}{\\partial s_{l_0,j_0}}$ érték számításához visszatérve\n",
    "\n",
    "$$ \\frac{\\partial C_{CCE}}{\\partial s_{l_0,j_0}} = \\frac{1}{m} \\sum_{j=1}^c \\left( - y_{l_0,j} \\ \\frac{1}{\\hat y_{l_0,j}} \\frac{\\partial \\hat y_{l_0,j}}{\\partial s^{(p)}_{l_0,j_0}} \\right) = \\\\\n",
    "= \\frac{1}{m} \\left(\\left( - y_{l_0,j_0} \\ \\frac{1}{\\hat y_{l_0,j_0}} \\hat y_{l_0,j_0} \\right) - \\sum_{j=1}^c \\left( - y_{l_0,j} \\ \\frac{1}{\\hat y_{l_0,j}} \\hat y_{l_0,j_0} \\ \\hat y_{l_0,j} \\right) \\right) = \\\\\n",
    "= \\frac{1}{m} \\left(- y_{l_0,j_0} - \\sum_{j=1}^c \\left( - y_{l_0,j} \\  \\hat y_{l_0,j_0} \\right) \\right) = \\frac{1}{m} \\left(\\sum_{j=1}^c y_{l_0,j} \\  \\hat y_{l_0,j_0} - y_{l_0,j_0} \\right) = \\frac{1}{m} \\left(\\hat y_{l_0,j_0} \\sum_{j=1}^c y_{l_0,j} - y_{l_0,j_0} \\right)$$\n",
    "\n",
    "Az $y_{l_0,j}$ értékek közül a one-hot enkódolásnak megfelelően pontosan egy elem értéke 1, a többi elem értéke 0, így $\\sum_{j=1}^c y_{l_0,j} = 1$. Ennek megfelelően\n",
    "\n",
    "$$ \\boxed{ \\delta^{(p)}_{l,j} = \\frac{\\partial C_{CCE}}{\\partial s_{l,j}} = \\frac{1}{m} \\left(\\hat y_{l,j} - y_{l,j} \\right) } $$\n",
    "\n",
    "Innen könnyen látható, hogy a multiclass classification feladatra, a CCE költségfüggvény és *softmax* kimeneti aktiváció függvény választással a kimeneti rétegre számított $\\mathbf{\\delta}^{(p)}$ érték mátrixos formában történő számítása megegyezik a a bináris klasszifikációnál (BCE költség és *sigmoid* kimeneti aktiváció) látottal.\n",
    "\n",
    "$$ \\boxed{ \\mathbf{\\delta}^{(p)} = \\frac{1}{m} \\left(\\mathbf{\\hat Y} - \\mathbf{Y}\\right)}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
