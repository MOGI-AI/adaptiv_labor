{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Labor: Többosztályos klasszifikáció Neurális Hálózattal\n",
    "\n",
    "A gyakorlat során az egyszerű MLP neurális háló többosztályos kalsszifikációra való adaptálása kerül bemutatásra.\n",
    "\n",
    "### Kézzel írott számjegyek felismerése\n",
    "\n",
    "A több osztályba történő klasszifikáció mitafeladata a kézzel írott számjegyeket tartalmazó MNIST dataset lesz. A cél egy olyan egyszerű MLP modell felépítése, amely képes egy 20x20-as szürkeárnyalatos kép alapján meghatározni milyen számjegy (0-9) szerepel az adott mintán."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Többosztályos klasszifikáció\n",
    "\n",
    "Az eddigiekben vizsgált modellek főként a bináris klasszifikáció feladatát mutatták be a logisztikus regresszió / perceptron struktúra alkalmazásával. Hogyan lehetne a perceptron struktúra felhasználásával egy olyan modellt készíteni, ami a bemenetek alapján képes 10 különböző osztály közül választani?\n",
    "- Perceptron 0-9 közötti kimenettel?\n",
    "\n",
    "Ha 10 osztály közül kell választani, akkor egy [0,10] kimeneti tartományú modell elsőre megfelelőnek tűnhet. Ez azonban olyan hierarchiát kényszerít a kimenetekre, amely korlátozza a megoldás lehetőségeit: *a '4' számjegyet tartazó kép bemeneti alapján a kimenet kétszer akkora legyen, mintha a kép egy '2' számjegyet tartalmazna*; *a macskát ábrázoló kép (3. osztály) a kimenetre harmadakkora értéket eredményezzen, mintha a kép nyulat (9. osztály) ábrázolna*. Érezhető, hogy ez nem egy jó megközelítés, ritkan célszerű a mintákra ilyen hierarchiát rákényszeríteni.\n",
    "\n",
    "- Külön-külön modell minden osztályba való tartozásra\n",
    "\n",
    "Egy perceptronnal egy bináris klasszifkációs probléma modellezhető. Az eredeti feladat (*melyik állat szerepel a képen?*) felbontható több, egyenként bináris kalsszifikációt tartalmazó feladatra (*mekkora az esélye, hogy macskát ábrázol a kép?*, *mekkora az esélye, hogy nyulat a kép?*, *stb*). Az egyes perceptronok egy **One vs. All** klasszifikációs feladatot valósítanak meg (*mekkora a valószínűsége, hogy az adott osztályba és nem bármelyik másikba tartozik inkább a bemeneti adatpont?*)\n",
    "\n",
    "10 osztály esetén ez 10 különböző logisztikus regresszió illesztését jelenti, amelyek egyenként az adott osztályra adnak becslést a **One vs. All** struktúrának megfelelően. A gyakorlaton használt adatokra 10 független logisztikus regresszió illesztésével a kézzel írott számjegyek felismerésének feladata (több-kevesebb pontossággal) megoldható a **One vs. All** struktúrát alkalmazva.\n",
    "\n",
    "<!---\n",
    "<center><img src=\"img/oneVSall.svg\" width=\"400\"></center>\n",
    "-->\n",
    "<center><img src=\"https://raw.githubusercontent.com/MOGI-AI/adaptiv_labor/main/Lab06_multiClassNN/img/oneVSall.svg\" width=\"400\"></center>\n",
    "\n",
    "\n",
    "\n",
    "Észrevehető, hogy az azonos bemeneti adatokon 10 különböző bináris klasszifikációt megvalósító perceptronok képe megyegyezik egy Feed Forward neurális hálózat 10 neuront tartalmazó rétegének alakjával. Amennyiben ez a réteg az MLP modell utolsó (kimeneti) rétege, úgy egy olyan neurális háló struktúra adódik, amely a bemenet 10 különböző osztályba való tartozásnak a valószínűségét képes modellezni. A rejtett rétegek ekkor a bemenetek egyre magasabb absztrakciós szintű *'tulajdonságait'* határozzák meg, amely jól reprezentálja a bemenetet, majd a legutolsó, kimeneti réteg ezen absztrakt tulajdonságok alapján végzi el az adott osztályba való tartozás valószínűségének megbecslését. Állatok osztályozásánál például a bemeneti kép alapján elképzelhető, hogy az utolsó rejtett rétegben egyes neuron a szőrösséget, egy másik a lábak számát, míg megint egy másik a testméretet, vagy a fül alakját írja le. A klasszifikáció ezen tulajdonságok alapján történik az eredeti egy-egy pixel adatot tartalmazó változók helyett."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding\n",
    "\n",
    "A bemenetekhez tartozó címkék gyakran a bemenethez tartozó osztály számaként, vagy az osztálynak megfelelő szöveges formátumú címkeként adott (sparse encoding).\n",
    "\n",
    "$$ y_l = 3; \\qquad y_l = \\text{\"cat\"} $$\n",
    "\n",
    "A fenti modell esetében egy bemenetre a kimenet az osztályok számának megfelelő darab valószínűségi értéket fog tartalmazni egy sorvektor formájában. A modell tanításához a bemeneti adatokhoz tartozó cimkéket ennek a struktúrának megfelelően kell megadni. Amennyiben az osztályok száma $c$ :\n",
    "\n",
    "$$\\mathbf{y}_l = \n",
    "\t\\left[ \\begin{array}{cccc}\n",
    " \t\ty_{l, 1} & y_{l, 2} & \\cdots & y_{l, c}\\\\\n",
    "\t\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "Mivel a bemenet általában pontosan egy osztályba tartozik, a $y_{l, j}$ értékek közül pontosan egy lehet 1, a többi 0. Egy 4 különöző osztályt megkülönböztető példa esetén, ahol a bemenet a harmadik osztályba tartozik:\n",
    "\n",
    "$$\\mathbf{y}_l = \n",
    "\t\\left[ \\begin{array}{cccc}\n",
    " \t\t0 & 0 & 1 & 0\\\\\n",
    "\t\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "Ezt a formátumot 'one-hot' enkódolt formátumnak nevezik, mivel a lehetséges értékek közül pontosan egy érvényes, vagy 'hot'. A feladatmegoldás során a kapott címkéket ilyen formátumra kell konvertálni. Ennek implementációját segíti, hogy számmal adott osztálycímkék esetén a one-hot enkódolt érték pont megfelel egy $c$ méretű egységmátrix megfelelő sorának:\n",
    "\n",
    "$$\\mathbf{Y}_{sparse} = \\left[ \\begin{array}{c}\n",
    " \t\t 1 \\\\\n",
    "\t\t 2 \\\\\n",
    "\t\t 3 \\\\\n",
    "\t\t 4 \\\\\n",
    "\t\\end{array}\\right] \\Rightarrow\n",
    "\n",
    "\\mathbf{Y}_{one-hot} = \\left[ \\begin{array}{cccc}\n",
    " \t\t 1 & 0 & 0 & 0\\\\\n",
    "\t\t 0 & 1 & 0 & 0\\\\\n",
    "\t\t 0 & 0 & 1 & 0\\\\\n",
    "\t\t 0 & 0 & 0 & 1\\\\\n",
    "\t\\end{array}\\right]$$\n",
    "\n",
    "Szövegként megadott címkék esetén hasonlóan járhatunk el, ha egy listába rendezzük a különböző címkéket, majd az adott osztály listában elfoglalt helye szerint rendeljük hozzá a one-hot enkódolt kimenetet."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SoftMax aktivációs függvény\n",
    "\n",
    "A kimeneti rétegben *sigmoid* aktivációs függvényt alkalmazva a modell minden osztályba való tartozásra meghatároz egy valószínűséget. A single label osztályozás feladattípusnál (egy bemenet pontosan egy osztályba tartozik) az összes osztályra vett valószínűségek összegének egyet kell adnia, mivel annak a valószínűsége, hogy a bemenet az adott osztályok közül valamelyikbe tartozik pontosan egy (biztos esemény). Matematikailag azonban semmi nem akadályozza, hogy a modell egy adott bemenetre olyan valószínűségeket prediktáljon, melyek összege nem egy (például állatokról készült képek klasszifikációjánál előfordulhat, hogy a predikció szerint 0.75 eséllyel kutya és 0.80 eséllyel macska). \n",
    "\n",
    "A kimeneti rétegre ezért egy más aktivációs függvényt kell alkalmazni, amely a kimeneti réteg $\\mathbf{s}_l^{(p)}$ értékei alapján úgy határozza meg a kimeneteket, hogy azok tartománya [0,1] legyen, összegük 1 és értékeik az $\\mathbf{s}_l^{(p)}$ értékekkel nem feltétlenül lineárisan, de arányosak (nagyobb $s$ értékhez nagyobb kimeneti valószínűség tartozzon). Jó megoldásnak tűnhet erre, ha a az adott kimenethez tartozó $s^{(p)}_{l,j_0}$ értéket az adatponthoz tartozó összes $s^{(p)}_{l,j}$ érték összegével normáljuk:\n",
    "\n",
    "$$\\hat y_{l,j_0} = a(s^{(p)}_{l,j_0}) = \\frac{s^{(p)}_{l,j_0}}{\\sum_{j=1}^c s^{(p)}_{l,j}}$$\n",
    "\n",
    "Amíg minden $s^{(p)}_{l,j}$ érték pozitív, ez teljesíti a követelményeket, miszerint\n",
    "\n",
    "$$ \\sum_{j=1}^c \\hat y_{l,j_0} = 1$$\n",
    "és\n",
    "$$ a(s^{(p)}_{l,j_a}) > a(s^{(p)}_{l,j_b}) \\quad \\text{ha} \\quad s^{(p)}_{l,j_a} > s^{(p)}_{l,j_b}$$\n",
    "\n",
    "Azonban ha az $s^{(p)}_{l,j}$ értékek között szerepel negatív is, a feltételek sérülnek. Erdemes tehát először ezeket az összegeket egy olyan függvénnyel transzformálni, amely a $[-\\infty, +\\infty]$ tartományt a $[0, +\\infty]$ tartományra képezi, és szigorúan monoton növekvő függvény. Ezeknek a feltételeknek eleget tesz az exponenciális függvény, amellyel az új aktivációs függvény:\n",
    "\n",
    "$$ a(s^{(p)}_{l,j_0}) = \\frac{e^{s^{(p)}_{l,j_0}}}{\\sum_{j=1}^c e^{s^{(p)}_{l,j}}}$$\n",
    "\n",
    "Az így kapott *softmax* aktivációs függvény (amelyet kizárólag a kimeneti rétegben szokás alkalmazni többosztályos klasszifikáció esetén) teljesíti mind a valószínűségek összegére, mind a bemenetekkel való arányosságra vonatkozó követelményeket:\n",
    "\n",
    "$$ \\boxed{softmax(z_{j_0}) = \\frac{e^{z_{j_0}}}{\\sum_{j=1}^c e^{z_j}}}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kategórikus keresztentrópia\n",
    "\n",
    "A több kategóriás klasszifikációhoz a bináris klasszifikáció - BCE költséghez hasonlóan  a *Maximum Likelihood* becslést alklamazva rendelhető költségfüggvény. A modell paramétereitől függően az összes $m$ adatpontra a likelihood függvény $\\mathcal{L}(\\mathbf{w} \\ | \\ \\mathbf{X})$ az egyes adatpontok valószínűségének szorzata:\n",
    "\n",
    "$$ \\mathcal{L}(\\mathbf{w} \\ | \\ \\mathbf{X}) = \\prod_{l=1}^m  \\mathcal{L}(\\mathbf{w} \\ | \\ \\mathbf{x}_{l}) $$\n",
    "\n",
    "ahol $\\mathbf{w}$ itt a modell *összes* paraméterét jelöli, $\\mathbf{X}$ az összes bemeneti adatpont minden változóját tartalmazó mátrix, míg $\\mathbf{X}_{l}$ az $l$-edik adatpont változóit tartalmazó (sor)vektor.\n",
    "A modell szerint annak a valószínűsége, hogy a $\\mathbf{w}$ modellparaméterek mellett a bemenetekkel leírható adatpont annak a tényleges osztályába tartozik, a modell adott osztályra prediktált valószínűsége lesz:\n",
    "\n",
    "$$ \\mathcal{L}(\\mathbf{w} \\ | \\ \\mathbf{x}_{l}) = \\hat y_{l,j_{l}} $$\n",
    "\n",
    "ahol $j_{l}$ az $l$-edik adatpont valós osztálya.\n",
    "\n",
    "A bináris keresztentrópiához hasonlóan a likelihood helyett annak logaritmusának, a log-likelihood függvény alakja:\n",
    "\n",
    "$$ ln \\left(\\mathcal{L}(\\mathbf{w} \\ | \\ \\mathbf{X})\\right) = ln \\left( \\prod_{l=1}^m  \\hat y_{l,j_{l}} \\right) $$\n",
    "\n",
    "A $\\log(a \\cdot b) = \\log(a)+\\log(b)$ azonosság alapján:\n",
    "\n",
    "$$ ln \\left(\\mathcal{L}(\\mathbf{w} \\ | \\ \\mathbf{X})\\right) =  \\sum_{l=1}^m ln \\left( \\hat y_{l,j_{l}} \\right) $$\n",
    "\n",
    "A $\\hat y_{l,j_{l}}$ specifikus kiválasztása helyett a one-hot enkódolt kimeneti vektorokat felhasználva\n",
    "\n",
    "$$ln \\left(\\hat y_{l,j_{l}} \\right) = \\sum_{j=1}^c y_{l,j} \\ ln \\left(\\hat y_{l,j} \\right)$$  \n",
    "\n",
    "ahol $c$ a feladathoz tartozó osztályok száma. A teljes log-likelihood függvény így:\n",
    "\n",
    "$$ ln \\left(\\mathcal{L}(\\mathbf{w} \\ | \\ \\mathbf{X})\\right) =  \\sum_{l=1}^m \\sum_{j=1}^c \\left( y_{l,j} \\ ln \\left(\\hat y_{l,j} \\right) \\right) $$\n",
    "\n",
    "A log-likelihood függvény maximalizálása megegyezik a a függvény negatívjának minimalizálsával, így a kategórikus keresztentrópia (Categorical Cross-Entropy) költségfüggvény értéke, a bemenetek számától függetlenítve:\n",
    "\n",
    "$$ \\boxed{C_{CCE} = \\frac{1}{m} \\sum_{l=1}^m \\sum_{j=1}^c \\left( - y_{l,j} \\ ln \\left(\\hat y_{l,j} \\right) \\right) }$$\n",
    "\n",
    "Észrevehető, hogy a Bináris Keresztentrópia a Kategórikus keresztentrópia egy speciális esete, ahol $c = 2$ (a két osztály: adott esetmény megtörténik, és adott esemény nem történik meg), illetve minden esetre a két valószínűség $\\hat y_{l,2} = 1- \\hat y_{l,1}$, és értelemszerűen a valós kimenetekre is igaz, hogy $\\hat y_{l,2} = 1 - \\hat y_{l,1}$. A KAtegórikus Keresztentrópia $j$ futóváltozót tartalmazó szummáját ezek alajpán felbontva és az előbbi összefüggéseket behelyettesítve megkapjuk a Bináris Kerensztentrópia költségfüggvényét:\n",
    "\n",
    "$$C_{BCE} = \\frac{1}{m} \\sum_{l=1}^m \\left(-y_l \\cdot \\ln (\\hat{y}_l) - (1-y_l) \\cdot \\ln \\left(1-\\hat{y}_l\\right) \\right)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BackProp a többosztályos klasszifikáció esetén\n",
    "\n",
    "A többosztályos klasszifikáció esetén a kimeneti réteg $\\mathbf{\\delta}^{(p)}$ értékének számítását az alkalmazott CCE költségfüggvény és a *softmax* aktivációs függvény alapján kell elvégezni. Mivel az egyes $\\mathbf{\\hat y}_{l,j}$ kimenetek a *softmax* aktiváció miatt az utolsó réteg $\\mathbf{S}_l$ vektorának minden értékétől függenek, a számítása nem triviális, de eredményként az eddigiekkel megegyező forma adódik:\n",
    "\n",
    "$$ \\mathbf{\\delta}^{(p)} = \\frac{1}{m} \\left(\\mathbf{\\hat Y} - \\mathbf{Y}\\right) $$\n",
    "\n",
    "A teljes levezetés az **Appendix** részben megtalálható.\n",
    "\n",
    "A többosztályos klasszifikációnál használt hálózat rejtett rétegeinek struktúrája az eddigiekhez képest változatlan. Az itt használható szokásos aktivációs függvények ritkán tartalmaznak a réteg többi $s$ értékétől való függést, így a backprop folyamata ezekre a rétegekre változatlan:\n",
    "\n",
    "$$ \\mathbf{\\delta^{(k)}} = \\left( \\mathbf \\delta^{(k+1)} \\cdot \\left( \\mathbf W^{(k+1)}_{-BIAS} \\right)^T \\right) a'\\left(\\mathbf s^{(k)} \\right) $$\n",
    "\n",
    "A gradiensek értéke szintén az eddigiekkel megegyezően számítható minden rétegben:\n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial \\mathbf{W}^{(k)}} = \\left(\\mathbf X^{(k)}\\right)^T \\cdot \\mathbf \\delta^{(k)} $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00: Könyvtár importálások\n",
    "\n",
    "Az első lépés a feladat megoldása során használt könyvtárak importálása.. Esetünkben ezek a következők lesznek:\n",
    "- Numpy a matematikia műveletek elvégzéséhez\n",
    "- Pandas az adatok beolvasásához és kezeléséhez\n",
    "- MatPlotLib.pyplot az eredményeink ábrázolásához\n",
    "- SciPy 'loadmat' modulja a matlab változókat tartalmazó fájlok importálására\n",
    "- SciKit-Learn confusion_matrix számoló modulja a hálózat értékeléséhez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Használjuk ezeket sötét téma esetén\n",
    "plt.style.use('dark_background')\n",
    "styleTemplate = 'plotly_dark'\n",
    "\n",
    "# Használjuk ezeket világos téma esetén\n",
    "#plt.style.use('default')\n",
    "#styleTemplate = 'plotly_white'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01: Adatbeolvasás\n",
    "Az adatok az  `mnist.mat` matlab változófájlban találhatók, ami a SciPy `loadmat` moduljával importálható."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = loadmat(\"mnist.mat\")                 # .mat file importálás\n",
    "X = mat[\"X\"]                               # X rendezése\n",
    "Y = mat[\"y\"]                               # Y rebbdezése\n",
    "m = X.shape[0]                             # minták száma\n",
    "n = X.shape[1]                             # változók száma\n",
    "c = np.unique(Y).size                      # osztályok száma\n",
    "\n",
    "print('''Shape of the dataset in order X and Y:\n",
    "''',X.shape,'\\n',Y.shape,'\\n')\n",
    "\n",
    "print('''Number of unique categories in Y:\n",
    "''', c)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02: Adatfelfedezés\n",
    "\n",
    "A beolvasott adatoksor 5000 mintát tartalmaz. A bemenet 400 változót tartalmaz, amelyek egyenként egy 20x20-as kép egyes pixeleit tartalmazzák egy kiterített vektorban, az alábbihoz hasonló logika szerint (a jelen adatok sorok helyett oszloponként vannak kiterítve - *Fortran-like index order*). \n",
    "<!--\n",
    "<center><img src=\"img/img_flatten.svg\" width=\"455\"></center>\n",
    "-->\n",
    "<center><img src=\"https://raw.githubusercontent.com/MOGI-AI/adaptiv_labor/main/Lab06_multiClassNN/img/img_flatten.svg\" width=\"455\"></center>\n",
    "\n",
    "A modellválasztás nem feltétlenül optimális, mivel a Feed forward neurális háló bemenetei között nincs struktúra feltételezve, véletlenszerű pixlesorrendben ugyan annyi infromációt tudna érzékelni. A modell a síkbeli struktúrát nem fogja jól érzékelni (melyik pixel melyik felett/mellett/alatt van), azonban a feladat egyszerűsége miatt még így is eredményes. Magasabb komplexitás esetén érdemes konvolúciós neurális háló struktúrát alkalmazni kép jellegű bemenetekre, amely képes az egyes pixelek egymáshoz képesti elhelyezkedését is figyelembe venni.\n",
    "\n",
    "Az adatfelfedezéshez először vizsgáljuk meg a címkék formátumát."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Az adatokban található osztályok: {}\".format(np.unique(Y)))\n",
    "\n",
    "print(\"Az első 10 mintához tartozó címkék: {}\".format(Y[:10,:].flatten())) # kiíratás sorvektorként"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ez alapján megállíptható, hogy 10 különböző osztály található (0-9 számjegyek), láthatóan rendezett sorrendben. Ábrázoljunk egy pár véletlen mintát, a hozzájuk tartozó címkékkel, illetve vizsgáljuk meg az egyes pixelértékek tartományát."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"15 véletlenszerű minta ...\")                             # kirajzoltatjuk az első 100 elemet\n",
    "rng = np.random.default_rng(42)                                 # véletlenszám generátor\n",
    "samples = rng.integers(m, size=15)\n",
    "fig, axis = plt.subplots(1,15, figsize = (10,4))                # mivel egy plotra akarunk több ábrát -> subplots\n",
    "for i in range(15):                                             # 15 kis ábra\n",
    "    pixels = X[samples[i],:].reshape([20,20], order='F')        # 20x20 alak visszaállítása, Fortran index order\n",
    "    axis[i].imshow(pixels)                                      # kirajzolás\n",
    "    axis[i].axis(\"off\")\n",
    "    axis[i].set_title(Y[samples[i],0])\n",
    "plt.show()\n",
    "\n",
    "print(\"A bemeneti adatok (pixelértékek) tartománya: [{}, {}]\".format(np.min(X), np.max(X)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Az ábrázolás alapján látszik, hogy (feltehetően a matlabban szokásos indexelés miatt) a 0-ás számjegyhez a 10-es címke tartozik. A bemeneti adatok bár nem a megszokott 0..1 vagy 0..255 tartományon vannak, nagyságrendük nem igényli a normalizálást.\n",
    "\n",
    "**Megjegyzés:** a képek szürkeárnyalatos formában adottak (1 érték pixelenként), a színskálát a matplotlib rendeli hozzá."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03: Adatok előkészítése\n",
    "\n",
    "Általában Nem szerencsés a tanításra használt adatokat rendezett módon megadni (ennek jelentősége akkor nagyobb, ha a súlymódosítást nem a teljes adathalmazra számoljuk, hanem 'batch' módban, egy-egy kissebb részre külön-külön végezzük el). Az adatok előkészítése így jelen esestben a következő lépésekből áll:\n",
    "- adatok sorrendjének randomizálása\n",
    "- 10-es címkék javítása 0-ra\n",
    "- one-hot enkódolt kimenetek előállítása"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "shuffledData = np.hstack([X,Y])\n",
    "rng.shuffle(shuffledData, axis = 0)\n",
    "X = shuffledData[:,:400]\n",
    "Y = shuffledData[:,400:].astype('uint8')\n",
    "\n",
    "# 10->0\n",
    "Y[Y[:,0]==10, 0] = 0\n",
    "\n",
    "# Méretek és keverés sikerességének ellenőrzése\n",
    "print('''Shape of the dataset in order X and Y:\n",
    "''',X.shape,'\\n',Y.shape,'\\n')\n",
    "print(\"Az első 20 mintához tartozó címke: {}\".format(Y[:20,:].flatten())) # kiíratás sorvektorként"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Láthatóan sikeres volt az elemek keverése és a 0 számjegyekhez tartozó címkék cseréje.\n",
    "\n",
    "**Feladat:** állítsa elő a címkéknek megfelelő one-hot enkódolt kimeneti mátrixot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "\n",
    "\n",
    "######################################################\n",
    "\n",
    "print(\"Az első 6 mintához várt one-hot enkódolt címkék:\")\n",
    "print('''[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
    " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
    " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
    " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
    " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]''')\n",
    "print(\"Az első 6 mintához tartozó tényleges one-hot enkódolt címkék:\")\n",
    "print(Y_oneHot[:6,:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04: Modell implementálása\n",
    "\n",
    "Az előző órán elkészített egyszerű neurális háló modellből kiindulva végezzük el a szükséges módosításokat a többosztályos klasszifikáció implementálásához."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z, derivate = False):       # Alapértelmezett a rendes sigmoid érték számítása\n",
    "    g = 1 / (1 + np.exp(-z))\n",
    "    if derivate == True:\n",
    "        g = g * (1-g)\n",
    "    return g"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feladat:** implementálja a `softmax()` aktivációs függvényt, amely a bemenetként adott $\\mathbf{S}^{(p)}$ mátrixra kiszámítja a kimeneti $\\mathbf{\\hat Y}^{(p)}$ mátrixot a *softmax* aktivációnak megfelelően!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):       # Alapértelmezett a rendes sigmoid érték számítása\n",
    "    ######################################    \n",
    "\n",
    "\n",
    "\n",
    "    ######################################\n",
    "    return g\n",
    "\n",
    "# Test softmax\n",
    "z = np.array([[2.52640812, 4.75720026, 5.76544649],\n",
    "       [4.93992941, 6.70753489, 4.90961306],\n",
    "       [5.64891358, 5.49916996, 4.72750292],\n",
    "       [5.26816593, 6.27247156, 2.79979483]])\n",
    "\n",
    "print()\n",
    "print('''Expected softmax outputs for test array:\n",
    "[[0.02792016 0.2598595  0.71222034]\n",
    " [0.12776367 0.74828786 0.12394847]\n",
    " [0.4426961  0.38112977 0.17617413]\n",
    " [0.26214149 0.71564916 0.02220935]]''')\n",
    "print('Actual softmax outputs for test array:')\n",
    "print(softmax(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addBias(z):\n",
    "    return np.hstack([np.ones([z.shape[0] ,1]), z])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feladat:** végezze el a szükséges változtatásokat az alábbi bináris klasszifikációra implementált neurális háló osztályon, hogy az képes legyen a többosztályos klasszifikáció elvégzésére!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN:\n",
    "    def __init__(self, layerSizes, activationFunction, seed):\n",
    "        self.layerSizes = layerSizes            # Háló konfigurációja\n",
    "        self.a = activationFunction             # Aktivációs függvény (később más aktivációs függvény is alkalmazható)\n",
    "        self.noLayers = len(layerSizes)-1       # Rétegek száma (iterációhoz, bemeneti réteg nélkül)\n",
    "        self.inputSize = layerSizes[0]          # Bemeneti változók (input feature) száma\n",
    "        self.Yhat = []                          # Lista az egyes rétegek kimenetének tárolására\n",
    "        self.X = []                             # Lista az egyes rétegek bemenetének tárolására\n",
    "        self.W = []                             # Lista a rétegekhez tartozó súlyoknak\n",
    "        self.initWeights(seed)                  # Súlyok inicializálása\n",
    "    \n",
    "    def initWeights(self, seed = None):\n",
    "        rng = np.random.default_rng(seed)\n",
    "        # Weight matrix dimension are based on number of neurons in previous and current layer\n",
    "        self.W = [rng.uniform(low = -1, high = 1, size = [self.layerSizes[k]+1, self.layerSizes[k+1]]) for k in range(self.noLayers)]\n",
    "\n",
    "    def checkInputSize(self, X):\n",
    "        if X.shape[1] != self.inputSize:\n",
    "            raise ValueError('Unexpected number of input features! Expected {} features but got {}.'.format(self.inputSize, X.shape[1])) \n",
    "\n",
    "    def forwardProp(self, X):\n",
    "        self.checkInputSize(X)\n",
    "\n",
    "        self.X = [[] for i in range(self.noLayers)]\n",
    "        self.Yhat = [[] for i in range(self.noLayers)]\n",
    "        \n",
    "        self.X[0] = addBias(X)  # FirstLayer\n",
    "        self.Yhat[0] = self.a(self.X[0]@self.W[0])\n",
    "\n",
    "        for k in range(1, self.noLayers-1): # HiddenLayers\n",
    "            self.X[k] = addBias(self.Yhat[k-1])\n",
    "            self.Yhat[k] = self.a(self.X[k]@self.W[k])\n",
    "\n",
    "        self.X[-1] = addBias(self.Yhat[-2])\n",
    "        self.Yhat[-1] = sigmoid(self.X[-1]@self.W[-1])\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.forwardProp(X)\n",
    "        return self.Yhat[-1]\n",
    "\n",
    "    def costBCE(self, X, Y):\n",
    "        eps = 10e-15\n",
    "        self.forwardProp(X)\n",
    "        return np.mean(-Y*np.log(self.Yhat[-1]+eps)-(1-Y)*np.log(1-self.Yhat[-1]+eps))\n",
    "\n",
    "    def backProp(self, trueY):\n",
    "        deltas = [[] for i in range(self.noLayers)]\n",
    "        m = trueY.shape[0]\n",
    "\n",
    "        # Delta a kimeneti rétegben\n",
    "        deltas[-1] = 1/m * (self.Yhat[-1] - trueY)\n",
    "        # Delta kiszámítása a rejtett réteg(ek)ben\n",
    "        for k in range(self.noLayers-2, -1, -1):\n",
    "            deltas[k] = (deltas[k+1]@self.W[k+1][1:,:].T) * self.a(self.X[k]@self.W[k], derivate = True)\n",
    "        \n",
    "        dW = [self.X[k].T@deltas[k] for k in range(self.noLayers)]\n",
    "        return dW\n",
    "\n",
    "    def updateWeights(self, learning_rate, dW):\n",
    "        for k in range(self.noLayers):\n",
    "            self.W[k] = self.W[k] - learning_rate * dW[k]\n",
    "\n",
    "    def fit(self, X, Y, learning_rate, epochs):\n",
    "        C_history = np.zeros([epochs+1])\n",
    "        C_history[0] = self.costBCE(X, Y)    # Forwardprop megtörténik\n",
    "        print('''\n",
    "        \\%\\%\\% ------- TANÍTÁS ------- \\%\\%\\%\n",
    "        ''')\n",
    "        for i in range(epochs):\n",
    "            dW = self.backProp(Y)\n",
    "            self.updateWeights(learning_rate, dW)\n",
    "            C_history[i+1] = self.costBCE(X,Y) # Forwardprop megtörténik\n",
    "\n",
    "            if ((i+1) % 100) == 0:\n",
    "                \n",
    "                print('Epoch {} / {} completed. Cost value:{}'.format(i+1, epochs, C_history[i+1])) \n",
    "\n",
    "        return C_history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A modell helyes működése az alábbi tesztek futtatásával ellenőrizhető:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layerSizes = [2, 3, 4]\n",
    "seed = 42\n",
    "\n",
    "testNN = FeedForwardNN(layerSizes, sigmoid, seed)\n",
    "\n",
    "testX = np.array([[0.3146, -0.65432], [-1.0123, -0.4215], [0.2351, 0.7533456]])\n",
    "testY = np.array([[0, 0, 1, 0], [1, 0, 0, 0], [0, 1, 0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test initweights\n",
    "print('''Expected initial weights with layer sizes {0} and random seed {1}:\n",
    "[array([[ 0.5479121 , -0.12224312,  0.71719584],\n",
    "       [ 0.39473606, -0.8116453 ,  0.9512447 ],\n",
    "       [ 0.5222794 ,  0.57212861, -0.74377273]]), array([[-0.09922812, -0.25840395,  0.85352998,  0.28773024],\n",
    "       [ 0.64552323, -0.1131716 , -0.54552256,  0.10916957],\n",
    "       [-0.87236549,  0.65526234,  0.2633288 ,  0.51617548],\n",
    "       [-0.29094806,  0.94139605,  0.78624224,  0.55676699]])]'''.format(layerSizes, seed))\n",
    "print('''Actual initial weights with layer sizes {0} and random seed {1}:\n",
    "{2}'''.format(layerSizes, seed, testNN.W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forwardProp & Predict\n",
    "testPred = testNN.predict(testX)\n",
    "\n",
    "print('''Expected prediction for test parameters:\n",
    "[[0.08834134 0.21661991 0.39782789 0.29721086]\n",
    " [0.07640996 0.21790532 0.39044155 0.31524317]\n",
    " [0.09595343 0.21828394 0.35748864 0.32827399]]'''.format(layerSizes, seed))\n",
    "print('''Actual prediction for test parameters:\n",
    "{0}'''.format(testPred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test backprop\n",
    "testNN.forwardProp(testX)\n",
    "testdW = testNN.backProp(testY)\n",
    "\n",
    "print('''Expected weight change values for test parameters:\n",
    "[array([[-0.03519045,  0.07277344,  0.04905192],\n",
    "       [ 0.07725816, -0.10234505, -0.08827836],\n",
    "       [ 0.00673892, -0.06410875, -0.04750743]]), array([[-0.24643176, -0.11573027,  0.04858602,  0.31357601],\n",
    "       [-0.10767695, -0.11520305,  0.03385651,  0.18902349],\n",
    "       [-0.16222644, -0.07032992,  0.07850704,  0.15404933],\n",
    "       [-0.11605659, -0.05816385, -0.02614065,  0.20036109]])]''')\n",
    "\n",
    "print('''Actual prediction for test parameters:\n",
    "{0}'''.format(testdW))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05: Modell tanítása\n",
    "\n",
    "Amennyiben minden tesztünkön megfelelő eredményt kaptunk, végezzük el a háló tanítását a `.fit()` metódus segítségével, és ábrázoljuk a költségfüggvény alakulását."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.5\n",
    "trainNN = FeedForwardNN([n, 25, c], sigmoid, 42)\n",
    "\n",
    "C_history = trainNN.fit(X, Y_oneHot, learning_rate, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(C_history.size), C_history) \n",
    "plt.ylabel('Cost value [-]')\n",
    "plt.xlabel('epochs')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06: Modell értékelése\n",
    "\n",
    "Jelen feladatnál már nehezen megoldható döntési határok vagy illesztett felületek vizsgálata. A modell értékelésének egy lehetséges módja egy véletlenszerű adatpont kiválasztása, és a modell kimenetének összevetése a valósággal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rndID = rng.integers(m)\n",
    "pred = trainNN.predict(X[rndID, :].reshape(1,400))\n",
    "\n",
    "plt.imshow(X[rndID, :].reshape([20,20], order='F'))\n",
    "plt.show\n",
    "\n",
    "print('Real label of data: {}'.format(Y[rndID,0]))\n",
    "print('Predicted label of data: {} --> {}'.format(np.argmax(pred[0]), pred[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A klasszifikáció értékelésénk egy másik lehetősége a konfúziós mátrix ábrázolása, amely megmutatja a valós és prediktált osztályok közötti kapcsolatokat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = trainNN.predict(X)\n",
    "conf = confusion_matrix(Y, np.argmax(preds, axis = 1))  # Sparse predicitons\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (6,6))\n",
    "im = ax.imshow(conf)\n",
    "\n",
    "ax.set_xticks(np.arange(c), labels=np.unique(Y))\n",
    "ax.set_xlabel('Prediktált számjegy')\n",
    "ax.set_yticks(np.arange(c), labels=np.unique(Y))\n",
    "ax.set_ylabel('Valós számjegy')\n",
    "\n",
    "# Annotate cells\n",
    "for i in range(c):\n",
    "    for j in range(c):\n",
    "        text = ax.text(j, i, conf[i, j],\n",
    "                       ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "ax.set_title(\"Kézzel írott számjegyek konfúziós mátrixa\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 07: További tesztelés\n",
    "\n",
    "Érdemes a modellt tovább vizsgálni, megfigyelni a tanítás paramétereinek hatásást, vagy akár implementálni a tanítóadatok szétbontását tanító és validációs halmazokra és megvizsgálni a tanítás korai leállásának lehetőségét a validációs adatokon számított költségfüggvény alapján."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XX: Megoldás Keras könyvtár segítségével\n",
    "\n",
    "A bináris klasszifikációhoz hasonlóan ezt a feladatot is megodldhatjuk a amgaszintű Keras könyvtár segítségével."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for array-handling and plotting\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# keras imports for building and training our neural network\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Adatok előkészítése\n",
    "mat = loadmat(\"mnist.mat\")                 # .mat file importálás\n",
    "X = mat[\"X\"]                               # X rendezése\n",
    "Y = mat[\"y\"]                               # Y rendezése\n",
    "c = np.unique(Y).size\n",
    "Y[Y[:,0]==10, 0] = 0                       # 10 -> 0 label csere\n",
    "oneHotMatrix = np.eye(c)\n",
    "Y_oneHot = oneHotMatrix[Y[:,0],:]          # onehot Mátrix\n",
    "\n",
    "# Keras háló definiálása\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_shape=(400,)))\n",
    "model.add(Activation('sigmoid'))                            \n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compiling the sequential model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=SGD(learning_rate=0.5))\n",
    "\n",
    "# training the model and saving metrics in history \n",
    "history = model.fit(X, Y_oneHot, epochs=10, verbose = 2) # Stochastic gradient descent can learn in a lot fewer epochs\n",
    "\n",
    "# Plot metrics\n",
    "fig, axis = plt.subplots(2,1)     \n",
    "\n",
    "axis[0].plot(history.history['loss'])\n",
    "axis[0].set_ylabel('Cost value [-]')\n",
    "\n",
    "axis[1].plot(history.history['accuracy'])\n",
    "axis[1].set_ylabel('Accuracy [%]')\n",
    "axis[1].set_xlabel('epochs')\n",
    "plt.show()\n",
    "\n",
    "# Plot confusion matrix\n",
    "preds = model.predict(X)\n",
    "conf = confusion_matrix(Y, np.argmax(preds, axis = 1))\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (6,6))\n",
    "im = ax.imshow(conf)\n",
    "\n",
    "ax.set_xticks(np.arange(c), labels=np.unique(Y))\n",
    "ax.set_xlabel('Prediktált számjegy')\n",
    "ax.set_yticks(np.arange(c), labels=np.unique(Y))\n",
    "ax.set_ylabel('Valós számjegy')\n",
    "\n",
    "# Annotate cells\n",
    "for i in range(c):\n",
    "    for j in range(c):\n",
    "        text = ax.text(j, i, conf[i, j],\n",
    "                       ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "ax.set_title(\"Kézzel írott számjegyek konfúziós mátrixa\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "## BackProp a kimeneti rétegre multiclass classification feladat esetében\n",
    "\n",
    "A backpropagation algoritmus implementálásához meg kell határoznunk a kimeneti réteghez tartozó $\\mathbf{\\delta}^{(p)}$ értékeket, amely alapján a háló többi $\\mathbf{\\delta}^{(k)}$ értéke számolható. Több kimeneti neuron esetében az egyes $\\delta$ értékek meghatározása:\n",
    "\n",
    "$$ \\delta^{(p)}_{l,j} = \\frac{\\partial C}{\\partial s^{(p)}_{l,j}}$$\n",
    "\n",
    "**Megjegyzés**: a bináris klasszifikáció esetén ezt a kifejezést a lánc szabály segítségével tovább bonthatuk, mivel adatpontonként egyetlen $\\hat y_{l}$ kimenet volt, amely kizárólag egy $s^{(p)}_{l}$ értéktől függött.\n",
    "\n",
    "Multiclass classification feladat esetén a költségfüggvény a kategórikus keresztentrópia, az aktivációs függvény pedig a softmax függvény:\n",
    "\n",
    "$$\\frac{\\partial C_{CCE}}{\\partial s_{l_0,j_0}} = \\frac{\\partial \\frac{1}{m} \\sum_{l=1}^m \\sum_{j=1}^c \\left( - y_{l,j} \\ ln \\left(\\hat y_{l,j} \\right) \\right)}{\\partial s^{(p)}_{l_0,j_0}} = \\frac{1}{m} \\sum_{l=1}^m \\sum_{j=1}^c \\left( - y_{l,j} \\frac{\\partial ln \\left(\\hat y_{l,j} \\right) }{\\partial s^{(p)}_{l_0,j_0}} \\right)$$\n",
    "\n",
    "Az összetett függvény deriválási szabája szerint:\n",
    "\n",
    "$$\\frac{1}{m} \\sum_{l=1}^m \\sum_{j=1}^c \\left( - y_{l,j} \\frac{\\partial ln \\left(\\hat y_{l,j} \\right) }{\\partial s^{(p)}_{l_0,j_0}} \\right) = \\frac{1}{m} \\sum_{l=1}^m \\sum_{j=1}^c \\left( - y_{l,j} \\ \\frac{1}{\\hat y_{l,j}} \\frac{\\partial \\hat y_{l,j}}{\\partial s^{(p)}_{l_0,j_0}} \\right)$$\n",
    "\n",
    "Amennyiben $l \\neq l_0$, úgy $\\frac{\\partial \\hat y_{l,j}}{\\partial s^{(p)}_{l_0,j_0}} = 0$, azaz egy bemeneti adatpont eredményei függetlenek a többi adatpont eredményeitől:\n",
    "\n",
    "$$ \\frac{1}{m} \\sum_{l=1}^m \\sum_{j=1}^c \\left( - y_{l,j} \\ \\frac{1}{\\hat y_{l,j}} \\frac{\\partial \\hat y_{l,j}}{\\partial s^{(p)}_{l_0,j_0}} \\right) = \\frac{1}{m} \\sum_{j=1}^c \\left( - y_{l_0,j} \\ \\frac{1}{\\hat y_{l_0,j}} \\frac{\\partial \\hat y_{l_0,j}}{\\partial s^{(p)}_{l_0,j_0}} \\right)$$ \n",
    "\n",
    "A single-label classification esetén (egy kimenet csak egy kategóriához tartozhat) az utolsó réteg aktivációs függvénye a *softmax*, így\n",
    "\n",
    "$$ \\hat y_{l_0,j} = softmax(s^{(p)}_{l_0,j}) = \\frac{e^{s^{(p)}_{l_0,j}}}{\\sum_{j'=1}^{c} e^{s^{(p)}_{l_0,j'}}} $$\n",
    "\n",
    "A törtfüggvény deriválásának szabálya $\\left(\\frac{f(x)}{g(x)}\\right)' = \\frac{f'(x)g(x)-g'(x)f(x)}{g^2(x)}$ alapján:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial z_{j_0}}softmax(z_{j}) = \\frac{\\partial}{\\partial z_{j_0}}\\left(\\frac{e^{z_{j}}}{\\sum_{j'=1}^{c} e^{z_j'}}\\right)$$\n",
    "\n",
    "Az egyes részfüggvények deriváltjai:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial z_{j_0}} e^{z_{j}} =\n",
    "\\begin{cases}\n",
    "    e^{z_{j_0}}, & \\text{ha}\\ j = j_0 \\\\\n",
    "    0, & \\text{ha}\\ j \\neq j_0\n",
    "\\end{cases}e^{z_{j_0}} $$\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial z_{j_0}} \\sum_{j'=1}^{c} e^{z'_j} = \\sum_{j=1}^{c} \\frac{\\partial}{\\partial z_{j_0}}e^{z'_{j}} = e^{z_{j_0}}$$\n",
    "\n",
    "Ezekből\n",
    "\n",
    "$$ \\frac{\\partial \\hat y_{l_0,j}}{\\partial s^{(p)}_{l_0,j_0}} = \\frac{\\partial}{\\partial s^{(p)}_{l_0,j_0}}softmax(s^{(p)}_{l_0,j}) = \\frac{\\partial}{\\partial s^{(p)}_{l_0,j_0}}\\left(\\frac{e^{s^{(p)}_{l_0,j}}}{\\sum_{j'=1}^{c} e^{s^{(p)}_{l_0,j'}}}\\right) $$\n",
    "\n",
    "Ha $j = j_0$:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial s^{(p)}_{l_0,j_0}}\\left(\\frac{e^{s^{(p)}_{l_0,j}}}{\\sum_{j'=1}^{c} e^{s^{(p)}_{l_0,j'}}}\\right) = \\frac{e^{s^{(p)}_{l_0,j_0}} \\ \\sum_{j'=1}^{c} e^{s^{(p)}_{l_0,j'}} - e^{s^{(p)}_{l_0,j_0}} \\ e^{s^{(p)}_{l_0,j}}}{\\left( \\sum_{j'=1}^{c} e^{s^{(p)}_{l_0,j'}} \\right)^2} = \\\\\n",
    "= \\frac{e^{s^{(p)}_{l_0,j_0}}}{\\sum_{j'=1}^{c} e^{s^{(p)}_{l_0,j'}}} \\frac{\\sum_{j'=1}^{c} e^{s^{(p)}_{l_0,j'}} - e^{s^{(p)}_{l_0,j}}}{\\sum_{j'=1}^{c} e^{s^{(p)}_{l_0,j'}}} = \\frac{e^{s^{(p)}_{l_0,j_0}}}{\\sum_{j'=1}^{c} e^{s^{(p)}_{l_0,j'}}} \\left(\\frac{\\sum_{j'=1}^{c} e^{s^{(p)}_{l_0,j'}}}{\\sum_{j'=1}^{c} e^{s^{(p)}_{l_0,j'}}} - \\frac{ e^{s^{(p)}_{l_0,j}}}{\\sum_{j'=1}^{c} e^{s^{(p)}_{l_0,j'}}}\\right) = $$\n",
    "$$ \\\\[20pt] $$\n",
    "$$ = \\hat y_{l_0,j_0} \\ \\left(1 -\\hat y_{l_0,j}\\right) = \\hat y_{l_0,j_0} - \\hat y_{l_0,j_0} \\ \\hat y_{l_0,j}$$\n",
    "\n",
    "Ha $j \\neq j_0$:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial s^{(p)}_{l_0,j_0}}\\left(\\frac{e^{s^{(p)}_{l_0,j}}}{\\sum_{j'=1}^{c} e^{s^{(p)}_{l_0,j'}}}\\right) = \\frac{- e^{s^{(p)}_{l_0,j_0}} \\ e^{s^{(p)}_{l_0,j}}}{\\left( \\sum_{j'=1}^{c} e^{s^{(p)}_{l_0,j'}} \\right)^2} = \\frac{e^{s^{(p)}_{l_0,j_0}}}{\\sum_{j'=1}^{c} e^{s^{(p)}_{l_0,j'}}} \\frac{- e^{s^{(p)}_{l_0,j}}}{\\sum_{j'=1}^{c} e^{s^{(p)}_{l_0,j'}}} = - \\hat y_{l_0,j_0} \\ \\hat y_{l_0,j}$$\n",
    "\n",
    "**Megjegyzés:** Az így kiszámított értékek gyakorlatilag az $ \\hat y_{l_0,j} = softmax(s^{(p)}_{l_0,j})$ vektor-vektor függvény Jakobi mátrixának elemei:\n",
    "\n",
    "$$\\mathbf{J}\\left(\\hat y_{l_0,j}\\right) = \\left[\n",
    "\t\\begin{array}{cccc}\n",
    " \t\t\\frac{\\partial \\hat y_{l_0,1}}{\\partial s^{(p)}_{l_0,1}} & \\frac{\\partial \\hat y_{l_0,1}}{\\partial s^{(p)}_{l_0,2}} & \\ldots & \\frac{\\partial \\hat y_{l_0,1}}{\\partial s^{(p)}_{l_0,c}}\\\\\n",
    "\t\t\\frac{\\partial \\hat y_{l_0,2}}{\\partial s^{(p)}_{l_0,1}} & \\frac{\\partial \\hat y_{l_0,2}}{\\partial s^{(p)}_{l_0,2}} & \\ldots & \\frac{\\partial \\hat y_{l_0,2}}{\\partial s^{(p)}_{l_0,c}}\\\\\n",
    " \t\t\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "        \\frac{\\partial \\hat y_{l_0,c}}{\\partial s^{(p)}_{l_0,1}} & \\frac{\\partial \\hat y_{l_0,c}}{\\partial s^{(p)}_{l_0,2}} & \\ldots & \\frac{\\partial \\hat y_{l_0,c}}{\\partial s^{(p)}_{l_0,c}}\\\\\n",
    "\t\\end{array}\t\\right] = $$\n",
    "$$ \\\\[20pt] $$\n",
    "$$    = \\left[\n",
    "\t\\begin{array}{cccc}\n",
    " \t\t\\hat y_{l_0,1} - \\hat y_{l_0,1} \\ \\hat y_{l_0,1} &- \\hat y_{l_0,2} \\ \\hat y_{l_0,1} & \\ldots & - \\hat y_{l_0,c} \\ \\hat y_{l_0,1}\\\\\n",
    "\t\t- \\hat y_{l_0,1} \\ \\hat y_{l_0,2} & \\hat y_{l_0,2} - \\hat y_{l_0,2} \\ \\hat y_{l_0,2} & \\ldots & - \\hat y_{l_0,c} \\ \\hat y_{l_0,2}\\\\\n",
    " \t\t\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "        - \\hat y_{l_0,1} \\ \\hat y_{l_0,c} & - \\hat y_{l_0,2} \\ \\hat y_{l_0,c} & \\ldots & \\hat y_{l_0,c} - \\hat y_{l_0,c} \\ \\hat y_{l_0,c}\\\\\n",
    "\t\\end{array}\t\\right]$$\n",
    "\n",
    "Az eredeti $\\frac{\\partial C_{CCE}}{\\partial s_{l_0,j_0}}$ érték számításához visszatérve\n",
    "\n",
    "$$ \\frac{\\partial C_{CCE}}{\\partial s_{l_0,j_0}} = \\frac{1}{m} \\sum_{j=1}^c \\left( - y_{l_0,j} \\ \\frac{1}{\\hat y_{l_0,j}} \\frac{\\partial \\hat y_{l_0,j}}{\\partial s^{(p)}_{l_0,j_0}} \\right) = \\\\\n",
    "= \\frac{1}{m} \\left(\\left( - y_{l_0,j_0} \\ \\frac{1}{\\hat y_{l_0,j_0}} \\hat y_{l_0,j_0} \\right) - \\sum_{j=1}^c \\left( - y_{l_0,j} \\ \\frac{1}{\\hat y_{l_0,j}} \\hat y_{l_0,j_0} \\ \\hat y_{l_0,j} \\right) \\right) = \\\\\n",
    "= \\frac{1}{m} \\left(- y_{l_0,j_0} - \\sum_{j=1}^c \\left( - y_{l_0,j} \\  \\hat y_{l_0,j_0} \\right) \\right) = \\frac{1}{m} \\left(\\sum_{j=1}^c y_{l_0,j} \\  \\hat y_{l_0,j_0} - y_{l_0,j_0} \\right) = \\frac{1}{m} \\left(\\hat y_{l_0,j_0} \\sum_{j=1}^c y_{l_0,j} - y_{l_0,j_0} \\right)$$\n",
    "\n",
    "Az $y_{l_0,j}$ értékek közül a one-hot enkódolásnak megfelelően pontosan egy elem értéke 1, a többi elem értéke 0, így $\\sum_{j=1}^c y_{l_0,j} = 1$. Ennek megfelelően\n",
    "\n",
    "$$ \\boxed{ \\delta^{(p)}_{l,j} = \\frac{\\partial C_{CCE}}{\\partial s_{l,j}} = \\frac{1}{m} \\left(\\hat y_{l,j} - y_{l,j} \\right) } $$\n",
    "\n",
    "Innen könnyen látható, hogy a multiclass classification feladatra, a CCE költségfüggvény és *softmax* kimeneti aktiváció függvény választással a kimeneti rétegre számított $\\mathbf{\\delta}^{(p)}$ érték mátrixos formában történő számítása megegyezik a a bináris klasszifikációnál (BCE költség és *sigmoid* kimeneti aktiváció) látottal.\n",
    "\n",
    "$$ \\boxed{ \\mathbf{\\delta}^{(p)} = \\frac{1}{m} \\left(\\mathbf{\\hat Y} - \\mathbf{Y}\\right)}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
