{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Labor: Lineáris Logisztikus regresszió\n",
    "\n",
    "A gyakorlat során megvizsgáljuk, hogy hogyan tudunk egy regressziós módszer (folyamatos kimeneti változó) segítségével bináris (igen/nem) kimenetet modellezni."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Felvételi siker:\n",
    "\n",
    "A feladatban tanulók tanulmányi eredményei alapján szeretnék előrejelezni, hogy felvételt nyernek-e egy adott oktatási intézménybe.\n",
    "\n",
    "Egy általános iskolai pedagógus szeretné megtudni, hogy mely diákoknak lenne szüksége extra segíségre és felkészülésre ahhoz, hogy sikeresen felvételizzenek egy hatosztályos gimnáziumba. Ehhez rendelkezésre áll korábbi diákjainak:\n",
    "- irodalom és matematika évvégi szintfelmérő eredmények az 5. év végéről (%)\n",
    "- sikerült-e az adot diáknak felvételt nyernie a kiszemelt gimnáziumba ([0, 1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logisztikus regresszió\n",
    "\n",
    "A már ismert lineáris regresszió esetében a magyarázó változó segítségével egy $[-\\infty; +\\infty]$ tartományon lévő, folytonos kimeneti változó modellezhető. Ezzel szemben itt egy ${0, 1}$ értékű bináris eseményt kell előrejelzni, azaz a bemeneti adatok alapján \"igen\" / \"nem\" kategóriába kell osztályozni a mintákat.\n",
    "\n",
    "Mivel ezen modellek lényege a predikció, kézenfekvő újrafogalmazása a feladatnak, ha nem közvetlenül a sikernek a tényét próbáljuk megadni, hanem az adott bemeneti adatok mellett a siker $[0, 1]$ tartományon folytonos **valószínűségét**  modellezzük:\n",
    "\n",
    "$$0 \\leq P_{\\mathbf{x}} = P(y=1|\\mathbf{x}) \\leq 1$$\n",
    "\n",
    "Amennyiben találunk valamilyen transzformációt, amellyel a $P_{\\mathbf{x}}$ változót a $[0, 1]$ tartományról a $[0, +\\infty]$ tartományra tudjuk transzormálni, akkor ebből logaritmus segítségével már könnyen $[-\\infty; +\\infty]$ tartományú változót csinálhatunk:\n",
    "\n",
    "$$ ln(f(P_{\\mathbf{x}})) = w_0 + w_1 x_1 + w_2 x_2 + \\ldots + w_n x_n $$\n",
    "\n",
    "Keresünk tehát egy olyan $f(x)$ függvényt, amely folytonos, és:\n",
    "\n",
    "$$ \\lim_{x \\to 0} f(x) = 0, $$\n",
    "$$ \\lim_{x \\to 1} f(x) = + \\infty.$$\n",
    "\n",
    "Az első feltétel teljesíthető egy alábbi alakú függvénnyel:\n",
    "$$ f(x) = \\frac{x \\cdot f_a(x)}{f_b(x)},$$\n",
    "feltéve, hogy $f_a(x)$ a nullában nem tart a végtelenhez, és $f_b(x)$ a nullában nem tart a nullához.\n",
    "\n",
    "A második feltétel teljesíthető egy alábbi alakú függvénnyel:\n",
    "$$ f(x) = \\frac{f_a(x)}{(1-x)\\cdot f_b(x)},$$\n",
    "feltéve, hogy $f_a(x)$ az egyben nem tart a nullához, és $f_b(x)$ az egyben nem tart a végtelenhez.\n",
    "\n",
    "A fenti két feltételt teljesítő legegyszerűbb függvény:\n",
    "$$f(x) = \\frac{x}{1-x}$$\n",
    "\n",
    "Ezek alapján a lineáris regresszióval az alábbi változó értékét tudjuk modellezni:\n",
    "$$\\ln(\\frac{P_{\\mathbf{x}}}{1-P_{\\mathbf{x}}}) = w_0 + w_1 x_1 + w_2 x_2 + \\ldots + w_n x_n = \\mathbf{w} \\cdot \\mathbf{x}$$\n",
    "ahol az $\\mathbf{x}$ vektor már tartalmazza a bias változót. Az összefüggést átalakítva megkaphatjuk a logisztikus regresszió alapját képző összefüggést:\n",
    "\n",
    "$$ \\frac{P_{\\mathbf{x}}}{1-P_{\\mathbf{x}}} = e^{\\mathbf{w} \\cdot \\mathbf{x}} $$\n",
    "$$ \\frac{1-P_{\\mathbf{x}}}{P_{\\mathbf{x}}} = \\frac{1}{P_{\\mathbf{x}}} - \\frac{P_{\\mathbf{x}}}{P_{\\mathbf{x}}}= e^{-\\mathbf{w} \\cdot \\mathbf{x}} $$\n",
    "$$ \\frac{1}{P_{\\mathbf{x}}} = 1+e^{-\\mathbf{w} \\cdot \\mathbf{x}} $$\n",
    "$$ P_{\\mathbf{x}} = \\frac{1}{1+e^{-\\mathbf{w} \\cdot \\mathbf{x}}} $$\n",
    "\n",
    "Az így meghatározott függvényalakot gyakran **Sigmoid** függvénynek nevezzük:\n",
    "$$\\boxed{ sigmoid(z) = \\sigma(z) = \\frac{1}{1+e^{-z}} }$$\n",
    "\n",
    "<!---\n",
    "<center><img src=\"img/sigmoid.svg\" width=\"600\"></center>\n",
    "-->\n",
    "<center><img src=\"https://raw.githubusercontent.com/MOGI-AI/adaptiv_labor/main/Lab03_linearLogisticRegression/img/sigmoid.svg\" width=\"600\"></center>\n",
    "\n",
    "A végleges modellünk a bináris változó becslésére, a lineáris regressziónál is alkalmazott mátrixos formában felírva, ahol a sigmoid függvényt a bementei argumentumára elemeneként kell alkalmazni:\n",
    "$$ \\boxed{ \\mathbf{\\hat{Y}} = P_{\\mathbf{X}} = \\sigma (\\mathbf{X} \\cdot \\mathbf{W}) } $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A bináris klasszifikáció költségfüggvénye\n",
    "\n",
    "A lineáris regressziónál megszokott MSE költségfüggvény a bináris klasszifikáció esetére is alkalmazható, azonban:\n",
    "- az optimálistól távoli súlyok esetén a tanulás nagyon lassú lehet a sigmoid görbe alakja miatt\n",
    "- a gradiens számítása kevésbé kellemes\n",
    "\n",
    "*Tipp:* a gyakorlat anyaga akár megoldható az MSE költségfüggvény és az ehhez tartozó megfelelő gradiens levezetésével és használatával is. Amennyiben a mélyebb megértés motivál valakit, érdemes ezt is implementálni, és összehasonlítani a két különböző módon kapott megoldásokat (tanulás sebessége, költésgfüggvények értékének alakulása, a modell által illesztett felület, a súlyok felett értelemzett optimalizálási felület).\n",
    "\n",
    "A klasszifikáció esetén a költségfüggvényt a Maximum Likelihood módszer alapján állítjuk elő. A Maximum Likelihood becslés esetén\n",
    "1. megvizsgáljuk, hogy adott modellparaméterek mellett mekkora az esélye, az egyes adatpontok előfordulásának\n",
    "2. meghatározzuk a teljes adatsor valószínűségét adott modellparaméterek mellett\n",
    "3. megkeressük azokat a modellparamétereket, amelyek mellett ez a valószínűség a lehető legnagyobb\n",
    "\n",
    "A bevezetett logisztikus regressziós modell esetén annak a valószínűsége, hogy egy adott $\\mathbf{x}$ bemeneti vektor esetén a magyarázott változó értéke 1 pontosan $P_x=\\hat{y}=\\sigma(\\mathbf{w} \\cdot \\mathbf{x})$, míg annak a valószínűsége, hogy a magyarázott változó értéke 0 pontosan $1-P_x = 1 - \\hat{y} = 1 - \\sigma(\\mathbf{w} \\cdot \\mathbf{x})$. Ezek alapján a regressziós paraméterek meghatározásával a\n",
    "\n",
    "$$ \\mathcal{L}(\\mathbf{w}|\\mathbf{X}) = \\prod_{y_l=1} (\\sigma(\\mathbf{w} \\cdot \\mathbf{x})) \\prod_{y_l=0} (1-\\sigma(\\mathbf{w} \\cdot \\mathbf{x})) = \\prod_{y_l=1} (\\hat{y}_l) \\prod_{y_l=0} (1-\\hat{y}_l)$$\n",
    "\n",
    "likelihood függvényt szeretnénk maximalizálni ( a továbbiakban az $\\mathbf{X}$ bementei mátrixtól való függőséget nem jelöljük). Ez 0 és 1 közötti valószínűségek szorzata, így az eredmény is 0 és egy közötti, azonban a nulla közelében sokkal \"sűrűbb\" (ha sok mintapont van, még nagy valószínűségek esetén is egyre jobban 0 felé fog tartani a valószínűség), így adott esteben $\\mathbf{w}$ még nagyléptékű változtatása sem fog jelentős változást eredményezni $\\mathcal{L}(\\mathbf{w}|\\mathbf{X})$ értékében, amely a gradiens módszer lassú tanulásához vezet. A logaritmus lépték használatával a 0-hoz közlebbi tartományon nagyobb érzékenység érhető el $\\mathbf{w}$ változtatásának függvényében, azonban az optimum nem változik; ahol a likelyhood függvénynek maximuma van, értelemszerűen a logaritmusának is ott van maximuma:\n",
    "\n",
    "$$\\ln \\left(\\mathcal{L}(\\mathbf{w})\\right) = \\ln \\left( \\prod_{y_l=1} \\hat{y}_l \\prod_{y_l=0} (1-\\hat{y}_l) \\right) $$\n",
    "\n",
    "A $\\log(a \\cdot b) = \\log(a)+\\log(b)$ azonosság alapján:\n",
    "\n",
    "$$\\ln \\left(\\mathcal{L}(\\mathbf{w})\\right) =  \\sum_{y_l=1} \\ln \\hat{y}_l + \\sum_{y_l=0} \\ln (1-\\hat{y}_l)$$\n",
    "\n",
    "A feltételes szummázást a bináris magyarázott változó segítségével kiváltva:\n",
    "\n",
    "$$\\ln \\left(\\mathcal{L}(\\mathbf{w})\\right) = \\sum_{l=1}^m \\left(y_l \\cdot \\ln \\hat{y}_l + (1-y_l) \\cdot \\ln (1-\\hat{y}_l) \\right)$$\n",
    "\n",
    "Ennek a függvénynek a maximalizálása egyenértékű a negatívjának a minimalizásálásval. A mintaelemszámtól függetlenítve az alábbi költségfüggvényt kapjuk:\n",
    "$$\\boxed{ C_{BCE} = \\frac{1}{m} \\sum_{l=1}^m \\left(-y_l \\cdot \\ln (\\hat{y}_l) - (1-y_l) \\cdot \\ln \\left(1-\\hat{y}_l\\right) \\right) }$$\n",
    "Ezt a függvény Binary Cross-Entropy költségfüggvénynek nevezik, matematikai háttere az itt bemutatottnál jóval mélyebbre nyúlik. A keresztentrópia pontos statisztikai jelentésének feltárását igény szerint mindenki önállóan megteheti.\n",
    "\n",
    "A költségfüggvényt megvizsgálva azt látjuk, hogy az adott adatpontra vonatkozóan a költségfüggvény komponens az alábbi alakot veszi fel:\n",
    "$$ C_{BCE}(w) = {-\\ln(\\hat{y}), \\qquad \\text{ha}\\ y=1} $$ \n",
    "$$ C_{BCE}(w) = {-\\ln(1-\\hat{y}),\\qquad \\text{ha}\\ y=0} $$\n",
    "\n",
    "<!---\n",
    "<center><img src=\"img/BCE.svg\" width=\"400\"></center>\n",
    "-->\n",
    "<center><img src=\"https://raw.githubusercontent.com/MOGI-AI/adaptiv_labor/main/Lab03_linearLogisticRegression/img/BCE.svg\" width=\"400\"></center>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Binary Cross-Entropy költségfüggvényhez tartozó gradiens\n",
    "\n",
    "A súlyok módosításához szükséges gradiens módszer alkalmazásához az új költségfüggvénnyel a gradiensvektor meghatározásához is új formula adódik. A parciális deriválást elvégezve a a gradiensvektor alakja mátrixos formában felírva (levezetés az **Appendix** részben megtalálható):\n",
    "\n",
    "$$ \n",
    "\\boxed { \\nabla C (\\mathbf{W}) = \n",
    "\\left[\n",
    "\t\\begin{array}{c}\n",
    " \t\t\\frac{\\delta C(\\mathbf{W})}{\\delta w_0}\\\\\n",
    "\t\t\\frac{\\delta C(\\mathbf{W})}{\\delta w_1}\\\\\n",
    "        \\frac{\\delta C(\\mathbf{W})}{\\delta w_2}\\\\\n",
    " \t\t\\vdots\\\\\n",
    "        \\frac{\\delta C(\\mathbf{W})}{\\delta w_n}\\\\\n",
    "\t\\end{array}\\right] \n",
    "= \\frac{1}{m}\\mathbf{X}^T \\cdot (\\sigma(\\mathbf{X} \\cdot \\mathbf{W}) - \\mathbf{Y}) }\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00: Könyvtár importálások\n",
    "\n",
    "Első lépésként importáljuk a feladat megoldása során használt könyvtárakat. Esetünkben ezek a következők lesznek:\n",
    "- Numpy a matematikia műveletek elvégzéséhez\n",
    "- Pandas az adatok beolvasásához és kezeléséhez\n",
    "- MatPlotLib.pyplot az eredményeink ábrázolásához\n",
    "- Plotly Express interaktív vizualizációhoz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Használjuk ezeket sötét téma esetén\n",
    "plt.style.use('dark_background')\n",
    "styleTemplate = 'plotly_dark'\n",
    "\n",
    "# Használjuk ezeket világos téma esetén\n",
    "#plt.style.use('default')\n",
    "#styleTemplate = 'plotly_white'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01: Adatbeolvasás\n",
    "Olvassuk be a szükséges adatokat az ```academicData.txt``` fájlból."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('academicData.txt', sep = ',', header=0)    # Olvassuk be az adatokat egy Pandas DataFrame ojektumba\n",
    "df.head(10)                                                  # Irassuk ki az első 10 sort, hogy ellenőrizzük sikerült-e a beolvasás"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02: Adatfelfedezés\n",
    "\n",
    "Új adthalmazzal történő első interrakció során érdemes azt először megvizsgálni, alapvető vizualizációkat ábrázolni, hogy legyen egy elsődleges \"benyomás\" az adatok jellegéről. Ábrázoljuk az adatokat egy X-Y diagrammon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formális vizualizáció MatPlotLib-el\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "plt.scatter(df[df['Acceptance'] == 0]['Math'], df[df['Acceptance'] == 0]['Literature'],marker='x',c=\"r\", label=\"Not admitted\")\n",
    "plt.scatter(df[df['Acceptance'] == 1]['Math'], df[df['Acceptance'] == 1]['Literature'],marker='o',c=\"g\", label=\"Admitted\")\n",
    "\n",
    "plt.title(\"Tanító adathalmaz\")\n",
    "plt.xlabel(\"Matematika eredmény [%]\")\n",
    "plt.ylabel(\"Irodalom eredmény [%]\")\n",
    "plt.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ábrázolás Plotly-val\n",
    "fig = go.Figure()\n",
    "\n",
    "# A magyarázott változót az előzőhöz hasonlóan transzponálni kell.\n",
    "fig.add_trace(go.Scatter3d(x=df['Math'], y=df['Literature'], z=df['Acceptance'], mode= \"markers\"))\n",
    "\n",
    "#Plot formázása\n",
    "fig.update_layout(\n",
    "    title = \"Felvétel sikerességének ésélye teszteredmények függvényében\",\n",
    "    scene = dict(\n",
    "        xaxis_title = \"Matematika eredmény [%]\",\n",
    "        yaxis_title = \"Irodalom eredmény [%]\",\n",
    "        zaxis_title = \"Felvételi sikeressége\"),\n",
    "    template=styleTemplate,\n",
    "    width=750,\n",
    "    height=500,\n",
    ")\n",
    "\n",
    "#Plot megjelenítése\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03: Adatok előkészítése\n",
    "\n",
    "A következő lépés az adatok esetleges előfeldolgozása és a modellillesztés elvégzéshez szükséges változók (mátrixok) létrehozása. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Math', 'Literature']].to_numpy() # Bemeneti változók oszlopainak kiemelése és tömmbé konvertálása\n",
    "Y = df[['Acceptance']].to_numpy() # Kimeneti változó oszlopának kiemelése és tömmbé konvertálása\n",
    "\n",
    "m,n = X.shape   \n",
    "print('X:',X.shape)                                            # adattömbök méretének / adatok számának kiírása\n",
    "print('Y:',Y.shape)\n",
    "print('Adatok száma',m)\n",
    "print('Változók (Feature) száma:',n)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Az előzőekhez hasonlóan itt is érdemes a bemeneti adatainkat normalizálni. A kimeneti változó bináris 0,1 értékű, így ezen nincs mit normalizálni.\n",
    "\n",
    "**Feladat:** Végezze el az X változóinak normalizálását!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureNormalize(Z):\n",
    "######################################################    \n",
    "\n",
    "\n",
    "\n",
    "######################################################\n",
    "    return Z_norm, mean, sigma                            # képlet alapján eredmény visszaadása\n",
    "\n",
    "print('Bementei adatok normalizálása ... \\n')                    \n",
    "X,Xmean,Xsigma = featureNormalize(X)                  # X standardizálása\n",
    "print('Vizsgapontok átlaga:', Xmean)\n",
    "print('Vizsgapontok szórása:', Xsigma, '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A kiszámolt átlag és szórás értékeket meg kell őriznünk azért, hogy később a modellünk valós bemeneti értkekkel tudjon predikciót végrehajtani. A könnyebb kezelhetőség érdekében ezt érdemes dictionary változóba rendeznünk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaleFactors = {\"Xmean\" : Xmean, \"Xsigma\" : Xsigma}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ellenőrizzuk a normalizálás sikerességét az új változók átlagának és szórásának kiszámításával!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Standardizált magyarázó változók átlaga:', np.mean(X,0))\n",
    "print('Standardizált magyarázó változók változók szórása:', np.std(X,0), '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amenniyben az átlagokra 0 közeli értéket, a szórásokra pedig 1 közelit kaptunk, a standardizálás sikeres volt."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feladat:** Egészítse ki az $\\mathbf{X}$ mátrixot a bias változóval!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X bővítése\n",
    "######################################################\n",
    "\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (X.shape[1] != 3):\n",
    "    print('Az X mátrix mérete nem megfelelő, ellenőrizze!')\n",
    "elif (np.sum(X, 0)[0] != 100):\n",
    "    print('Az X mátrix bias változóval való kiegészítése nem megfelelő, ellenőrizze!')\n",
    "else:\n",
    "    print('X:',X.shape)                                            # adattömbök méretének / adatok számának kiírása"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04: Modell definiálása\n",
    "\n",
    "Az adatok előkészítése utáni következő lépés a modell és tanítási logika definiálása."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feladat:** Definiálja a sigmoid függvényt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "############################################    \n",
    "\n",
    "############################################    \n",
    "    return g                               # szigmoid függvény értékének visszaadása\n",
    "\n",
    "print('Függvény érték -6 bemenő értékre: %.3f' % sigmoid(-6))           # teszt -6 -ra\n",
    "print('Függvény érték  0 bemenő értékre: %.3f' % sigmoid(0))            # teszt 0 -ra\n",
    "print('Függvény érték  6 bemenő értékre: %.3f' % sigmoid(6))            # teszt 6 -ra \n",
    "\n",
    "if sigmoid(-6) < 0.01 and sigmoid(0) == 0.5 and sigmoid(6) > 0.99:\n",
    "    print(\"\\n A sigmoid() függvény megfelelő.\")\n",
    "else:\n",
    "    print(\"\\n A sigmoid függvény implementációja hibás!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-6, 6, 100)\n",
    "y = sigmoid(x)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig .add_trace(go.Scatter(x = x, y = y, mode='lines'))\n",
    "\n",
    "fig .update_layout(\n",
    "    template=styleTemplate,\n",
    "    xaxis_title = \"z\",\n",
    "    yaxis_title = \"sigmoid(z)\",\n",
    "    title = \"Sigmoid függvény\",\n",
    "    width=600,\n",
    "    height=320,\n",
    "    showlegend = False\n",
    ")\n",
    "fig .update_traces(line=dict( width=3))\n",
    "fig .update_xaxes(showgrid=True, gridwidth=0.5, gridcolor='grey', zeroline=True, zerolinewidth=3, zerolinecolor='grey')\n",
    "fig .update_yaxes(showgrid=True, gridwidth=0.5, gridcolor='grey', zeroline=True, zerolinewidth=3, zerolinecolor='grey')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feladat:** Készítse el a BCE költségfüggvényt (a NumPy könyvtárban a természetes logaritmus az `np.log()` metódussal számítható)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Költségfüggvény\n",
    "def costBCE(X,Y,W):\n",
    "#############################################     \n",
    "\n",
    "\n",
    "############################################# \n",
    "   return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(((n+1),1))                                             # kezdeti súlyok (null vektor) létrehozása\n",
    "C = costBCE(X, Y, initial_w)                                                # költségfüggvény tesztelése 1\n",
    "\n",
    "print('''BCE Költségfüggvény értéke nullvektor kezdeti súlyokra:\n",
    "Várt költség (kb.): 0.693\n",
    "Számított:''',C)\n",
    "\n",
    "test_w = np.array([[-24], [13], [16]])                                      # teszt súlyok létrehozása [-24;0.2;0.2]\n",
    "C = costBCE(X, Y, test_w)                                                   # költségfüggvény tesztelése 2\n",
    "print('\\nTeszt súlyok:\\n',test_w)\n",
    "print('''BCE Költségfüvvény értéke teszt súlyokra:\n",
    "Várt költség (kb.): 7.74\n",
    "Számított:''',C)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feladat:** Készítse el logisztikus regresszió problémájat gradiens módszerrel megoldó algoritmust! Az algoritmus számolja ki a kezdeti, majd minden iteráció utáni költéségfüggvény értéket is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticGradientDescent(X, Y, W, learning_rate, epochs):\n",
    "    m = Y.size\n",
    "    C_history = np.zeros(epochs+1)\n",
    "    C_history[0] = costBCE(X,Y,W)\n",
    "######################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################\n",
    "    return W,C_history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05: Modell tanítás\n",
    "\n",
    "Futtassuk le a gradiens algoritmust és ellenőrizzük a kapott eredményeket, illetve vizsgáljuk meg a tanulási ráta hatását a tanulás folyamatára."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Gradiens algoritmus futtatása ...')\n",
    "epochs = 400                                             # epoch szám\n",
    "W=np.zeros([3,1])                                        # kezdeti súly (0;0;0)\n",
    "\n",
    "W_a, C_history_a = logisticGradientDescent(X,Y,W,0.01,epochs)                             \n",
    "plt.plot(range(C_history_a.size), C_history_a, label= \"lr = 0.01\")\n",
    "                                                       \n",
    "W_b, C_history_b = logisticGradientDescent(X,Y,W,0.1, epochs)                             \n",
    "plt.plot(range(C_history_b.size), C_history_b, label= \"lr = 0.1\")\n",
    "\n",
    "W, C_history = logisticGradientDescent(X,Y,W,1,epochs)\n",
    "plt.plot(range(C_history.size), C_history, label= \"lr = 1\")              \n",
    "\n",
    "plt.title(\"Különböző tanulási ráták hatása a konvergenciára\")\n",
    "plt.xlabel(\"Iteráció\")\n",
    "plt.ylabel(\"BCE költség\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fenti ábrán láthatjuk, hogy a tanulási ráta miként befolyásolta az eredményünket: <br>\n",
    "A tanulási rátát $(\\mu)$ kicsi értéknek választva a konvergencia lassú. <br>\n",
    "A tanulási rátát $(\\mu)$ növelve a konvergencia gyorsul (azonban adott esetben túllőhet a célon, ingadozást okozhat)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('''A tanulás végére kialakult költségfüggvény értéke:\n",
    "Elvárt (hozzávetőleges): 0.203\n",
    "Számított: %.04f''' % C_history[-1])\n",
    "\n",
    "print('''Várt súlyok (hozzávetőlegesen):\n",
    "[1.658 3.883 3.619]\n",
    "Az algoritmus által számított súlyok: \\n''', W.T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06: Modell értékelése"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 06.1: Predikció\n",
    "\n",
    "A modell használatához és értékeléséhez szükség van a modellel történő predikció lehetőségére. A predikció kiszámítása során ügyelni kell, hogy a mintán ugyan azokat az előfeldolgozó lépéseket végre kell hajtani, mint a tanítás előtt az adatok előkészítésénél. Az új, predikció alapjául szolgáló adatot is normalizálni kell, illetve a BIAS-t hozzáfűzni. Ezek után az elmentett súlyvektrral a predikció számolható.\n",
    "\n",
    "**Feladat:** Implementálja a modellhez tartozó predikció funkciót!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, W, scaleFactors):              # predikciós függvény\n",
    "###########################################    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###########################################    \n",
    "    return prob, pred                                    \n",
    "\n",
    "testScore = np.array([45,85])\n",
    "prob, pred = predict(testScore, W, scaleFactors)            # eredmény 45 és 85 pontra\n",
    "print('''A [45 , 85] teszteredményekre elvárt kimenet:\n",
    "Felvéve (1) 0.767 valószínűséggel\n",
    "Számított: %.0f; %.4f valószínűséggel''' % (pred, prob))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 06.2: Pontosság\n",
    "\n",
    "Vizsgáljuk meg hogy teljesít a modell az eredeti adatok kiértékelése során. Ehhez számoljuk ki az algoritmus pontosságát (az eredeti adatok hány százalékában adott jó választ.)\n",
    "\n",
    "**Feladat:** Implementálja a pontosság számítást végző metódust!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateAccuracy(X,Y,W):                               # pontosság függvény\n",
    "#######################################\n",
    "\n",
    "\n",
    "#######################################\n",
    "    return accuracy                                         # összehasonlítjuk az Y elemeivel az eredményt és százalékot\n",
    "                                                            # számolunk, ami tükrözi a pontosságot\n",
    "\n",
    "print(float(calculateAccuracy(X,Y,W)), '% pontoság (89.0 % elvárt)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 06.3: Döntési határ\n",
    "A döntési határ (decision boundary) kirajzolása. Ez a vonal választja el egymástól a két osztályt. Jelen esetben ez a $P_x = 0.5$ vonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "plt.scatter(df[df['Acceptance'] == 0]['Math'], df[df['Acceptance'] == 0]['Literature'],marker='x',c=\"r\", label=\"Not admitted\")\n",
    "plt.scatter(df[df['Acceptance'] == 1]['Math'], df[df['Acceptance'] == 1]['Literature'],marker='o',c=\"g\", label=\"Admitted\")\n",
    "\n",
    "Exam1_val = np.array([min(df['Math'])-2, max(df['Math']+2)])    # döntési határhoz felvesszünk két x értéket\n",
    "Exam1_norm = (Exam1_val - scaleFactors['Xmean'][0]) / scaleFactors['Xsigma'][0]\n",
    "Exam2_norm = (-W[0]-W[1]*Exam1_norm)/W[2]                                       # kiszámoljuk a hozzájuk tartozó y -t\n",
    "Exam2_val     = (Exam2_norm * scaleFactors['Xsigma'][1]) + scaleFactors['Xmean'][1]\n",
    "\n",
    "plt.plot(Exam1_val,Exam2_val, \"b\")                                   # döntési határt rajzolása kékkel\n",
    "\n",
    "plt.title(\"Logisztikus regresszió döntési határvonala\")\n",
    "plt.xlabel(\"Matematika eredmény [%]\")\n",
    "plt.ylabel(\"Irodalom eredmény [%]\")\n",
    "plt.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 06.4: Regressziós felület\n",
    "\n",
    "A teljesség érdekében nézzük meg a tanítás során illesztett teljes felületetet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bementek tartományát határozzuk meg az eredeti adataink alapján\n",
    "x = np.linspace(np.min(df['Math']),np.max(df['Math']),100)          \n",
    "y = np.linspace(np.min(df['Literature']),np.max(df['Literature']),100)\n",
    "z = np.zeros((y.size,y.size))\n",
    "\n",
    "# A becsült felvételi esélyeket határozzuk meg a már előkészített predict függvénnyel\n",
    "for i in range(x.size):\n",
    "    for j in range(y.size):\n",
    "        z[[i],[j]], _= predict([x[i], y[j]], W, scaleFactors)\n",
    "\n",
    "# Ábrázolás Plotly-val\n",
    "fig = go.Figure()\n",
    "\n",
    "# A magyarázott változót transzponálni kell a helyes megjelenítésért.\n",
    "fig.add_trace(go.Scatter3d(x=df['Math'], y=df['Literature'], z=Y[:,0], mode= \"markers\"))\n",
    "fig.add_trace(go.Surface(x=x, y=y, z=z.T, colorscale ='Blues'))\n",
    "\n",
    "#Plot formázása\n",
    "fig.update_layout(\n",
    "    title = \"Felvétel esélyének becslése\",\n",
    "    scene = dict(\n",
    "        xaxis_title = \"Matematika eredmény [%]\",\n",
    "        yaxis_title = \"Irodalom eredmény [%]\",\n",
    "        zaxis_title = \"Felvételi sikeressége\"),\n",
    "    template=styleTemplate,\n",
    "    width=750,\n",
    "    height=500,\n",
    ")\n",
    "\n",
    "#Plot megjelenítése\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XX: Megoldás a Scikit-learn könyvtár segítségével\n",
    "\n",
    "A feladat az eddigiekhez hasonló módon megoldható egy magasabb szintű könyvtár által implementált modellel is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "df = pd.read_csv('academicData.txt', sep = ',', header=0)       # adatok beolvasása\n",
    "XX = df[['Math', 'Literature']].to_numpy()                      # adatok szétválogatása\n",
    "YY = df['Acceptance'].to_numpy()                                # adatok szétválogatása (LogReg fit-je egy 1d tömböt vár!)\n",
    "\n",
    "logReg = LogisticRegression().fit(XX,YY)\n",
    "test = np.array([[45, 85]])                             # mivel egy mintánk van ezért (1,2) array kell ezért a dupla []\n",
    "pred = logReg.predict(test)                             # sima predikció\n",
    "pred_p = logReg.predict_proba(test)                     # a teszteset valszínűségi predikciója\n",
    "\n",
    "print(\"\"\"Prediction for the approval:\"\"\",int(pred),\"\"\"\n",
    "The value of the probability:\"\"\",pred_p[0,1])\n",
    "\n",
    "acc = logReg.score(XX,YY)                               # pontosság kiszámolása \n",
    "print('Accuracy on the training data:',acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix - Gradiensvektorok levezetése"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logisztikus regresszio BCE költségfüggvénnyel\n",
    "\n",
    "Az BCE költségfüggvény alakja logisztikus regresszió esetén\n",
    "\n",
    "$$C_{BCE} = \\frac{1}{m} \\sum_{l=1}^m \\left(-y_l \\cdot \\ln (\\hat{y}_l) - (1-y_l) \\cdot \\ln \\left(1-\\hat{y}_l\\right) \\right) $$\n",
    "\n",
    "A gradiens számításának elve nem változik a lineáris regressziós esethez képest:\n",
    "\n",
    "$$\\nabla C (\\mathbf{W}) = \n",
    "\\left[\n",
    "\t\\begin{array}{c}\n",
    " \t\t\\frac{\\delta C(\\mathbf{W})}{\\delta w_0}\\\\\n",
    "\t\t\\frac{\\delta C(\\mathbf{W})}{\\delta w_1}\\\\\n",
    "        \\frac{\\delta C(\\mathbf{W})}{\\delta w_2}\\\\\n",
    " \t\t\\vdots\\\\\n",
    "        \\frac{\\delta C(\\mathbf{W})}{\\delta w_n}\\\\\n",
    "\t\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "Egy adott parciális derivált számítása az $(f \\circ g)' = (f' \\circ g) \\cdot g'$ deriválási azonosság alapján:\n",
    "\n",
    "$$ \\frac{\\delta C(\\mathbf{W})}{\\delta w_i} = \\frac{\\delta \\frac{1}{m} \\sum_{l=1}^m \\left(-y_l \\cdot \\ln (\\hat{y}_l) - (1-y_l) \\cdot \\ln \\left(1-\\hat{y}_l\\right) \\right)}{{\\delta w_i}} = \\frac{1}{m} \\sum_{l=1}^m \\left(\\frac{\\delta (-y_l \\cdot \\ln (\\hat{y}_l))} {{\\delta w_i}} - \\frac{ \\delta ((1-y_l) \\cdot \\ln (1-\\hat{y}_l))} {{\\delta w_i}} \\right) = $$\n",
    "\n",
    "$$ = \\frac{1}{m} \\sum_{l=1}^m \\left(-y_l \\cdot \\frac{1}{\\hat{y}_l} \\cdot \\frac{\\delta (\\hat{y}_l)} {{\\delta w_i}} -  (1-y_l) \\cdot \\frac{1}{1-\\hat{y}_l} \\cdot \\frac{ \\delta (1-\\hat{y}_l)} {{\\delta w_i}} \\right) = \\frac{1}{m} \\sum_{l=1}^m \\left(-y_l \\cdot \\frac{1}{\\hat{y}_l} \\cdot \\frac{\\delta (\\hat{y}_l)} {{\\delta w_i}} -  (1-y_l) \\cdot \\frac{1}{1-\\hat{y}_l} \\cdot \\frac{ - \\delta (\\hat{y}_l)} {{\\delta w_i}} \\right) = $$\n",
    "\n",
    "$$ = \\frac{1}{m} \\sum_{l=1}^m \\left( \\left(-y_l \\cdot \\frac{1}{\\hat{y}_l} +  (1-y_l) \\cdot \\frac{1}{1-\\hat{y}_l} \\right) \\cdot \\left(\\frac{ \\delta (\\hat{y}_l)} {{\\delta w_i}} \\right) \\right) = \\frac{1}{m} \\sum_{l=1}^m \\left( \\left(-y_l \\cdot \\frac{1}{\\sigma(\\mathbf{w} \\cdot \\mathbf{x}_l)} +  (1-y_l) \\cdot \\frac{1}{1-\\sigma(\\mathbf{w} \\cdot \\mathbf{x}_l)} \\right) \\cdot \\left(\\frac{ \\delta (\\sigma(\\mathbf{w} \\cdot \\mathbf{x}_l))} {{\\delta w_i}} \\right) \\right) = $$\n",
    "\n",
    "$$ = \\frac{1}{m} \\sum_{l=1}^m \\left( \\left(-y_l \\cdot \\frac{1}{\\sigma(\\mathbf{w} \\cdot \\mathbf{x}_l)} +  (1-y_l) \\cdot \\frac{1}{1-\\sigma(\\mathbf{w} \\cdot \\mathbf{x}_l)} \\right) \\cdot \\sigma'(\\mathbf{w} \\cdot \\mathbf{x}_l) \\cdot x_{l,i} \\right) $$\n",
    "\n",
    "$$ = \\frac{1}{m} \\sum_{l=1}^m \\left( \\left(-y_l \\cdot \\frac{1}{\\sigma(\\mathbf{w} \\cdot \\mathbf{x}_l)} +  (1-y_l) \\cdot \\frac{1}{1-\\sigma(\\mathbf{w} \\cdot \\mathbf{x}_l)} \\right) \\cdot \\left( \\sigma(\\mathbf{w} \\cdot \\mathbf{x}_l) \\cdot (1-\\sigma(\\mathbf{w} \\cdot \\mathbf{x}_l)) \\right) \\cdot x_{l,i} \\right) $$\n",
    "\n",
    "$$ = \\frac{1}{m} \\sum_{l=1}^m \\left( \\left(-y_l \\cdot (1-\\sigma(\\mathbf{w} \\cdot \\mathbf{x}_l)) +  (1-y_l) \\cdot \\sigma(\\mathbf{w} \\cdot \\mathbf{x}_l) \\right) \\cdot \\cdot x_{l,i} \\right) = \\frac{1}{m} \\sum_{l=1}^m \\left( \\left(-y_l \\cdot (1-\\hat{y}_l) +  (1-y_l) \\cdot \\hat{y}_l \\right) \\cdot x_{l,i} \\right)$$\n",
    "\n",
    "\n",
    "Vegyük észre, hogy:\n",
    "\n",
    "$$ \\left( -y_l \\cdot (1-\\hat{y}_l) +  (1-y_l) \\cdot \\hat{y}_l \\right)|_{y_l = 0 } = \\hat{y}_l = \\hat{y}_l - 0 = \\hat{y}_l - y_l $$\n",
    "$$ \\left( -y_l \\cdot (1-\\hat{y}_l) +  (1-y_l) \\cdot \\hat{y}_l \\right)|_{y_l = 1 } = -1 \\cdot \\left(1 - \\hat{y}_l \\right) = \\hat{y}_l - 1 = \\hat{y}_l - y_l $$\n",
    "\n",
    "Így tehát a költségfüggvény $w_i$ szerintei parciális deriváltja:\n",
    "\n",
    "$$ \\frac{\\delta C(\\mathbf{W})}{\\delta w_i} = \\frac{1}{m} \\sum_{l=1}^m \\left( \\left(\\hat{y}_l - y_l \\right) \\cdot x_{l,i} \\right) $$\n",
    "\n",
    "Mátrixos alakban felírva :\n",
    "\n",
    "$$ \n",
    "\\nabla C (\\mathbf{W}) = \n",
    "\\left[\n",
    "\t\\begin{array}{c}\n",
    " \t\t\\frac{\\delta C(\\mathbf{W})}{\\delta w_0}\\\\\n",
    "\t\t\\frac{\\delta C(\\mathbf{W})}{\\delta w_1}\\\\\n",
    "        \\frac{\\delta C(\\mathbf{W})}{\\delta w_2}\\\\\n",
    " \t\t\\vdots\\\\\n",
    "        \\frac{\\delta C(\\mathbf{W})}{\\delta w_n}\\\\\n",
    "\t\\end{array}\\right] \n",
    "= \\frac{1}{m}\\mathbf{X}^T \\cdot (\\sigma (\\mathbf{X} \\cdot \\mathbf{W}) - \\mathbf{Y})\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
